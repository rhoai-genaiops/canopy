{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "45fc9086-93aa-4645-8ba2-380c3acbbed9",
   "metadata": {},
   "source": [
    "# Level 1: Simple RAG\n",
    "\n",
    "This notebook will show you how to build a simple RAG application with Llama Stack. You will learn how the API's provided by Llama Stack can be used to directly control and invoke all common RAG stages, including indexing, retrieval and inference. \n",
    "\n",
    "_Note: This notebook contains a non-agentic implementation of RAG. We will show you how to build an agentic RAG application later in this tutorial in [Level4_RAG_agent](Level4_RAG_agent.ipynb)._\n",
    "\n",
    "## Overview\n",
    "\n",
    "This tutorial covers the following steps:\n",
    "1. Indexing a collection of documents into a vector database for later retrieval.\n",
    "2. Executing the built-in RAG tool to retrieve the document chunks relevant to a given query.\n",
    "3. Using the retrieved context to answer user queries during the inference step."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6db34e4b-ed29-4007-b760-59543d4caca1",
   "metadata": {},
   "source": [
    "## 1. Setting Up this Notebook\n",
    "\n",
    "First, we will start with a few imports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "332e6cec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: llama_stack_client in /opt/app-root/lib64/python3.11/site-packages (0.2.12)\n",
      "Requirement already satisfied: fire in /opt/app-root/lib64/python3.11/site-packages (0.7.0)\n",
      "Requirement already satisfied: dotenv in /opt/app-root/lib64/python3.11/site-packages (0.9.9)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /opt/app-root/lib64/python3.11/site-packages (from llama_stack_client) (4.9.0)\n",
      "Requirement already satisfied: click in /opt/app-root/lib64/python3.11/site-packages (from llama_stack_client) (8.2.1)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /opt/app-root/lib64/python3.11/site-packages (from llama_stack_client) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /opt/app-root/lib64/python3.11/site-packages (from llama_stack_client) (0.28.1)\n",
      "Requirement already satisfied: pandas in /opt/app-root/lib64/python3.11/site-packages (from llama_stack_client) (2.2.3)\n",
      "Requirement already satisfied: prompt-toolkit in /opt/app-root/lib64/python3.11/site-packages (from llama_stack_client) (3.0.50)\n",
      "Requirement already satisfied: pyaml in /opt/app-root/lib64/python3.11/site-packages (from llama_stack_client) (25.5.0)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in /opt/app-root/lib64/python3.11/site-packages (from llama_stack_client) (2.11.7)\n",
      "Requirement already satisfied: rich in /opt/app-root/lib64/python3.11/site-packages (from llama_stack_client) (14.0.0)\n",
      "Requirement already satisfied: sniffio in /opt/app-root/lib64/python3.11/site-packages (from llama_stack_client) (1.3.1)\n",
      "Requirement already satisfied: termcolor in /opt/app-root/lib64/python3.11/site-packages (from llama_stack_client) (3.1.0)\n",
      "Requirement already satisfied: tqdm in /opt/app-root/lib64/python3.11/site-packages (from llama_stack_client) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.7 in /opt/app-root/lib64/python3.11/site-packages (from llama_stack_client) (4.12.2)\n",
      "Requirement already satisfied: python-dotenv in /opt/app-root/lib64/python3.11/site-packages (from dotenv) (1.1.1)\n",
      "Requirement already satisfied: idna>=2.8 in /opt/app-root/lib64/python3.11/site-packages (from anyio<5,>=3.5.0->llama_stack_client) (3.10)\n",
      "Requirement already satisfied: certifi in /opt/app-root/lib64/python3.11/site-packages (from httpx<1,>=0.23.0->llama_stack_client) (2025.1.31)\n",
      "Requirement already satisfied: httpcore==1.* in /opt/app-root/lib64/python3.11/site-packages (from httpx<1,>=0.23.0->llama_stack_client) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in /opt/app-root/lib64/python3.11/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->llama_stack_client) (0.16.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /opt/app-root/lib64/python3.11/site-packages (from pydantic<3,>=1.9.0->llama_stack_client) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /opt/app-root/lib64/python3.11/site-packages (from pydantic<3,>=1.9.0->llama_stack_client) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /opt/app-root/lib64/python3.11/site-packages (from pydantic<3,>=1.9.0->llama_stack_client) (0.4.1)\n",
      "Requirement already satisfied: numpy>=1.23.2 in /opt/app-root/lib64/python3.11/site-packages (from pandas->llama_stack_client) (2.2.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/app-root/lib64/python3.11/site-packages (from pandas->llama_stack_client) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/app-root/lib64/python3.11/site-packages (from pandas->llama_stack_client) (2025.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/app-root/lib64/python3.11/site-packages (from pandas->llama_stack_client) (2025.1)\n",
      "Requirement already satisfied: wcwidth in /opt/app-root/lib64/python3.11/site-packages (from prompt-toolkit->llama_stack_client) (0.2.13)\n",
      "Requirement already satisfied: PyYAML in /opt/app-root/lib64/python3.11/site-packages (from pyaml->llama_stack_client) (6.0.2)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /opt/app-root/lib64/python3.11/site-packages (from rich->llama_stack_client) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/app-root/lib64/python3.11/site-packages (from rich->llama_stack_client) (2.19.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in /opt/app-root/lib64/python3.11/site-packages (from markdown-it-py>=2.2.0->rich->llama_stack_client) (0.1.2)\n",
      "Requirement already satisfied: six>=1.5 in /opt/app-root/lib64/python3.11/site-packages (from python-dateutil>=2.8.2->pandas->llama_stack_client) (1.17.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install llama_stack_client fire dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f15080a6-48be-4475-8813-c584701d69bf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import uuid\n",
    "\n",
    "from llama_stack_client import RAGDocument\n",
    "from llama_stack_client.types.shared.content_delta import TextDelta, ToolCallDelta\n",
    "import base64\n",
    "import requests\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "631e8c70-6f28-440b-b71a-85d4040ffac4",
   "metadata": {},
   "source": [
    "Next, we will initialize our environment as described in detail in our [\"Getting Started\" notebook](Level0_getting_started_with_Llama_Stack.ipynb). Please refer to it for additional explanations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "558909bb-955c-40a3-a0c2-1f4acb0dd62e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected to Llama Stack server\n",
      "Inference Parameters:\n",
      "\tModel: llama32-3b\n",
      "\tSampling Parameters: {'strategy': {'type': 'greedy'}, 'max_tokens': 4096}\n",
      "\tstream: True\n"
     ]
    }
   ],
   "source": [
    "# for accessing the environment variables\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "# for communication with Llama Stack\n",
    "from llama_stack_client import LlamaStackClient\n",
    "\n",
    "# pretty print of the results returned from the model/agent\n",
    "import sys\n",
    "sys.path.append('..')  \n",
    "from src.utils import step_printer\n",
    "from termcolor import cprint\n",
    "\n",
    "base_url = \"http://llama-stack.genaiops-rag.svc.cluster.local:80\" #os.getenv(\"REMOTE_BASE_URL\")\n",
    "\n",
    "# Tavily search API key is required for some of our demos and must be provided to the client upon initialization.\n",
    "# We will cover it in the agentic demos that use the respective tool. Please ignore this parameter for all other demos.\n",
    "tavily_search_api_key = os.getenv(\"TAVILY_SEARCH_API_KEY\")\n",
    "if tavily_search_api_key is None:\n",
    "    provider_data = None\n",
    "else:\n",
    "    provider_data = {\"tavily_search_api_key\": tavily_search_api_key}\n",
    "\n",
    "\n",
    "client = LlamaStackClient(\n",
    "    base_url=base_url,\n",
    "    provider_data=provider_data\n",
    ")\n",
    "    \n",
    "print(f\"Connected to Llama Stack server\")\n",
    "\n",
    "# model_id for the model you wish to use that is configured with the Llama Stack server\n",
    "model_id = \"llama32-3b\"\n",
    "\n",
    "temperature = float(os.getenv(\"TEMPERATURE\", 0.0))\n",
    "if temperature > 0.0:\n",
    "    top_p = float(os.getenv(\"TOP_P\", 0.95))\n",
    "    strategy = {\"type\": \"top_p\", \"temperature\": temperature, \"top_p\": top_p}\n",
    "else:\n",
    "    strategy = {\"type\": \"greedy\"}\n",
    "\n",
    "max_tokens = int(os.getenv(\"MAX_TOKENS\", 4096))\n",
    "\n",
    "# sampling_params will later be used to pass the parameters to Llama Stack Agents/Inference APIs\n",
    "sampling_params = {\n",
    "    \"strategy\": strategy,\n",
    "    \"max_tokens\": max_tokens,\n",
    "}\n",
    "\n",
    "stream_env = os.getenv(\"STREAM\", \"True\")\n",
    "# the Boolean 'stream' parameter will later be passed to Llama Stack Agents/Inference APIs\n",
    "# any value non equal to 'False' will be considered as 'True'\n",
    "stream = (stream_env != \"False\")\n",
    "\n",
    "print(f\"Inference Parameters:\\n\\tModel: {model_id}\\n\\tSampling Parameters: {sampling_params}\\n\\tstream: {stream}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "841eaadf-f5ac-4d7c-bb9d-f039ccd8d9a3",
   "metadata": {},
   "source": [
    "Finally, we complete the setup by initializing the document collection we will use for RAG ingestion and retrieval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c725c2da-05e5-474f-9a44-cf5615557665",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "vector_db_id = f\"test_vector_db_{uuid.uuid4()}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87510929-fe4b-428c-8f9e-14d47a03daa2",
   "metadata": {},
   "source": [
    "## 2. Indexing the Documents\n",
    "- Initialize a new document collection in our vector database. All parameters related to the vector database, such as the embedding model and dimension, must be specified here.\n",
    "- Provide a list of document URLs to the RAG tool. Llama Stack will handle fetching, converting, and chunking the content of the documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "8d81ffb2-2089-4cb8-adae-f32965f206c7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://llama-stack.genaiops-rag.svc.cluster.local/v1/vector-dbs \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://llama-stack.genaiops-rag.svc.cluster.local/v1/tool-runtime/rag-tool/insert \"HTTP/1.1 200 OK\"\n"
     ]
    }
   ],
   "source": [
    "# define and register the document collection to be used\n",
    "client.vector_dbs.register(\n",
    "    vector_db_id=vector_db_id,\n",
    "    embedding_model=\"all-MiniLM-L6-v2\", #os.getenv(\"VDB_EMBEDDING\"),\n",
    "    embedding_dimension=int(os.getenv(\"VDB_EMBEDDING_DIMENSION\", 384)),\n",
    "    provider_id=\"milvus\", #os.getenv(\"VDB_PROVIDER\"),\n",
    ")\n",
    "\n",
    "# ingest the documents into the newly created document collection\n",
    "urls = [\n",
    "    (\"https://raw.githubusercontent.com/rhoai-genaiops/deploy-lab/main/university-data/canopy-in-botany.pdf\", \"application/pdf\"),\n",
    "    # (\"https://www.openshift.guide/openshift-guide-screen.pdf\", \"application/pdf\"),\n",
    "]\n",
    "documents = [\n",
    "    RAGDocument(\n",
    "        document_id=f\"num-{i}\",\n",
    "        content=url,\n",
    "        mime_type=url_type,\n",
    "        metadata={\"source_url\": url},\n",
    "    )\n",
    "    for i, (url, url_type) in enumerate(urls)\n",
    "]\n",
    "\n",
    "#image_url = \"https://raw.githubusercontent.com/rhoai-genaiops/deploy-lab/main/university-data/canopy-image.jpg\"\n",
    "#image_url = \"https://ehq-production-australia.imgix.net/4a07fa408fdd60d1d4052d0d83911f60696fd584/photos/images/000/037/330/original/Tree_Canopy_Health.png\"\n",
    "# image_url = \"https://raw.githubusercontent.com/meta-llama/llama-stack/refs/heads/main/docs/_static/llama-stack.png\"\n",
    "# B64_ENCODED_IMAGE = base64.b64encode(requests.get(image_url).content).decode(\"utf-8\")\n",
    "\n",
    "# image_document = RAGDocument(\n",
    "#     document_id=\"num-image-3\",\n",
    "#     content={\n",
    "#         \"type\": \"image\",\n",
    "#         \"image\": {\"data\": B64_ENCODED_IMAGE},\n",
    "#     },\n",
    "#     metadata={\"source_url\": image_url},\n",
    "#     mime_type=\"image/jpeg\",\n",
    "# )\n",
    "\n",
    "documents.append(image_document)\n",
    "\n",
    "client.tool_runtime.rag_tool.insert(\n",
    "    documents=documents,\n",
    "    vector_db_id=vector_db_id,\n",
    "    chunk_size_in_tokens=int(os.getenv(\"VECTOR_DB_CHUNK_SIZE\", 512)),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5639413-90d6-42ae-add4-6c89da0297e2",
   "metadata": {},
   "source": [
    "## 3. Executing Queries via the Built-in RAG Tool\n",
    "- Directly invoke the RAG tool to query the vector database we ingested into at the previous stage.\n",
    "- Construct an extended prompt using the retrieved chunks.\n",
    "- Query the model with the extended prompt.\n",
    "- Output the reply received from the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "142e1ed0",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "0d39ab00-2a65-4b72-b5ed-4dd61f1204a2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\n",
      "User> What are the types of Canopy?\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://llama-stack.genaiops-rag.svc.cluster.local/v1/tool-runtime/rag-tool/query \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST http://llama-stack.genaiops-rag.svc.cluster.local/v1/inference/chat-completion \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33m\n",
      "--- RAG Metadata ---\u001b[0m\n",
      "\u001b[36m{'document_ids': ['num-0', 'num-0', 'num-0', 'num-0', 'num-0']}\u001b[0m\n",
      "\u001b[35minference> \u001b[0m\u001b[35m\u001b[0m\u001b[35mBased\u001b[0m\u001b[35m on\u001b[0m\u001b[35m the\u001b[0m\u001b[35m provided\u001b[0m\u001b[35m knowledge\u001b[0m\u001b[35m search\u001b[0m\u001b[35m tool\u001b[0m\u001b[35m results\u001b[0m\u001b[35m,\u001b[0m\u001b[35m the\u001b[0m\u001b[35m types\u001b[0m\u001b[35m of\u001b[0m\u001b[35m Can\u001b[0m\u001b[35mopy\u001b[0m\u001b[35m can\u001b[0m\u001b[35m be\u001b[0m\u001b[35m summarized\u001b[0m\u001b[35m as\u001b[0m\u001b[35m follows\u001b[0m\u001b[35m:\n",
      "\n",
      "\u001b[0m\u001b[35m1\u001b[0m\u001b[35m.\u001b[0m\u001b[35m **\u001b[0m\u001b[35mD\u001b[0m\u001b[35mense\u001b[0m\u001b[35m Can\u001b[0m\u001b[35mopy\u001b[0m\u001b[35m**:\u001b[0m\u001b[35m Forms\u001b[0m\u001b[35m a\u001b[0m\u001b[35m continuous\u001b[0m\u001b[35m or\u001b[0m\u001b[35m semi\u001b[0m\u001b[35m-\u001b[0m\u001b[35mcontinuous\u001b[0m\u001b[35m cover\u001b[0m\u001b[35m of\u001b[0m\u001b[35m foliage\u001b[0m\u001b[35m and\u001b[0m\u001b[35m branches\u001b[0m\u001b[35m that\u001b[0m\u001b[35m stretches\u001b[0m\u001b[35m above\u001b[0m\u001b[35m the\u001b[0m\u001b[35m forest\u001b[0m\u001b[35m floor\u001b[0m\u001b[35m,\u001b[0m\u001b[35m typically\u001b[0m\u001b[35m found\u001b[0m\u001b[35m in\u001b[0m\u001b[35m rain\u001b[0m\u001b[35mfore\u001b[0m\u001b[35msts\u001b[0m\u001b[35m.\u001b[0m\u001b[35m This\u001b[0m\u001b[35m type\u001b[0m\u001b[35m of\u001b[0m\u001b[35m canopy\u001b[0m\u001b[35m supports\u001b[0m\u001b[35m a\u001b[0m\u001b[35m unique\u001b[0m\u001b[35m ecosystem\u001b[0m\u001b[35m distinct\u001b[0m\u001b[35m from\u001b[0m\u001b[35m the\u001b[0m\u001b[35m forest\u001b[0m\u001b[35m floor\u001b[0m\u001b[35m.\n",
      "\u001b[0m\u001b[35m2\u001b[0m\u001b[35m.\u001b[0m\u001b[35m **\u001b[0m\u001b[35mOpen\u001b[0m\u001b[35m Can\u001b[0m\u001b[35mopy\u001b[0m\u001b[35m**:\u001b[0m\u001b[35m Allows\u001b[0m\u001b[35m more\u001b[0m\u001b[35m sunlight\u001b[0m\u001b[35m to\u001b[0m\u001b[35m penetrate\u001b[0m\u001b[35m through\u001b[0m\u001b[35m to\u001b[0m\u001b[35m the\u001b[0m\u001b[35m lower\u001b[0m\u001b[35m layers\u001b[0m\u001b[35m of\u001b[0m\u001b[35m vegetation\u001b[0m\u001b[35m,\u001b[0m\u001b[35m typically\u001b[0m\u001b[35m found\u001b[0m\u001b[35m in\u001b[0m\u001b[35m more\u001b[0m\u001b[35m open\u001b[0m\u001b[35m wood\u001b[0m\u001b[35mlands\u001b[0m\u001b[35m or\u001b[0m\u001b[35m sav\u001b[0m\u001b[35mann\u001b[0m\u001b[35mas\u001b[0m\u001b[35m.\u001b[0m\u001b[35m This\u001b[0m\u001b[35m type\u001b[0m\u001b[35m of\u001b[0m\u001b[35m canopy\u001b[0m\u001b[35m encourages\u001b[0m\u001b[35m the\u001b[0m\u001b[35m growth\u001b[0m\u001b[35m of\u001b[0m\u001b[35m shr\u001b[0m\u001b[35mubs\u001b[0m\u001b[35m and\u001b[0m\u001b[35m ground\u001b[0m\u001b[35m flora\u001b[0m\u001b[35m.\n",
      "\u001b[0m\u001b[35m3\u001b[0m\u001b[35m.\u001b[0m\u001b[35m **\u001b[0m\u001b[35mStr\u001b[0m\u001b[35mat\u001b[0m\u001b[35mified\u001b[0m\u001b[35m Can\u001b[0m\u001b[35mopy\u001b[0m\u001b[35m**:\u001b[0m\u001b[35m Exhib\u001b[0m\u001b[35mits\u001b[0m\u001b[35m multiple\u001b[0m\u001b[35m horizontal\u001b[0m\u001b[35m layers\u001b[0m\u001b[35m composed\u001b[0m\u001b[35m of\u001b[0m\u001b[35m different\u001b[0m\u001b[35m species\u001b[0m\u001b[35m that\u001b[0m\u001b[35m specialize\u001b[0m\u001b[35m in\u001b[0m\u001b[35m capturing\u001b[0m\u001b[35m light\u001b[0m\u001b[35m at\u001b[0m\u001b[35m various\u001b[0m\u001b[35m heights\u001b[0m\u001b[35m.\u001b[0m\u001b[35m This\u001b[0m\u001b[35m type\u001b[0m\u001b[35m of\u001b[0m\u001b[35m canopy\u001b[0m\u001b[35m is\u001b[0m\u001b[35m found\u001b[0m\u001b[35m in\u001b[0m\u001b[35m some\u001b[0m\u001b[35m forests\u001b[0m\u001b[35m,\u001b[0m\u001b[35m where\u001b[0m\u001b[35m the\u001b[0m\u001b[35m density\u001b[0m\u001b[35m and\u001b[0m\u001b[35m arrangement\u001b[0m\u001b[35m of\u001b[0m\u001b[35m the\u001b[0m\u001b[35m canopy\u001b[0m\u001b[35m influence\u001b[0m\u001b[35m environmental\u001b[0m\u001b[35m factors\u001b[0m\u001b[35m such\u001b[0m\u001b[35m as\u001b[0m\u001b[35m light\u001b[0m\u001b[35m availability\u001b[0m\u001b[35m,\u001b[0m\u001b[35m wind\u001b[0m\u001b[35m speed\u001b[0m\u001b[35m,\u001b[0m\u001b[35m humidity\u001b[0m\u001b[35m,\u001b[0m\u001b[35m and\u001b[0m\u001b[35m temperature\u001b[0m\u001b[35m.\n",
      "\n",
      "\u001b[0m\u001b[35mThese\u001b[0m\u001b[35m types\u001b[0m\u001b[35m of\u001b[0m\u001b[35m Can\u001b[0m\u001b[35mopy\u001b[0m\u001b[35m are\u001b[0m\u001b[35m not\u001b[0m\u001b[35m mutually\u001b[0m\u001b[35m exclusive\u001b[0m\u001b[35m,\u001b[0m\u001b[35m and\u001b[0m\u001b[35m some\u001b[0m\u001b[35m forests\u001b[0m\u001b[35m may\u001b[0m\u001b[35m exhibit\u001b[0m\u001b[35m a\u001b[0m\u001b[35m combination\u001b[0m\u001b[35m of\u001b[0m\u001b[35m these\u001b[0m\u001b[35m characteristics\u001b[0m\u001b[35m.\u001b[0m\u001b[35m\u001b[0m"
     ]
    }
   ],
   "source": [
    "queries = [\n",
    "    \"What are the types of Canopy?\",\n",
    "]\n",
    "\n",
    "for prompt in queries:\n",
    "    cprint(f\"\\nUser> {prompt}\", \"blue\")\n",
    "    \n",
    "    # RAG retrieval call\n",
    "    rag_response = client.tool_runtime.rag_tool.query(\n",
    "        content=prompt, \n",
    "        vector_db_ids=[vector_db_id],\n",
    "        query_config={\n",
    "            \"chunk_template\": \"Result {index}\\nContent: {chunk.content}\\nMetadata: {metadata}\\n\",\n",
    "        },\n",
    "        )\n",
    "\n",
    "    cprint(f\"\\n--- RAG Metadata ---\", \"yellow\")\n",
    "    cprint(rag_response.metadata, \"cyan\")\n",
    "\n",
    "    # the list of messages to be sent to the model must start with the system prompt\n",
    "    messages = [{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}]\n",
    "\n",
    "    # construct the actual prompt to be executed, incorporating the original query and the retrieved content\n",
    "    prompt_context = rag_response.content\n",
    "    extended_prompt = f\"Please answer the given query using the context below.\\n\\nCONTEXT:\\n{prompt_context}\\n\\nQUERY:\\n{prompt}\"\n",
    "    messages.append({\"role\": \"user\", \"content\": extended_prompt})\n",
    "\n",
    "    # use Llama Stack inference API to directly communicate with the desired model\n",
    "    response = client.inference.chat_completion(\n",
    "        messages=messages,\n",
    "        model_id=model_id,\n",
    "        sampling_params=sampling_params,\n",
    "        stream=stream,\n",
    "    )\n",
    "    \n",
    "    # print the response\n",
    "    cprint(\"inference> \", color=\"magenta\", end='')\n",
    "    if stream:\n",
    "        for chunk in response:\n",
    "            response_delta = chunk.event.delta\n",
    "            if isinstance(response_delta, TextDelta):\n",
    "                cprint(response_delta.text, color=\"magenta\", end='')\n",
    "            elif isinstance(response_delta, ToolCallDelta):\n",
    "                cprint(response_delta.tool_call, color=\"magenta\", end='')\n",
    "    else:\n",
    "        cprint(response.completion_message.content, color=\"magenta\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "app-root",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
