{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 🐣 Document Intelligence with Docling: Unlocking Complex Academic Content\n",
    "\n",
    "This notebook demonstrates **Document Intelligence** - the advanced capability to understand and process complex documents like research papers, academic materials, and structured content that traditional RAG systems struggle with.\n",
    "\n",
    "**The Challenge:**\n",
    "Imagine trying to build an educational AI assistant using only basic text extraction from research papers. You'd lose:\n",
    "- **📊 Table data** with crucial research findings\n",
    "- **🧮 Mathematical formulas** and scientific notation  \n",
    "- **📈 Charts and figures** that provide key insights\n",
    "- **🏛️ Document structure** like sections, references, and metadata\n",
    "- **📝 Multi-column layouts** common in academic papers\n",
    "\n",
    "**The Solution: Docling**\n",
    "Docling is an advanced document processing toolkit that acts like a brilliant research assistant, understanding the **meaning and structure** of complex academic documents.\n",
    "\n",
    "**What You'll Build:**\n",
    "- **🔬 Intelligent Document Processor**: Extract rich content from complex PDFs\n",
    "- **📚 Enhanced RAG System**: Query tables, formulas, and structured content  \n",
    "- **🎯 Academic AI Assistant**: Answer questions using complete document understanding\n",
    "- **⚡ Production Pipeline**: Handle real-world educational materials at scale\n",
    "\n",
    "**Why This Matters:**\n",
    "Traditional RAG systems often fail with academic content, missing critical information trapped in tables or losing context from complex layouts. Docling transforms these challenging documents into fully searchable, queryable knowledge.\n",
    "\n",
    "Let's build document intelligence that truly understands academic content! 🚀\n",
    "\n",
    "## 📦 Install Required Packages\n",
    "\n",
    "Install packages for LlamaStack RAG and advanced document processing.\n",
    "\n",
    "**Note:** Docling processing can take 1-2 minutes for complex academic papers as it performs comprehensive analysis including layout detection, table extraction, and formula recognition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core libraries for document intelligence and RAG\n",
    "import uuid      # For generating unique vector database identifiers\n",
    "import requests  # For HTTP communication with Docling service and document fetching\n",
    "import base64    # For encoding binary data if needed (images, complex formats)\n",
    "import json      # For handling Docling API responses and metadata\n",
    "import os        # System utilities\n",
    "import sys       # System path management\n",
    "sys.path.append('..')  # Add parent directory for custom utilities\n",
    "\n",
    "# LlamaStack client and RAG-specific classes\n",
    "from llama_stack_client import LlamaStackClient  # Main interface for RAG operations\n",
    "from llama_stack_client import RAGDocument  # Represents documents for RAG ingestion\n",
    "from llama_stack_client.types.shared.content_delta import TextDelta, ToolCallDelta  # For streaming responses\n",
    "\n",
    "# Display and utility imports\n",
    "from src.utils import step_printer  # For progress tracking\n",
    "from termcolor import cprint        # For colorized output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected to LlamaStack server\n",
      "Model Configuration:\n",
      "  • Model: llama32\n",
      "  • Strategy: greedy\n",
      "  • Max Tokens: 4096 (enhanced for complex documents)\n",
      "  • Stream: True\n"
     ]
    }
   ],
   "source": [
    "# === LlamaStack Connection Setup ===\n",
    "# Connect to the LlamaStack server that coordinates document intelligence\n",
    "base_url = \"http://llama-stack-service:8321\"\n",
    "\n",
    "# Configure provider data (none needed for this demo)\n",
    "provider_data = None\n",
    "\n",
    "# Create the LlamaStack client for document intelligence and RAG\n",
    "client = LlamaStackClient(\n",
    "    base_url=base_url,\n",
    "    provider_data=provider_data\n",
    ")\n",
    "\n",
    "print(f\"Connected to LlamaStack server\")\n",
    "\n",
    "# === Model Configuration for Document Intelligence ===\n",
    "# Configure the LLM that will reason about processed documents\n",
    "model_id = \"llama32\"       # Llama 3.2 model for text generation\n",
    "temperature = 0.0         # Deterministic responses for factual document queries  \n",
    "max_tokens = 4096         # Larger context for complex document reasoning\n",
    "stream = True             # Stream responses for better user experience\n",
    "\n",
    "# Configure sampling strategy for consistent, factual responses\n",
    "if temperature > 0.0:\n",
    "    top_p = 0.95\n",
    "    strategy = {\"type\": \"top_p\", \"temperature\": temperature, \"top_p\": top_p}\n",
    "else:\n",
    "    strategy = {\"type\": \"greedy\"}  # Deterministic for factual document analysis\n",
    "\n",
    "# Package parameters for LlamaStack inference API\n",
    "sampling_params = {\n",
    "    \"strategy\": strategy,\n",
    "    \"max_tokens\": max_tokens,\n",
    "}\n",
    "\n",
    "# Display configuration\n",
    "print(f\"Model Configuration:\")\n",
    "print(f\"  • Model: {model_id}\")\n",
    "print(f\"  • Strategy: {strategy['type']}\")  \n",
    "print(f\"  • Max Tokens: {max_tokens} (enhanced for complex documents)\")\n",
    "print(f\"  • Stream: {stream}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🏗️ The Docling-LlamaStack Architecture\n",
    "\n",
    "Docling integrates with LlamaStack to create an intelligent document processing pipeline that transforms complex academic materials into searchable, queryable knowledge.\n",
    "\n",
    "### 🔍 What is Docling?\n",
    "\n",
    "**Docling** is an advanced document processing toolkit that simplifies the handling of diverse document formats with a focus on intelligent PDF understanding. Think of it as a universal translator for documents - it can read, understand, and convert complex academic materials into formats that AI systems can work with effectively.\n",
    "\n",
    "**Key Features & Capabilities:**\n",
    "- **📄 Multi-Format Support**: PDF, DOCX, PPTX, XLSX, HTML, WAV, MP3, PNG, TIFF, JPEG\n",
    "- **🧠 Intelligent PDF Understanding**: Layout analysis, table structure, formula recognition, image classification\n",
    "- **🔒 Enterprise-Ready**: Local execution, air-gapped support, extensive OCR, visual language models\n",
    "\n",
    "### 🔧 The Three-Phase Docling Pipeline\n",
    "\n",
    "Docling processes documents through three intelligent phases:\n",
    "\n",
    "#### Phase 1: Intelligent Document Analysis\n",
    "```\n",
    "📄 PDF Input → 🔍 Layout Detection → 📋 Structure Analysis → 🧠 Content Extraction\n",
    "```\n",
    "- **Layout Detection**: Understands page structure, reading order, and multi-column layouts\n",
    "- **Structure Analysis**: Identifies headers, paragraphs, lists, tables, and figures\n",
    "- **Content Extraction**: Extracts text while preserving semantic meaning and relationships\n",
    "\n",
    "#### Phase 2: Content Enhancement  \n",
    "```\n",
    "📝 Raw Text → 🏷️ Semantic Tagging → 📊 Table Extraction → 🖼️ Figure Processing\n",
    "```\n",
    "- **Semantic Tagging**: Identifies document sections, references, and metadata\n",
    "- **Table Extraction**: Preserves complex table relationships and formatting\n",
    "- **Figure Processing**: Handles mathematical equations, charts, and diagrams\n",
    "\n",
    "#### Phase 3: RAG Integration\n",
    "```\n",
    "🔧 Intelligent Chunking → 🎯 Embedding Generation → 🗄️ Vector Storage → 🔍 LlamaStack RAG\n",
    "```\n",
    "- **Intelligent Chunking**: Splits documents into optimal pieces for retrieval\n",
    "- **Embedding Generation**: Creates vector representations using sentence transformers\n",
    "- **Vector Storage**: Stores embeddings in your Milvus database\n",
    "- **LlamaStack RAG**: Enables semantic search and intelligent question answering\n",
    "\n",
    "### 🧠 Why Docling Matters for Educational RAG\n",
    "\n",
    "Traditional document processing often fails with academic content. Consider a typical computer science research paper:\n",
    "- **Complex layouts** with multiple columns and sections\n",
    "- **Mathematical equations** that need special handling\n",
    "- **Figures and tables** that provide crucial context\n",
    "- **Reference lists** that need to be preserved and linked\n",
    "- **Metadata** like authors, institutions, and publication dates\n",
    "\n",
    "Without intelligent processing, a RAG system might miss important information trapped in tables, lose context from figures, or struggle with multi-column layouts. Docling solves these problems by understanding document structure and extracting content intelligently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def docling_processing(url):\n",
    "    \"\"\"\n",
    "    Process a document URL using the Docling service for intelligent content extraction.\n",
    "    \n",
    "    This function performs advanced document analysis including:\n",
    "    - Layout detection and structure analysis\n",
    "    - Table extraction with preserved formatting  \n",
    "    - Mathematical formula recognition\n",
    "    - Figure and chart processing\n",
    "    - Multi-column layout understanding\n",
    "    - Semantic content structuring\n",
    "    \n",
    "    Args:\n",
    "        url (str): URL of the document to process (PDF, DOCX, etc.)\n",
    "        \n",
    "    Returns:\n",
    "        str: Structured Markdown content with preserved document intelligence\n",
    "        \n",
    "    Note: Processing can take 1-2 minutes for complex academic documents\n",
    "    \"\"\"\n",
    "    # === Docling Service Configuration ===\n",
    "    # Connect to the deployed Docling service in the cluster\n",
    "    api_address = \"http://docling-v0-7-0-predictor.ai501.svc.cluster.local:5001\"\n",
    "    \n",
    "    # Configure headers (no authentication needed for cluster-internal service)\n",
    "    headers = {\"Content-Type\": \"application/json\"}\n",
    "    \n",
    "    print(f\"🔗 Docling Service: {api_address}/v1alpha/convert/source\")\n",
    "    print(f\"📄 Processing document: {url}\")\n",
    "    print(f\"⏰ This may take 1-2 minutes for complex documents...\")\n",
    "    \n",
    "    # === Document Processing Request ===\n",
    "    # Configure Docling to extract maximum intelligence from the document\n",
    "    payload = {\n",
    "        \"http_sources\": [{\"url\": url}],              # Document source\n",
    "        \"options\": {\n",
    "            \"to_formats\": [\"md\"],                    # Output as structured Markdown\n",
    "            \"image_export_mode\": \"placeholder\"      # Handle images appropriately\n",
    "        },\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        # === Submit Processing Request ===\n",
    "        # Send document to Docling for intelligent analysis\n",
    "        response = requests.post(\n",
    "            f\"{api_address}/v1alpha/convert/source\",\n",
    "            json=payload,\n",
    "            headers=headers,\n",
    "            timeout=180  # 3-minute timeout for complex documents\n",
    "        )\n",
    "        \n",
    "        # Check for successful processing\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        # === Extract Processed Content ===\n",
    "        # Docling returns structured Markdown with preserved document intelligence\n",
    "        result_data = response.json()\n",
    "        md_content = result_data[\"document\"][\"md_content\"]\n",
    "        \n",
    "        print(f\"✅ Document processing complete!\")\n",
    "        print(f\"📊 Processed content length: {len(md_content)} characters\")\n",
    "        \n",
    "        return md_content\n",
    "        \n",
    "    except requests.exceptions.Timeout:\n",
    "        print(f\"⏰ Processing timeout - complex documents may need more time\")\n",
    "        raise\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"❌ Docling processing failed: {e}\")\n",
    "        raise\n",
    "    except KeyError as e:\n",
    "        print(f\"❌ Unexpected response format: {e}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎯 Selected document: https://arxiv.org/pdf/2404.14661\n",
      "📋 This paper likely contains tables, formulas, figures, and structured sections\n",
      "⚡ Starting intelligent document processing...\n",
      "🔗 Docling Service: http://docling-v0-7-0-predictor.ai501.svc.cluster.local:5001/v1alpha/convert/source\n",
      "📄 Processing document: https://arxiv.org/pdf/2404.14661\n",
      "⏰ This may take 1-2 minutes for complex documents...\n",
      "✅ Document processing complete!\n",
      "📊 Processed content length: 159318 characters\n",
      "\n",
      "🎉 Document intelligence processing complete!\n",
      "📊 Content preview (first 500 characters):\n",
      "============================================================\n",
      "## Highlights\n",
      "\n",
      "## First Mapping the Canopy Height of Primeval Forests in the Tallest Tree Area of Asia\n",
      "\n",
      "Guangpeng Fan,Fei Yan,Xiangquan Zeng,Qingtao Xu,Ruoyoulan Wang,Binghong Zhang,Jialing Zhou,Liangliang Nan,Jinhu Wang,Zhiwei Zhang,Jia Wang\n",
      "\n",
      "- • First mapping the primeval forest canopy height of the tallest tree growing in Asia\n",
      "- • Deep learning driven by multisource Earth observation to monitor the giant trees area\n",
      "- • Customized pyramid receptive field depth separable CNN, with an RMSE of 5....\n",
      "============================================================\n",
      "📈 Total processed content: 159318 characters\n",
      "📝 Docling has extracted and structured the complete document content!\n"
     ]
    }
   ],
   "source": [
    "# === Select Complex Academic Document ===\n",
    "# Choose a research paper with tables, formulas, and complex structure\n",
    "# This ArXiv paper contains the kind of complex content that showcases Docling's capabilities\n",
    "\n",
    "# Option 1: Computer Vision research paper with tables and technical content\n",
    "url = \"https://arxiv.org/pdf/2404.14661\"\n",
    "\n",
    "# Alternative papers for testing (comment/uncomment as needed):\n",
    "# url = \"https://arxiv.org/pdf/2006.07156\"  # Machine Learning paper with mathematical content\n",
    "# url = \"https://raw.githubusercontent.com/rhoai-genaiops/deploy-lab/main/university-data/canopy-in-botany.pdf\"  # Simpler PDF for comparison\n",
    "\n",
    "print(f\"🎯 Selected document: {url}\")\n",
    "print(f\"📋 This paper likely contains tables, formulas, figures, and structured sections\")\n",
    "print(f\"⚡ Starting intelligent document processing...\")\n",
    "\n",
    "# === Process Document with Docling Intelligence ===\n",
    "# This will take 1-2 minutes as Docling performs comprehensive analysis\n",
    "md_content = docling_processing(url)\n",
    "\n",
    "print(f\"\\n🎉 Document intelligence processing complete!\")\n",
    "print(f\"📊 Content preview (first 500 characters):\")\n",
    "print(f\"{'='*60}\")\n",
    "print(md_content[:500] + \"...\" if len(md_content) > 500 else md_content)\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"📈 Total processed content: {len(md_content)} characters\")\n",
    "print(f\"📝 Docling has extracted and structured the complete document content!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 📊 Document Processing Demonstration\n",
    "\n",
    "Let's test Docling's document intelligence on a complex academic paper. We'll use a real research paper that contains:\n",
    "- **📊 Tables** with numerical data and results\n",
    "- **🧮 Mathematical formulas** and equations  \n",
    "- **📈 Figures** and charts with captions\n",
    "- **📝 Multi-column layout** typical of academic papers\n",
    "- **🏛️ Structured sections** like Abstract, Methods, Results, References\n",
    "\n",
    "**Example Document:** We'll process an ArXiv research paper that demonstrates the full complexity of academic content that traditional text extraction would struggle with.\n",
    "\n",
    "### 🔬 Intelligent Processing in Action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://llama-stack-service:8321/v1/vector-dbs \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 Created vector database ID: test_vector_db_6e276e9b-94d1-42a0-9b7e-bf18273d2b21\n",
      "✅ Registered vector database for document intelligence:\n",
      "  • Database ID: test_vector_db_6e276e9b-94d1-42a0-9b7e-bf18273d2b21\n",
      "  • Embedding Model: all-MiniLM-L6-v2 (384 dimensions)\n",
      "  • Provider: Milvus vector database\n",
      "  • Ready for Docling-processed content ingestion!\n"
     ]
    }
   ],
   "source": [
    "# === STEP 1: Create Unique Vector Database ===\n",
    "# Generate a unique identifier for this vector database instance\n",
    "# Using UUID ensures no conflicts when multiple users run this notebook\n",
    "vector_db_id = f\"test_vector_db_{uuid.uuid4()}\"\n",
    "print(f\"📊 Created vector database ID: {vector_db_id}\")\n",
    "\n",
    "# === STEP 2: Register Vector Database for Document Intelligence ===\n",
    "# Configure the vector database to handle intelligently-processed documents\n",
    "client.vector_dbs.register(\n",
    "    vector_db_id=vector_db_id,                      # Unique identifier for this database\n",
    "    embedding_model=\"all-MiniLM-L6-v2\",            # Sentence transformer for embeddings\n",
    "    embedding_dimension=384,                        # Vector dimensions (must match model)\n",
    "    provider_id=\"milvus\",                           # Use Milvus as the vector store backend\n",
    ")\n",
    "\n",
    "print(f\"✅ Registered vector database for document intelligence:\")\n",
    "print(f\"  • Database ID: {vector_db_id}\")\n",
    "print(f\"  • Embedding Model: all-MiniLM-L6-v2 (384 dimensions)\")\n",
    "print(f\"  • Provider: Milvus vector database\")\n",
    "print(f\"  • Ready for Docling-processed content ingestion!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📚 Preparing to ingest intelligently-processed document:\n",
      "  • Document ID: docling-processed-doc\n",
      "  • Content length: 159318 characters\n",
      "  • Processing method: Docling document intelligence\n",
      "  • Content includes: tables, formulas, figures, and structured text\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://llama-stack-service:8321/v1/tool-runtime/rag-tool/insert \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Document ingestion complete!\n",
      "🎯 Docling-processed content is now searchable via semantic similarity!\n",
      "📊 Complex academic content (tables, formulas, figures) is now queryable!\n"
     ]
    }
   ],
   "source": [
    "# === STEP 3: Ingest Docling-Processed Content into RAG System ===\n",
    "# Create a RAGDocument object with the intelligently-processed content\n",
    "documents = [\n",
    "    RAGDocument(\n",
    "        document_id=f\"docling-processed-doc\",        # Unique identifier for this document\n",
    "        content=md_content,                          # The Docling-processed Markdown content\n",
    "        metadata={                                   # Enhanced metadata for complex documents\n",
    "            \"source_url\": url,                       # Original document URL\n",
    "            \"processing_method\": \"docling\",          # Processing pipeline used\n",
    "            \"document_type\": \"academic_paper\",       # Content classification\n",
    "            \"has_tables\": True,                      # Contains structured tabular data\n",
    "            \"has_formulas\": True,                    # Contains mathematical content\n",
    "            \"has_figures\": True,                     # Contains visual elements\n",
    "        },\n",
    "    )\n",
    "]\n",
    "\n",
    "print(f\"📚 Preparing to ingest intelligently-processed document:\")\n",
    "print(f\"  • Document ID: docling-processed-doc\")\n",
    "print(f\"  • Content length: {len(md_content)} characters\")\n",
    "print(f\"  • Processing method: Docling document intelligence\")\n",
    "print(f\"  • Content includes: tables, formulas, figures, and structured text\")\n",
    "\n",
    "# === STEP 4: Use LlamaStack RAG Tool for Intelligent Chunking ===\n",
    "# The RAG tool will automatically chunk the content optimally for retrieval\n",
    "try:\n",
    "    client.tool_runtime.rag_tool.insert(\n",
    "        documents=documents,                         # List of RAGDocument objects to process\n",
    "        vector_db_id=vector_db_id,                  # Target vector database\n",
    "        chunk_size_in_tokens=512,                   # Optimal chunk size for academic content\n",
    "    )\n",
    "    \n",
    "    print(f\"\\n✅ Document ingestion complete!\")\n",
    "    print(f\"🎯 Docling-processed content is now searchable via semantic similarity!\")\n",
    "    print(f\"📊 Complex academic content (tables, formulas, figures) is now queryable!\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\n❌ Document ingestion failed: {e}\")\n",
    "    print(f\"💡 Check Docling processing and vector database configuration\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🔍 Testing Document Intelligence RAG System\n",
    "\n",
    "Now let's test our enhanced RAG system with queries that showcase Docling's document intelligence capabilities. We'll ask questions that would require understanding of:\n",
    "- **📊 Tabular data** and structured information\n",
    "- **🧮 Mathematical content** and technical details\n",
    "- **📈 Research findings** and experimental results\n",
    "- **🏛️ Document structure** and relationships\n",
    "\n",
    "**The Power of Document Intelligence:**\n",
    "Traditional text extraction would miss most of this information, but Docling's intelligent processing preserves the meaning and structure that enables accurate, comprehensive answers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\n",
      "User> What is the PRFXception?\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://llama-stack-service:8321/v1/tool-runtime/rag-tool/query \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST http://llama-stack-service:8321/v1/inference/chat-completion \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QueryResult(metadata={'document_ids': ['docling-processed-doc', 'docling-processed-doc', 'docling-processed-doc', 'docling-processed-doc', 'docling-processed-doc'], 'chunks': [\" 256, num\\\\_sepconv\\\\_filters] as the feature extractor. This part is mainly responsible for the initial feature extraction and dimension transformation of the input data. Second, by building a series of separated convolution blocks sepconv\\\\_blocks. These blocks further process and extract features to better capture complex patterns and relationships in the input data, and separating convolutional structures helps improve the model's perception of local features. Finally, output is generated from three 1×1 convolutional layers predictions, variances and second\\\\_moments. These convolution layers are used to generate the model's predictions, variances, and second moments, respectively. PRFXception preserves the residual connection of Xception to mitigate the vanishing gradient problem(Chollet, 2017). These connections are between DSC blocks, facilitating smooth propagation of gradients and making the network easier to train. All blocks are residuals, and their inputs are also passed by skipping joins over that block, and are added to the output activation map so that the whole block learns additional residuals for the identity map. Skipping connections facilitates learning of very deep networks by creating shortcuts that prevent error gradients from disappearing before reaching earlier layers. PRFXception reduces the number of channels through 1×1 convolution to gather information, and then carries out feature extraction and pooling of different scales, and then superposes the features after obtaining the information of multiple scales. PRFXception makes heavy use of jump joins, does not use activation functions and BatchNorm on the jump join side, and it aligns the number of channels by 1×1 convolution. The combination of convolution blocks and separation convolution blocks improves the ability of the model to characterize multisource remote sensing data by stacking and combining these blocks. Through the final convolutional layer, multiple outputs are generated, and the network architecture that meets the requirements of the forest canopy height regression task in this paper is developed.\\n\\nPrevious studies have shown that the spatial information around pixels is very important to the performance of image classification algorithms. In many cases, a single pixel may not provide enough information to make an accurate prediction, especially if there is noise or spectral mixing. The multi-scale parallel calculation of PRFXception takes into account the spatial-spectral characteristics of each pixel of the fused spaceborne LiDAR image, the intensity of the light reflected, absorbed and emitted by the original forest with the unique geographical environment of the world at different wavelengths in 13 bands, and the physical and chemical properties of the forest are integrated in the convolution process(Fig. 4). In the local\", \" PRFXception has four parts, including 1×1 convolution, 3×3 convolution, 5×5 convolution, 7×7 convolution, and 3×3 maximum pooling. In the convolution operation, the convolution layer with different kernel size and the pooling layer are used in the way of padding='SAME' to ensure that the output feature image has the same size, so that the final result is combined on the Depth channel. Multiple convolution kernels are used to extract the information of different scales of the image and finally fuse it to obtain better representation of the image. Convolution kernel of different sizes (1×1, 3×3, 5×5, 7×7) are used respectively on the same layer. Convolution kernal of different sizes provide receptive fields of different scales, and features of different scales are extracted from the same layer. Features of different scales can be combined to produce more features than using a single size. The PRFXception is weighted to sum the responses of different scale receptive fields so that the neurons can synthesize input information from multiple scales (Eq. (1)).\\n\\n<!-- formula-not-decoded -->\\n\\nIn the convolutional neural network of a certain layer of PRFXception, there are a set of convolutional or pooled kernels of size k, which correspond to different scale receptive fields. Let 𝑓 𝑖 ( 𝑥 ) represent the response of the convolutional kernel or pooled check of the i scale to input x, and 𝐹 𝑥 ( ) represent the response of the layer's neurons to input x, then the response of the layer's neurons can be expressed as a summary of the responses of all scales.\\n\\nFirst, the entry part of the model is built by PointwiseBlock, which accepts the number of input channels in\\\\_channels, and uses a set of convolutional layers with channel numbers [128, 256, num\\\\_sepconv\\\\_filters] as the feature extractor. This part is mainly responsible for the initial feature extraction and dimension transformation of the input data. Second, by building a series of separated convolution blocks sepconv\\\\_blocks. These blocks further process and extract features to better capture complex patterns and relationships in the input data, and separating convolutional structures helps improve the model's perception of local features. Finally, output is generated from three 1×1 convolutional layers predictions, variances and second\\\\_moments. These convolution layers are used to generate the model's predictions, variances, and second moments, respectively. PR\", \" be obtained when\\n\\n## RMSE=8.59, MAE=6.59, ME=-3.45\\n\\nFig. 11. 10-fold cross-check validation evaluates the performance of PRFXception\\n\\n<!-- image -->\\n\\nFig. 12. Comparison of 5 band combinations\\n\\n<!-- image -->\\n\\nusing only RGB or woRGBN instead of all bands. However, assessing the significance of this result based on one test region is not statistically significant and further testing of different regions is required. Perhaps the clearest message of this study is that the use of near-infrared rays alone is not enough, and when we predict the height of the primary forest canopy, we cannot obtain enough multi-spectral satellite images, and selecting data with RGB band will be a potential alternative.\\n\\n## 4.4. The influence of multiple receptive field characteristics\\n\\nOur PRFXception was specifically customized and developed for remote sensing applications in search of worldlevel giant trees, and the biggest difference from regular CNNS is a deep regression network dedicated to predicting the height of primary forest canopy. PRFXception combines features of different scales to obtain more features than a single size. The multi-scale parallel structure not only enhances the ability of the network to capture features of different scales, but also improves the flexibility and robustness of the model. Fig. 13 represents the height contrast of the canopy in the same location drawn by the multi-receptive field model and the single receptive field model. The height detail of the multi-receptive field drawn in the first column is clearer, especially in the forest edge and spatial structure, which indicates that the introduction of the multi-receptive field mechanism can make the model better capture the complexity of terrain and space. In contrast, the height of the single receptive field is relatively blurred, and the forest edges, textures, and detailed features are not as obvious. At the same time, we can find that multiple receptive fields also\\n\\nOne reason for using CNN as a regressor is that we hypothesize that with Sentinel-2's resolution up to 10 meters, individual trees can be as large as a pixel footprint, and texture features may play an important role. By stacking many convolutional layers with filter cores of different scales, such as 1×1, 3×3, 5×5, 7×7, etc., PRFXception learns to pick up patterns of texture features in a larger sensory field centered on each pixel, if they are highly correlated with the canopy. To check if this is necessary, we set the space\", 'AT-2 in southeast Tibet. 465486 training samples were extracted from the CHM generated from the UAV-LS point clouds, and the predicted height of the corresponding PRFXception output was obtained. A total of 3281705 training samples were obtained in the experiment. Finally, training datasets including GEDI, ICESat-2, UAV-LS point clouds, and Sentinel-2 were produced to retrain PRFXception. We used the survey data of (GEDI, ICESat-2, UAV-LS), (GEDI, ICESat-2), (UAVLS) and 227 permanent sample sites as reference values, respectively, to verify the accuracy of driving PRFXception to predict canopy height after integrating training data set into UAV-LS point clouds. Fig. 18 shows the relationship between the predicted height values and the reference values.\\n\\nmost of the points are distributed near the diagonal, and as the reference height increases, the gap between the predicted value and the reference value gradually increases. Especially in the region of 30 ~40m, for the higher reference height, the model prediction height will be low and the accuracy will decline.\\n\\nThe following figure is the scatter density map obtained by using the UAV-LS reference value training model to predict the region, RMSE=5.75m, MAE=3.72m, ME=0.82m. Compared with GEDI &amp; ICESat-2 of \"4.1\" as a reference value, the reference height using UAV-LS has higher accuracy. From this scatter density plot, we can observe that\\n\\nSurprisingly, we found the geographic location of the tallest tree in Asia and its forest community in the canopy height map drawn by the UAV-LS point cloud-driven deep learning modeling on the basis of existing training data, and verified the geographical location of the tallest tree in Asia. And its forest community through our field investigation as a reference value. This result provides a valuable reference for us to discover more world level giant tree individuals and communities. On the basis of 10m resolution mapping of primary forest canopy height by PRFXception, we produced a vector map of potential distribution of giant trees in the fourth world-level giant tree distribution area (Fig. 19). To find as many world-level giant tree individuals and communities as possible, the reference height value and predicted value were used for inter-zone statistics, and the accuracy of each interval was calculated as the possibility index. Potential giant trees are mainly distributed in river valleys', ' the PRFXception model through cross-validation. We divide the dataset into 10 mutually exclusive subsets, 9 of which serve as the training set and the remaining 1 as the validation set, and repeat 10 times, with each subset used as the test set once. To obtain a more robust estimate of model performance. The experiment produced a total of 10 models, and we counted the prediction results of each model separately. Finally, the model performance was evaluated using the rootmean-square error RMSE, MAE, and ME (Fayad et al., 2021). In this study, random cross-validation on this dataset produced RMSE=8.59m, MAE=6.59m, ME=-3.45m (Fig.\\n\\n## RMSE=6.75, MAE=5.56, ME=2.14\\n\\n<!-- image -->\\n\\n- Pinaceae Spreng ex F.Rudolphi(2000~3000m)\\n- Salicaceae Mirb.(3000~4000m)\\n- Mixed tree species(3000~40O0m)\\n\\nFagaceae Dumort.(3000~4000m)\\n\\nMixed tree species(&gt;40OOm)\\n\\n- Other soft broad categories(3000~4OOOm)\\n\\nTable 1 RMSE,MAE,MEvaluesgenerated by different band combinations\\n\\n| RGB    | 6.32   |   5.32 | 0.43   |\\n|--------|--------|--------|--------|\\n| N      | 11.91  |  10.07 |        |\\n|        | 6.85   |   5.77 |        |\\n| woRGBN | 6.34   |   5.36 |        |\\n|        |        |   5.51 | -0.73  |\\n\\nband\\n\\nRMSE\\n\\nMAE\\n\\nME\\n\\nFig. 12 shows the results of using different spectral bands to draw the forest canopy height at the same location. It can be found that the canopy height drawn by the data set containing RGB band provides more abundant information contouring, texture, tone and shape, while only N or the introduction of short-wave infrared will reduce the canopy height rendering effect.\\n\\nWechanged the number of input band channels, training them separately while keeping all other configurations of the PRFXception unchanged. Table 1 shows the performance evaluation of canopy height regression accuracy for different data sets (RGB, N, RGBN, woRGBN,'], 'scores': [0.46592146158218384, 0.4288713335990906, 0.30390211939811707, 0.29446688294410706, 0.29117751121520996]}, content=[TextContentItem(text='knowledge_search tool found 5 chunks:\\nBEGIN of knowledge_search tool results.\\n', type='text'), TextContentItem(text=\"Result 1\\nContent:  256, num\\\\_sepconv\\\\_filters] as the feature extractor. This part is mainly responsible for the initial feature extraction and dimension transformation of the input data. Second, by building a series of separated convolution blocks sepconv\\\\_blocks. These blocks further process and extract features to better capture complex patterns and relationships in the input data, and separating convolutional structures helps improve the model's perception of local features. Finally, output is generated from three 1×1 convolutional layers predictions, variances and second\\\\_moments. These convolution layers are used to generate the model's predictions, variances, and second moments, respectively. PRFXception preserves the residual connection of Xception to mitigate the vanishing gradient problem(Chollet, 2017). These connections are between DSC blocks, facilitating smooth propagation of gradients and making the network easier to train. All blocks are residuals, and their inputs are also passed by skipping joins over that block, and are added to the output activation map so that the whole block learns additional residuals for the identity map. Skipping connections facilitates learning of very deep networks by creating shortcuts that prevent error gradients from disappearing before reaching earlier layers. PRFXception reduces the number of channels through 1×1 convolution to gather information, and then carries out feature extraction and pooling of different scales, and then superposes the features after obtaining the information of multiple scales. PRFXception makes heavy use of jump joins, does not use activation functions and BatchNorm on the jump join side, and it aligns the number of channels by 1×1 convolution. The combination of convolution blocks and separation convolution blocks improves the ability of the model to characterize multisource remote sensing data by stacking and combining these blocks. Through the final convolutional layer, multiple outputs are generated, and the network architecture that meets the requirements of the forest canopy height regression task in this paper is developed.\\n\\nPrevious studies have shown that the spatial information around pixels is very important to the performance of image classification algorithms. In many cases, a single pixel may not provide enough information to make an accurate prediction, especially if there is noise or spectral mixing. The multi-scale parallel calculation of PRFXception takes into account the spatial-spectral characteristics of each pixel of the fused spaceborne LiDAR image, the intensity of the light reflected, absorbed and emitted by the original forest with the unique geographical environment of the world at different wavelengths in 13 bands, and the physical and chemical properties of the forest are integrated in the convolution process(Fig. 4). In the local\\nMetadata: {'source_url': 'https://arxiv.org/pdf/2404.14661', 'processing_method': 'docling', 'document_type': 'academic_paper', 'has_tables': True, 'has_formulas': True, 'has_figures': True, 'document_id': 'docling-processed-doc'}\\n\", type='text'), TextContentItem(text=\"Result 2\\nContent:  PRFXception has four parts, including 1×1 convolution, 3×3 convolution, 5×5 convolution, 7×7 convolution, and 3×3 maximum pooling. In the convolution operation, the convolution layer with different kernel size and the pooling layer are used in the way of padding='SAME' to ensure that the output feature image has the same size, so that the final result is combined on the Depth channel. Multiple convolution kernels are used to extract the information of different scales of the image and finally fuse it to obtain better representation of the image. Convolution kernel of different sizes (1×1, 3×3, 5×5, 7×7) are used respectively on the same layer. Convolution kernal of different sizes provide receptive fields of different scales, and features of different scales are extracted from the same layer. Features of different scales can be combined to produce more features than using a single size. The PRFXception is weighted to sum the responses of different scale receptive fields so that the neurons can synthesize input information from multiple scales (Eq. (1)).\\n\\n<!-- formula-not-decoded -->\\n\\nIn the convolutional neural network of a certain layer of PRFXception, there are a set of convolutional or pooled kernels of size k, which correspond to different scale receptive fields. Let 𝑓 𝑖 ( 𝑥 ) represent the response of the convolutional kernel or pooled check of the i scale to input x, and 𝐹 𝑥 ( ) represent the response of the layer's neurons to input x, then the response of the layer's neurons can be expressed as a summary of the responses of all scales.\\n\\nFirst, the entry part of the model is built by PointwiseBlock, which accepts the number of input channels in\\\\_channels, and uses a set of convolutional layers with channel numbers [128, 256, num\\\\_sepconv\\\\_filters] as the feature extractor. This part is mainly responsible for the initial feature extraction and dimension transformation of the input data. Second, by building a series of separated convolution blocks sepconv\\\\_blocks. These blocks further process and extract features to better capture complex patterns and relationships in the input data, and separating convolutional structures helps improve the model's perception of local features. Finally, output is generated from three 1×1 convolutional layers predictions, variances and second\\\\_moments. These convolution layers are used to generate the model's predictions, variances, and second moments, respectively. PR\\nMetadata: {'source_url': 'https://arxiv.org/pdf/2404.14661', 'processing_method': 'docling', 'document_type': 'academic_paper', 'has_tables': True, 'has_formulas': True, 'has_figures': True, 'document_id': 'docling-processed-doc'}\\n\", type='text'), TextContentItem(text=\"Result 3\\nContent:  be obtained when\\n\\n## RMSE=8.59, MAE=6.59, ME=-3.45\\n\\nFig. 11. 10-fold cross-check validation evaluates the performance of PRFXception\\n\\n<!-- image -->\\n\\nFig. 12. Comparison of 5 band combinations\\n\\n<!-- image -->\\n\\nusing only RGB or woRGBN instead of all bands. However, assessing the significance of this result based on one test region is not statistically significant and further testing of different regions is required. Perhaps the clearest message of this study is that the use of near-infrared rays alone is not enough, and when we predict the height of the primary forest canopy, we cannot obtain enough multi-spectral satellite images, and selecting data with RGB band will be a potential alternative.\\n\\n## 4.4. The influence of multiple receptive field characteristics\\n\\nOur PRFXception was specifically customized and developed for remote sensing applications in search of worldlevel giant trees, and the biggest difference from regular CNNS is a deep regression network dedicated to predicting the height of primary forest canopy. PRFXception combines features of different scales to obtain more features than a single size. The multi-scale parallel structure not only enhances the ability of the network to capture features of different scales, but also improves the flexibility and robustness of the model. Fig. 13 represents the height contrast of the canopy in the same location drawn by the multi-receptive field model and the single receptive field model. The height detail of the multi-receptive field drawn in the first column is clearer, especially in the forest edge and spatial structure, which indicates that the introduction of the multi-receptive field mechanism can make the model better capture the complexity of terrain and space. In contrast, the height of the single receptive field is relatively blurred, and the forest edges, textures, and detailed features are not as obvious. At the same time, we can find that multiple receptive fields also\\n\\nOne reason for using CNN as a regressor is that we hypothesize that with Sentinel-2's resolution up to 10 meters, individual trees can be as large as a pixel footprint, and texture features may play an important role. By stacking many convolutional layers with filter cores of different scales, such as 1×1, 3×3, 5×5, 7×7, etc., PRFXception learns to pick up patterns of texture features in a larger sensory field centered on each pixel, if they are highly correlated with the canopy. To check if this is necessary, we set the space\\nMetadata: {'source_url': 'https://arxiv.org/pdf/2404.14661', 'processing_method': 'docling', 'document_type': 'academic_paper', 'has_tables': True, 'has_formulas': True, 'has_figures': True, 'document_id': 'docling-processed-doc'}\\n\", type='text'), TextContentItem(text='Result 4\\nContent: AT-2 in southeast Tibet. 465486 training samples were extracted from the CHM generated from the UAV-LS point clouds, and the predicted height of the corresponding PRFXception output was obtained. A total of 3281705 training samples were obtained in the experiment. Finally, training datasets including GEDI, ICESat-2, UAV-LS point clouds, and Sentinel-2 were produced to retrain PRFXception. We used the survey data of (GEDI, ICESat-2, UAV-LS), (GEDI, ICESat-2), (UAVLS) and 227 permanent sample sites as reference values, respectively, to verify the accuracy of driving PRFXception to predict canopy height after integrating training data set into UAV-LS point clouds. Fig. 18 shows the relationship between the predicted height values and the reference values.\\n\\nmost of the points are distributed near the diagonal, and as the reference height increases, the gap between the predicted value and the reference value gradually increases. Especially in the region of 30 ~40m, for the higher reference height, the model prediction height will be low and the accuracy will decline.\\n\\nThe following figure is the scatter density map obtained by using the UAV-LS reference value training model to predict the region, RMSE=5.75m, MAE=3.72m, ME=0.82m. Compared with GEDI &amp; ICESat-2 of \"4.1\" as a reference value, the reference height using UAV-LS has higher accuracy. From this scatter density plot, we can observe that\\n\\nSurprisingly, we found the geographic location of the tallest tree in Asia and its forest community in the canopy height map drawn by the UAV-LS point cloud-driven deep learning modeling on the basis of existing training data, and verified the geographical location of the tallest tree in Asia. And its forest community through our field investigation as a reference value. This result provides a valuable reference for us to discover more world level giant tree individuals and communities. On the basis of 10m resolution mapping of primary forest canopy height by PRFXception, we produced a vector map of potential distribution of giant trees in the fourth world-level giant tree distribution area (Fig. 19). To find as many world-level giant tree individuals and communities as possible, the reference height value and predicted value were used for inter-zone statistics, and the accuracy of each interval was calculated as the possibility index. Potential giant trees are mainly distributed in river valleys\\nMetadata: {\\'source_url\\': \\'https://arxiv.org/pdf/2404.14661\\', \\'processing_method\\': \\'docling\\', \\'document_type\\': \\'academic_paper\\', \\'has_tables\\': True, \\'has_formulas\\': True, \\'has_figures\\': True, \\'document_id\\': \\'docling-processed-doc\\'}\\n', type='text'), TextContentItem(text=\"Result 5\\nContent:  the PRFXception model through cross-validation. We divide the dataset into 10 mutually exclusive subsets, 9 of which serve as the training set and the remaining 1 as the validation set, and repeat 10 times, with each subset used as the test set once. To obtain a more robust estimate of model performance. The experiment produced a total of 10 models, and we counted the prediction results of each model separately. Finally, the model performance was evaluated using the rootmean-square error RMSE, MAE, and ME (Fayad et al., 2021). In this study, random cross-validation on this dataset produced RMSE=8.59m, MAE=6.59m, ME=-3.45m (Fig.\\n\\n## RMSE=6.75, MAE=5.56, ME=2.14\\n\\n<!-- image -->\\n\\n- Pinaceae Spreng ex F.Rudolphi(2000~3000m)\\n- Salicaceae Mirb.(3000~4000m)\\n- Mixed tree species(3000~40O0m)\\n\\nFagaceae Dumort.(3000~4000m)\\n\\nMixed tree species(&gt;40OOm)\\n\\n- Other soft broad categories(3000~4OOOm)\\n\\nTable 1 RMSE,MAE,MEvaluesgenerated by different band combinations\\n\\n| RGB    | 6.32   |   5.32 | 0.43   |\\n|--------|--------|--------|--------|\\n| N      | 11.91  |  10.07 |        |\\n|        | 6.85   |   5.77 |        |\\n| woRGBN | 6.34   |   5.36 |        |\\n|        |        |   5.51 | -0.73  |\\n\\nband\\n\\nRMSE\\n\\nMAE\\n\\nME\\n\\nFig. 12 shows the results of using different spectral bands to draw the forest canopy height at the same location. It can be found that the canopy height drawn by the data set containing RGB band provides more abundant information contouring, texture, tone and shape, while only N or the introduction of short-wave infrared will reduce the canopy height rendering effect.\\n\\nWechanged the number of input band channels, training them separately while keeping all other configurations of the PRFXception unchanged. Table 1 shows the performance evaluation of canopy height regression accuracy for different data sets (RGB, N, RGBN, woRGBN,\\nMetadata: {'source_url': 'https://arxiv.org/pdf/2404.14661', 'processing_method': 'docling', 'document_type': 'academic_paper', 'has_tables': True, 'has_formulas': True, 'has_figures': True, 'document_id': 'docling-processed-doc'}\\n\", type='text'), TextContentItem(text='END of knowledge_search tool results.\\n', type='text'), TextContentItem(text='The above results were retrieved to help answer the user\\'s query: \"What is the PRFXception?\". Use them as supporting information only in answering this query.\\n', type='text')])\u001b[0m\n",
      "\u001b[33m\n",
      "--- RAG Metadata ---\u001b[0m\n",
      "\u001b[36m{'document_ids': ['docling-processed-doc', 'docling-processed-doc', 'docling-processed-doc', 'docling-processed-doc', 'docling-processed-doc'], 'chunks': [\" 256, num\\\\_sepconv\\\\_filters] as the feature extractor. This part is mainly responsible for the initial feature extraction and dimension transformation of the input data. Second, by building a series of separated convolution blocks sepconv\\\\_blocks. These blocks further process and extract features to better capture complex patterns and relationships in the input data, and separating convolutional structures helps improve the model's perception of local features. Finally, output is generated from three 1×1 convolutional layers predictions, variances and second\\\\_moments. These convolution layers are used to generate the model's predictions, variances, and second moments, respectively. PRFXception preserves the residual connection of Xception to mitigate the vanishing gradient problem(Chollet, 2017). These connections are between DSC blocks, facilitating smooth propagation of gradients and making the network easier to train. All blocks are residuals, and their inputs are also passed by skipping joins over that block, and are added to the output activation map so that the whole block learns additional residuals for the identity map. Skipping connections facilitates learning of very deep networks by creating shortcuts that prevent error gradients from disappearing before reaching earlier layers. PRFXception reduces the number of channels through 1×1 convolution to gather information, and then carries out feature extraction and pooling of different scales, and then superposes the features after obtaining the information of multiple scales. PRFXception makes heavy use of jump joins, does not use activation functions and BatchNorm on the jump join side, and it aligns the number of channels by 1×1 convolution. The combination of convolution blocks and separation convolution blocks improves the ability of the model to characterize multisource remote sensing data by stacking and combining these blocks. Through the final convolutional layer, multiple outputs are generated, and the network architecture that meets the requirements of the forest canopy height regression task in this paper is developed.\\n\\nPrevious studies have shown that the spatial information around pixels is very important to the performance of image classification algorithms. In many cases, a single pixel may not provide enough information to make an accurate prediction, especially if there is noise or spectral mixing. The multi-scale parallel calculation of PRFXception takes into account the spatial-spectral characteristics of each pixel of the fused spaceborne LiDAR image, the intensity of the light reflected, absorbed and emitted by the original forest with the unique geographical environment of the world at different wavelengths in 13 bands, and the physical and chemical properties of the forest are integrated in the convolution process(Fig. 4). In the local\", \" PRFXception has four parts, including 1×1 convolution, 3×3 convolution, 5×5 convolution, 7×7 convolution, and 3×3 maximum pooling. In the convolution operation, the convolution layer with different kernel size and the pooling layer are used in the way of padding='SAME' to ensure that the output feature image has the same size, so that the final result is combined on the Depth channel. Multiple convolution kernels are used to extract the information of different scales of the image and finally fuse it to obtain better representation of the image. Convolution kernel of different sizes (1×1, 3×3, 5×5, 7×7) are used respectively on the same layer. Convolution kernal of different sizes provide receptive fields of different scales, and features of different scales are extracted from the same layer. Features of different scales can be combined to produce more features than using a single size. The PRFXception is weighted to sum the responses of different scale receptive fields so that the neurons can synthesize input information from multiple scales (Eq. (1)).\\n\\n<!-- formula-not-decoded -->\\n\\nIn the convolutional neural network of a certain layer of PRFXception, there are a set of convolutional or pooled kernels of size k, which correspond to different scale receptive fields. Let 𝑓 𝑖 ( 𝑥 ) represent the response of the convolutional kernel or pooled check of the i scale to input x, and 𝐹 𝑥 ( ) represent the response of the layer's neurons to input x, then the response of the layer's neurons can be expressed as a summary of the responses of all scales.\\n\\nFirst, the entry part of the model is built by PointwiseBlock, which accepts the number of input channels in\\\\_channels, and uses a set of convolutional layers with channel numbers [128, 256, num\\\\_sepconv\\\\_filters] as the feature extractor. This part is mainly responsible for the initial feature extraction and dimension transformation of the input data. Second, by building a series of separated convolution blocks sepconv\\\\_blocks. These blocks further process and extract features to better capture complex patterns and relationships in the input data, and separating convolutional structures helps improve the model's perception of local features. Finally, output is generated from three 1×1 convolutional layers predictions, variances and second\\\\_moments. These convolution layers are used to generate the model's predictions, variances, and second moments, respectively. PR\", \" be obtained when\\n\\n## RMSE=8.59, MAE=6.59, ME=-3.45\\n\\nFig. 11. 10-fold cross-check validation evaluates the performance of PRFXception\\n\\n<!-- image -->\\n\\nFig. 12. Comparison of 5 band combinations\\n\\n<!-- image -->\\n\\nusing only RGB or woRGBN instead of all bands. However, assessing the significance of this result based on one test region is not statistically significant and further testing of different regions is required. Perhaps the clearest message of this study is that the use of near-infrared rays alone is not enough, and when we predict the height of the primary forest canopy, we cannot obtain enough multi-spectral satellite images, and selecting data with RGB band will be a potential alternative.\\n\\n## 4.4. The influence of multiple receptive field characteristics\\n\\nOur PRFXception was specifically customized and developed for remote sensing applications in search of worldlevel giant trees, and the biggest difference from regular CNNS is a deep regression network dedicated to predicting the height of primary forest canopy. PRFXception combines features of different scales to obtain more features than a single size. The multi-scale parallel structure not only enhances the ability of the network to capture features of different scales, but also improves the flexibility and robustness of the model. Fig. 13 represents the height contrast of the canopy in the same location drawn by the multi-receptive field model and the single receptive field model. The height detail of the multi-receptive field drawn in the first column is clearer, especially in the forest edge and spatial structure, which indicates that the introduction of the multi-receptive field mechanism can make the model better capture the complexity of terrain and space. In contrast, the height of the single receptive field is relatively blurred, and the forest edges, textures, and detailed features are not as obvious. At the same time, we can find that multiple receptive fields also\\n\\nOne reason for using CNN as a regressor is that we hypothesize that with Sentinel-2's resolution up to 10 meters, individual trees can be as large as a pixel footprint, and texture features may play an important role. By stacking many convolutional layers with filter cores of different scales, such as 1×1, 3×3, 5×5, 7×7, etc., PRFXception learns to pick up patterns of texture features in a larger sensory field centered on each pixel, if they are highly correlated with the canopy. To check if this is necessary, we set the space\", 'AT-2 in southeast Tibet. 465486 training samples were extracted from the CHM generated from the UAV-LS point clouds, and the predicted height of the corresponding PRFXception output was obtained. A total of 3281705 training samples were obtained in the experiment. Finally, training datasets including GEDI, ICESat-2, UAV-LS point clouds, and Sentinel-2 were produced to retrain PRFXception. We used the survey data of (GEDI, ICESat-2, UAV-LS), (GEDI, ICESat-2), (UAVLS) and 227 permanent sample sites as reference values, respectively, to verify the accuracy of driving PRFXception to predict canopy height after integrating training data set into UAV-LS point clouds. Fig. 18 shows the relationship between the predicted height values and the reference values.\\n\\nmost of the points are distributed near the diagonal, and as the reference height increases, the gap between the predicted value and the reference value gradually increases. Especially in the region of 30 ~40m, for the higher reference height, the model prediction height will be low and the accuracy will decline.\\n\\nThe following figure is the scatter density map obtained by using the UAV-LS reference value training model to predict the region, RMSE=5.75m, MAE=3.72m, ME=0.82m. Compared with GEDI &amp; ICESat-2 of \"4.1\" as a reference value, the reference height using UAV-LS has higher accuracy. From this scatter density plot, we can observe that\\n\\nSurprisingly, we found the geographic location of the tallest tree in Asia and its forest community in the canopy height map drawn by the UAV-LS point cloud-driven deep learning modeling on the basis of existing training data, and verified the geographical location of the tallest tree in Asia. And its forest community through our field investigation as a reference value. This result provides a valuable reference for us to discover more world level giant tree individuals and communities. On the basis of 10m resolution mapping of primary forest canopy height by PRFXception, we produced a vector map of potential distribution of giant trees in the fourth world-level giant tree distribution area (Fig. 19). To find as many world-level giant tree individuals and communities as possible, the reference height value and predicted value were used for inter-zone statistics, and the accuracy of each interval was calculated as the possibility index. Potential giant trees are mainly distributed in river valleys', ' the PRFXception model through cross-validation. We divide the dataset into 10 mutually exclusive subsets, 9 of which serve as the training set and the remaining 1 as the validation set, and repeat 10 times, with each subset used as the test set once. To obtain a more robust estimate of model performance. The experiment produced a total of 10 models, and we counted the prediction results of each model separately. Finally, the model performance was evaluated using the rootmean-square error RMSE, MAE, and ME (Fayad et al., 2021). In this study, random cross-validation on this dataset produced RMSE=8.59m, MAE=6.59m, ME=-3.45m (Fig.\\n\\n## RMSE=6.75, MAE=5.56, ME=2.14\\n\\n<!-- image -->\\n\\n- Pinaceae Spreng ex F.Rudolphi(2000~3000m)\\n- Salicaceae Mirb.(3000~4000m)\\n- Mixed tree species(3000~40O0m)\\n\\nFagaceae Dumort.(3000~4000m)\\n\\nMixed tree species(&gt;40OOm)\\n\\n- Other soft broad categories(3000~4OOOm)\\n\\nTable 1 RMSE,MAE,MEvaluesgenerated by different band combinations\\n\\n| RGB    | 6.32   |   5.32 | 0.43   |\\n|--------|--------|--------|--------|\\n| N      | 11.91  |  10.07 |        |\\n|        | 6.85   |   5.77 |        |\\n| woRGBN | 6.34   |   5.36 |        |\\n|        |        |   5.51 | -0.73  |\\n\\nband\\n\\nRMSE\\n\\nMAE\\n\\nME\\n\\nFig. 12 shows the results of using different spectral bands to draw the forest canopy height at the same location. It can be found that the canopy height drawn by the data set containing RGB band provides more abundant information contouring, texture, tone and shape, while only N or the introduction of short-wave infrared will reduce the canopy height rendering effect.\\n\\nWechanged the number of input band channels, training them separately while keeping all other configurations of the PRFXception unchanged. Table 1 shows the performance evaluation of canopy height regression accuracy for different data sets (RGB, N, RGBN, woRGBN,'], 'scores': [0.46592146158218384, 0.4288713335990906, 0.30390211939811707, 0.29446688294410706, 0.29117751121520996]}\u001b[0m\n",
      "\u001b[35minference> \u001b[0m\u001b[35m\u001b[0m\u001b[35mThe\u001b[0m\u001b[35m PR\u001b[0m\u001b[35mFX\u001b[0m\u001b[35mception\u001b[0m\u001b[35m is\u001b[0m\u001b[35m a\u001b[0m\u001b[35m deep\u001b[0m\u001b[35m learning\u001b[0m\u001b[35m model\u001b[0m\u001b[35m specifically\u001b[0m\u001b[35m designed\u001b[0m\u001b[35m for\u001b[0m\u001b[35m remote\u001b[0m\u001b[35m sensing\u001b[0m\u001b[35m applications\u001b[0m\u001b[35m,\u001b[0m\u001b[35m particularly\u001b[0m\u001b[35m for\u001b[0m\u001b[35m predicting\u001b[0m\u001b[35m the\u001b[0m\u001b[35m height\u001b[0m\u001b[35m of\u001b[0m\u001b[35m primary\u001b[0m\u001b[35m forest\u001b[0m\u001b[35m can\u001b[0m\u001b[35mopies\u001b[0m\u001b[35m.\u001b[0m\u001b[35m It\u001b[0m\u001b[35m combines\u001b[0m\u001b[35m features\u001b[0m\u001b[35m of\u001b[0m\u001b[35m different\u001b[0m\u001b[35m scales\u001b[0m\u001b[35m to\u001b[0m\u001b[35m obtain\u001b[0m\u001b[35m more\u001b[0m\u001b[35m features\u001b[0m\u001b[35m than\u001b[0m\u001b[35m a\u001b[0m\u001b[35m single\u001b[0m\u001b[35m size\u001b[0m\u001b[35m,\u001b[0m\u001b[35m and\u001b[0m\u001b[35m its\u001b[0m\u001b[35m multi\u001b[0m\u001b[35m-scale\u001b[0m\u001b[35m parallel\u001b[0m\u001b[35m structure\u001b[0m\u001b[35m enhances\u001b[0m\u001b[35m the\u001b[0m\u001b[35m ability\u001b[0m\u001b[35m of\u001b[0m\u001b[35m the\u001b[0m\u001b[35m network\u001b[0m\u001b[35m to\u001b[0m\u001b[35m capture\u001b[0m\u001b[35m features\u001b[0m\u001b[35m of\u001b[0m\u001b[35m different\u001b[0m\u001b[35m scales\u001b[0m\u001b[35m.\u001b[0m\u001b[35m The\u001b[0m\u001b[35m model\u001b[0m\u001b[35m uses\u001b[0m\u001b[35m a\u001b[0m\u001b[35m series\u001b[0m\u001b[35m of\u001b[0m\u001b[35m separated\u001b[0m\u001b[35m convolution\u001b[0m\u001b[35m blocks\u001b[0m\u001b[35m and\u001b[0m\u001b[35m \u001b[0m\u001b[35m1\u001b[0m\u001b[35m×\u001b[0m\u001b[35m1\u001b[0m\u001b[35m convolution\u001b[0m\u001b[35mal\u001b[0m\u001b[35m layers\u001b[0m\u001b[35m to\u001b[0m\u001b[35m generate\u001b[0m\u001b[35m predictions\u001b[0m\u001b[35m,\u001b[0m\u001b[35m var\u001b[0m\u001b[35miances\u001b[0m\u001b[35m,\u001b[0m\u001b[35m and\u001b[0m\u001b[35m second\u001b[0m\u001b[35m moments\u001b[0m\u001b[35m,\u001b[0m\u001b[35m respectively\u001b[0m\u001b[35m.\u001b[0m\u001b[35m It\u001b[0m\u001b[35m also\u001b[0m\u001b[35m preserves\u001b[0m\u001b[35m the\u001b[0m\u001b[35m residual\u001b[0m\u001b[35m connection\u001b[0m\u001b[35m of\u001b[0m\u001b[35m X\u001b[0m\u001b[35mception\u001b[0m\u001b[35m to\u001b[0m\u001b[35m mitigate\u001b[0m\u001b[35m the\u001b[0m\u001b[35m van\u001b[0m\u001b[35mishing\u001b[0m\u001b[35m gradient\u001b[0m\u001b[35m problem\u001b[0m\u001b[35m.\u001b[0m\u001b[35m\u001b[0m\u001b[34m\n",
      "--- End of RAG Answer ---\u001b[0m\n",
      "\u001b[34m\n",
      "User> The accuracy values of overall model prediction and residual cross-validation for five regions in southeast Tibet and four regions in northwest Yunnan\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://llama-stack-service:8321/v1/tool-runtime/rag-tool/query \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST http://llama-stack-service:8321/v1/inference/chat-completion \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QueryResult(metadata={'document_ids': ['docling-processed-doc', 'docling-processed-doc', 'docling-processed-doc', 'docling-processed-doc', 'docling-processed-doc'], 'chunks': [' whether the model overfits the training location or whether it supports common features such as making predictions at unseen locations. We conducted geographical cross-validation for 5 regions in southeast Tibet and 4 regions in northwest Yunnan respectively (Fig. 5), which will produce 5 times and 4 times cross-validation. We conducted two types of geographic cross-validation. In the first, we trained two separate regions, southeast Tibet and northwest Yunnan, and then cross-predicted their respective regions. The second method is to cross-verify the five selected regions in southeast Tibet and four regions in northwest Yunnan. All but one area is trained, and the remaining test area is predicted so that the training data near the test area is not visible during the training period.\\n\\n## 3.6. Evaluation index\\n\\nWe use the root mean square error (RMSE) as the primary error measure to quantify the deviation between the predicted height and the reference height. The mean error (ME) is used to measure the performance of a training model evaluated over an invisible test area. For completeness and to allow comparisons with other studies, we also report the mean absolute error (MAE).\\n\\n<!-- formula-not-decoded -->\\n\\n<!-- formula-not-decoded -->\\n\\n<!-- formula-not-decoded -->\\n\\nwhere 𝑓 𝑥 𝑖 ( ) represents the predicted value of the model set, and 𝑦 𝑖 represents the reference value at sample 𝑖 . According to this definition, a positive ME (mean square error) means that the value predicted by the model is higher than the true value of the reference data.\\n\\nFig. 5. Geographical cross-validation region selection distribution map\\n\\n<!-- image -->\\n\\n## 4. Results and discussion\\n\\n4.1.1. Validation of reference data for GEDI and ICESat-2\\n\\n4.1. Canopy height regression\\n\\nWe use three different combinations of training data as sparse supervised samples to train PRFXception and predict height separately. The data combination types were ICESat-2, GEDI, and ICESat-2 fusion GEDI, respectively, and sample labels were used as reference data to verify the accuracy of high regression. The results show that the canopy height predicted by PRFXception is in good agreement with the reference data. By comparison, it can be seen that the combination effect of ICESat-2 &amp; GEDI is better, 𝑅𝑀𝑆𝐸 =7.56m, 𝑀�', ' ranges from 6.15m to 9.77m, MAE ranges from 5.17m to 8.35m, and ME ranges from -0.55m to 3.75m in southeast Tibet. In northwestern Yunnan, RMSE ranges from 5.43m to 7.19m, MAE ranges from 4.60m to 6.20m, and ME ranges from 3.96m to -0.20m. It can be seen that the overall prediction effect in southeast Tibet is lower than that in northwest Yunnan, which may be due to the greater elevation difference in southeast Tibet. For southeast Tibet, PRFXception showed higher accuracy in RLM area, with lower RMSE, MAE and ME, and better prediction effect. The overall error deviation of PRFXception in northwest Yunnan is low, and the uncertainty in RNM is relatively high, which may be due to the relatively high canopy or elevation of Shangri-La Grand Canyon compared to other regions. There is a low deviation in the RNH area, which is likely to belong to the national wetland Park with a low canopy height, so the accuracy of the model prediction is higher. Geographic cross-validation is able to test models beyond the limitations of their training environment and make predictions across regions. We tested the prediction results of the model trained on the dataset of southeast Tibet in northwest Yunnan, RMSE=10.09m, MAE=8.79m, ME=6.59m. In the whole northwest Yunnan region, the RML, RMJ, RNH and RNM predictions were exceeded by the root-mean-square error, and the mean error (ME) was greater than 10-fold. This may be because the canopy height in northwest Yunnan is mostly low, and the prediction deviation caused by the training model in southeast Tibet is slightly larger. When using Northwest Yunnan as the test set, PRFXception already sees some data from Northwest Yunnan, while section \"4.1\" takes all available data from Southeast Tibet-Northwest Yunnan as the test set. However, we did not observe any significant performance differences. Overall, the experiments in this section show that the proposed model generalizes fairly well to geographic regions not seen in training and does not overfit to specific (training) regions. Through geographic cross-validation, we can find the performance difference of the model in different regions, so', ' prediction deviation caused by the training model in southeast Tibet is slightly larger. When using Northwest Yunnan as the test set, PRFXception already sees some data from Northwest Yunnan, while section \"4.1\" takes all available data from Southeast Tibet-Northwest Yunnan as the test set. However, we did not observe any significant performance differences. Overall, the experiments in this section show that the proposed model generalizes fairly well to geographic regions not seen in training and does not overfit to specific (training) regions. Through geographic cross-validation, we can find the performance difference of the model in different regions, so as to provide a basis for revealing the influence of factors such as climate, soil and vegetation type on the prediction of forest canopy height in different regions. Our model has a strong generalization ability and can provide policymakers with accurate data support across regions in the future, which is important for scientific research and environmental policy formulation.\\n\\n## 4.6. Mapping the height of primary forest canopy in\\n\\nAs described in section 3.3, we trained the proposed PRFXception model and mapped the primary forest canopy height of the fourth world-level giant tree distribution center at 10m spatial resolution. Using Sentinel-2, ICESat-2, GEDI and other multi-source remote sensing data fusion as training data set, the modeling ability of PRFXception to\\n\\n## the fourth world-level giant tree distribution area\\n\\nColumn 1\\n\\nColumn 2\\n\\n|     | Overall model prediction   | Overall model prediction   | Overall model prediction   | Residual cross-validation   | Residual cross-validation   | Residual cross-validation   |\\n|-----|----------------------------|----------------------------|----------------------------|-----------------------------|-----------------------------|-----------------------------|\\n|     | RMSE                       | MAE                        | ME                         | RMSE                        | MAE                         | ME                          |\\n| RDR | 9.77                       | 8.35                       | 3.75                       | 8.14                        | 6.94                        | 0.35                        |\\n| RFU | 8.19                       | 6.96                       | 0.94                       | 14.28                       | 11.74                       | -3.35                       |\\n| RGS | 9.08                       | 7.77                       | 3.39                       | 8.07                        | 6.79                        | -1.75                       |\\n| RGT | 7.40                       | 6.32', '47 |  10.17 |  8.42 |   7.34 |\\n|  2 | RFU |   9.83 |  8.36 | -2.94 |  12.55 | 10.48 |  -8.87 |\\n|  3 | RGS |   8.15 |  6.86 | -1.74 |  13.8  | 11.88 | -10.34 |\\n|  4 | RGT |   7.4  |  6.78 | -2.9  |  14.24 | 15.32 | -14.33 |\\n|  5 | RLM |   7.02 |  5.96 | -3.06 |  10.17 |  8.59 |  -7.39 |\\n\\nseem to have advantages in capturing high canopy areas, which can be concluded from the color distribution and change of pixel values in the map.\\n\\nWe used the model trained by multiple receptive field and single receptive field to predict five different sites in southeast Tibet, and the accuracy of the multi-receptive field model was better than that of the single receptive field model in different regions of southeast Tibet (Table 2). The RMSE of the multi-receptive field in the five regions ranged from 7.02m to 8.26m, the MAE from 5.96m to 8.36m, and the ME from -3.06m to 0.47m. The single receptive field in the five regions had RMSE ranging from 10.17m to 14.24m, MAEranging from 8.42m to 15.32m, and ME ranging from -14.39m to 7.34m. On the whole, the prediction error of multi-receptive field model is lower than that of singlereceptive field model, which emphasizes the potential of PRFXception model in improving the prediction accuracy of\\n\\nFrom Fig. 13, we can see that PRFXception has a more refined spatial rendering mode, because the pyramid receptive field can capture more subtle features, the painted canopy height transitions and changes in different locations are smoother and smoother, and the texture information of the terrain is richer. In addition, details are shown more precisely, such as small forest areas or the edges of the tree canopy. In contrast, the height of', 'RGS), Medog County National Nature Reserve (RGT), and Zayü Nature Reserve (RLM). The selected areas in northwest Yunnan are Gaogongshan National Nature Reserve (RML), Nujiang Grand Canyon (RMJ), Qinghai Hua National Wetland Park (RNH) and Shangri-La Grand Canyon (RNM). We first trained the models of the two overall regions of southeast Tibet and northwest Yunnan, and then predicted the selected subregions respectively (Column 1 of Table 3). After that, we carried out cross-verification in southeast Tibet and northwest Yunnan. Train all but one subregion for prediction so\\n\\nDeep neural networks have a large model capacity, that is, enough trainable parameters to adapt to the details of the training data distribution. To verify whether PRFXception is overfitted in a particular training area or suitable for areas other than the training area, we conducted a geographical training/test segmentation experiment on the study area to study geographical generalization. We used the model trained in southeast Tibet to verify the forest canopy height in northwest Yunnan, so as to evaluate the generalization ability of the model. Geographic cross-validation plays an important role here, not only to reveal the applicability of the model to new regions, but also to help identify and correct the limitations of the model in different geographical conditions. In the experiment, all samples from the test area are used for evaluation, and the model is trained from scratch without seeing any samples from that test area. The geographical generalization results of the fourth world level megalopolis are given in Table 3.\\n\\nthat training data near the test area is not visible during training. The experiment will yield 5x and 4x cross-validation, respectively (Column 2 of Table 3).\\n\\nAscanbeseen from Table 3, on the whole, RMSE ranges from 6.15m to 9.77m, MAE ranges from 5.17m to 8.35m, and ME ranges from -0.55m to 3.75m in southeast Tibet. In northwestern Yunnan, RMSE ranges from 5.43m to 7.19m, MAE ranges from 4.60m to 6.20m, and ME ranges from 3.96m to -0.20m. It can be seen that the overall prediction effect in southeast Tibet is lower than that in northwest Yunnan, which may'], 'scores': [0.6618093252182007, 0.6336410045623779, 0.5971870422363281, 0.5701791048049927, 0.5690022706985474]}, content=[TextContentItem(text='knowledge_search tool found 5 chunks:\\nBEGIN of knowledge_search tool results.\\n', type='text'), TextContentItem(text=\"Result 1\\nContent:  whether the model overfits the training location or whether it supports common features such as making predictions at unseen locations. We conducted geographical cross-validation for 5 regions in southeast Tibet and 4 regions in northwest Yunnan respectively (Fig. 5), which will produce 5 times and 4 times cross-validation. We conducted two types of geographic cross-validation. In the first, we trained two separate regions, southeast Tibet and northwest Yunnan, and then cross-predicted their respective regions. The second method is to cross-verify the five selected regions in southeast Tibet and four regions in northwest Yunnan. All but one area is trained, and the remaining test area is predicted so that the training data near the test area is not visible during the training period.\\n\\n## 3.6. Evaluation index\\n\\nWe use the root mean square error (RMSE) as the primary error measure to quantify the deviation between the predicted height and the reference height. The mean error (ME) is used to measure the performance of a training model evaluated over an invisible test area. For completeness and to allow comparisons with other studies, we also report the mean absolute error (MAE).\\n\\n<!-- formula-not-decoded -->\\n\\n<!-- formula-not-decoded -->\\n\\n<!-- formula-not-decoded -->\\n\\nwhere 𝑓 𝑥 𝑖 ( ) represents the predicted value of the model set, and 𝑦 𝑖 represents the reference value at sample 𝑖 . According to this definition, a positive ME (mean square error) means that the value predicted by the model is higher than the true value of the reference data.\\n\\nFig. 5. Geographical cross-validation region selection distribution map\\n\\n<!-- image -->\\n\\n## 4. Results and discussion\\n\\n4.1.1. Validation of reference data for GEDI and ICESat-2\\n\\n4.1. Canopy height regression\\n\\nWe use three different combinations of training data as sparse supervised samples to train PRFXception and predict height separately. The data combination types were ICESat-2, GEDI, and ICESat-2 fusion GEDI, respectively, and sample labels were used as reference data to verify the accuracy of high regression. The results show that the canopy height predicted by PRFXception is in good agreement with the reference data. By comparison, it can be seen that the combination effect of ICESat-2 &amp; GEDI is better, 𝑅𝑀𝑆𝐸 =7.56m, 𝑀�\\nMetadata: {'source_url': 'https://arxiv.org/pdf/2404.14661', 'processing_method': 'docling', 'document_type': 'academic_paper', 'has_tables': True, 'has_formulas': True, 'has_figures': True, 'document_id': 'docling-processed-doc'}\\n\", type='text'), TextContentItem(text='Result 2\\nContent:  ranges from 6.15m to 9.77m, MAE ranges from 5.17m to 8.35m, and ME ranges from -0.55m to 3.75m in southeast Tibet. In northwestern Yunnan, RMSE ranges from 5.43m to 7.19m, MAE ranges from 4.60m to 6.20m, and ME ranges from 3.96m to -0.20m. It can be seen that the overall prediction effect in southeast Tibet is lower than that in northwest Yunnan, which may be due to the greater elevation difference in southeast Tibet. For southeast Tibet, PRFXception showed higher accuracy in RLM area, with lower RMSE, MAE and ME, and better prediction effect. The overall error deviation of PRFXception in northwest Yunnan is low, and the uncertainty in RNM is relatively high, which may be due to the relatively high canopy or elevation of Shangri-La Grand Canyon compared to other regions. There is a low deviation in the RNH area, which is likely to belong to the national wetland Park with a low canopy height, so the accuracy of the model prediction is higher. Geographic cross-validation is able to test models beyond the limitations of their training environment and make predictions across regions. We tested the prediction results of the model trained on the dataset of southeast Tibet in northwest Yunnan, RMSE=10.09m, MAE=8.79m, ME=6.59m. In the whole northwest Yunnan region, the RML, RMJ, RNH and RNM predictions were exceeded by the root-mean-square error, and the mean error (ME) was greater than 10-fold. This may be because the canopy height in northwest Yunnan is mostly low, and the prediction deviation caused by the training model in southeast Tibet is slightly larger. When using Northwest Yunnan as the test set, PRFXception already sees some data from Northwest Yunnan, while section \"4.1\" takes all available data from Southeast Tibet-Northwest Yunnan as the test set. However, we did not observe any significant performance differences. Overall, the experiments in this section show that the proposed model generalizes fairly well to geographic regions not seen in training and does not overfit to specific (training) regions. Through geographic cross-validation, we can find the performance difference of the model in different regions, so\\nMetadata: {\\'source_url\\': \\'https://arxiv.org/pdf/2404.14661\\', \\'processing_method\\': \\'docling\\', \\'document_type\\': \\'academic_paper\\', \\'has_tables\\': True, \\'has_formulas\\': True, \\'has_figures\\': True, \\'document_id\\': \\'docling-processed-doc\\'}\\n', type='text'), TextContentItem(text='Result 3\\nContent:  prediction deviation caused by the training model in southeast Tibet is slightly larger. When using Northwest Yunnan as the test set, PRFXception already sees some data from Northwest Yunnan, while section \"4.1\" takes all available data from Southeast Tibet-Northwest Yunnan as the test set. However, we did not observe any significant performance differences. Overall, the experiments in this section show that the proposed model generalizes fairly well to geographic regions not seen in training and does not overfit to specific (training) regions. Through geographic cross-validation, we can find the performance difference of the model in different regions, so as to provide a basis for revealing the influence of factors such as climate, soil and vegetation type on the prediction of forest canopy height in different regions. Our model has a strong generalization ability and can provide policymakers with accurate data support across regions in the future, which is important for scientific research and environmental policy formulation.\\n\\n## 4.6. Mapping the height of primary forest canopy in\\n\\nAs described in section 3.3, we trained the proposed PRFXception model and mapped the primary forest canopy height of the fourth world-level giant tree distribution center at 10m spatial resolution. Using Sentinel-2, ICESat-2, GEDI and other multi-source remote sensing data fusion as training data set, the modeling ability of PRFXception to\\n\\n## the fourth world-level giant tree distribution area\\n\\nColumn 1\\n\\nColumn 2\\n\\n|     | Overall model prediction   | Overall model prediction   | Overall model prediction   | Residual cross-validation   | Residual cross-validation   | Residual cross-validation   |\\n|-----|----------------------------|----------------------------|----------------------------|-----------------------------|-----------------------------|-----------------------------|\\n|     | RMSE                       | MAE                        | ME                         | RMSE                        | MAE                         | ME                          |\\n| RDR | 9.77                       | 8.35                       | 3.75                       | 8.14                        | 6.94                        | 0.35                        |\\n| RFU | 8.19                       | 6.96                       | 0.94                       | 14.28                       | 11.74                       | -3.35                       |\\n| RGS | 9.08                       | 7.77                       | 3.39                       | 8.07                        | 6.79                        | -1.75                       |\\n| RGT | 7.40                       | 6.32\\nMetadata: {\\'source_url\\': \\'https://arxiv.org/pdf/2404.14661\\', \\'processing_method\\': \\'docling\\', \\'document_type\\': \\'academic_paper\\', \\'has_tables\\': True, \\'has_formulas\\': True, \\'has_figures\\': True, \\'document_id\\': \\'docling-processed-doc\\'}\\n', type='text'), TextContentItem(text=\"Result 4\\nContent: 47 |  10.17 |  8.42 |   7.34 |\\n|  2 | RFU |   9.83 |  8.36 | -2.94 |  12.55 | 10.48 |  -8.87 |\\n|  3 | RGS |   8.15 |  6.86 | -1.74 |  13.8  | 11.88 | -10.34 |\\n|  4 | RGT |   7.4  |  6.78 | -2.9  |  14.24 | 15.32 | -14.33 |\\n|  5 | RLM |   7.02 |  5.96 | -3.06 |  10.17 |  8.59 |  -7.39 |\\n\\nseem to have advantages in capturing high canopy areas, which can be concluded from the color distribution and change of pixel values in the map.\\n\\nWe used the model trained by multiple receptive field and single receptive field to predict five different sites in southeast Tibet, and the accuracy of the multi-receptive field model was better than that of the single receptive field model in different regions of southeast Tibet (Table 2). The RMSE of the multi-receptive field in the five regions ranged from 7.02m to 8.26m, the MAE from 5.96m to 8.36m, and the ME from -3.06m to 0.47m. The single receptive field in the five regions had RMSE ranging from 10.17m to 14.24m, MAEranging from 8.42m to 15.32m, and ME ranging from -14.39m to 7.34m. On the whole, the prediction error of multi-receptive field model is lower than that of singlereceptive field model, which emphasizes the potential of PRFXception model in improving the prediction accuracy of\\n\\nFrom Fig. 13, we can see that PRFXception has a more refined spatial rendering mode, because the pyramid receptive field can capture more subtle features, the painted canopy height transitions and changes in different locations are smoother and smoother, and the texture information of the terrain is richer. In addition, details are shown more precisely, such as small forest areas or the edges of the tree canopy. In contrast, the height of\\nMetadata: {'source_url': 'https://arxiv.org/pdf/2404.14661', 'processing_method': 'docling', 'document_type': 'academic_paper', 'has_tables': True, 'has_formulas': True, 'has_figures': True, 'document_id': 'docling-processed-doc'}\\n\", type='text'), TextContentItem(text=\"Result 5\\nContent: RGS), Medog County National Nature Reserve (RGT), and Zayü Nature Reserve (RLM). The selected areas in northwest Yunnan are Gaogongshan National Nature Reserve (RML), Nujiang Grand Canyon (RMJ), Qinghai Hua National Wetland Park (RNH) and Shangri-La Grand Canyon (RNM). We first trained the models of the two overall regions of southeast Tibet and northwest Yunnan, and then predicted the selected subregions respectively (Column 1 of Table 3). After that, we carried out cross-verification in southeast Tibet and northwest Yunnan. Train all but one subregion for prediction so\\n\\nDeep neural networks have a large model capacity, that is, enough trainable parameters to adapt to the details of the training data distribution. To verify whether PRFXception is overfitted in a particular training area or suitable for areas other than the training area, we conducted a geographical training/test segmentation experiment on the study area to study geographical generalization. We used the model trained in southeast Tibet to verify the forest canopy height in northwest Yunnan, so as to evaluate the generalization ability of the model. Geographic cross-validation plays an important role here, not only to reveal the applicability of the model to new regions, but also to help identify and correct the limitations of the model in different geographical conditions. In the experiment, all samples from the test area are used for evaluation, and the model is trained from scratch without seeing any samples from that test area. The geographical generalization results of the fourth world level megalopolis are given in Table 3.\\n\\nthat training data near the test area is not visible during training. The experiment will yield 5x and 4x cross-validation, respectively (Column 2 of Table 3).\\n\\nAscanbeseen from Table 3, on the whole, RMSE ranges from 6.15m to 9.77m, MAE ranges from 5.17m to 8.35m, and ME ranges from -0.55m to 3.75m in southeast Tibet. In northwestern Yunnan, RMSE ranges from 5.43m to 7.19m, MAE ranges from 4.60m to 6.20m, and ME ranges from 3.96m to -0.20m. It can be seen that the overall prediction effect in southeast Tibet is lower than that in northwest Yunnan, which may\\nMetadata: {'source_url': 'https://arxiv.org/pdf/2404.14661', 'processing_method': 'docling', 'document_type': 'academic_paper', 'has_tables': True, 'has_formulas': True, 'has_figures': True, 'document_id': 'docling-processed-doc'}\\n\", type='text'), TextContentItem(text='END of knowledge_search tool results.\\n', type='text'), TextContentItem(text='The above results were retrieved to help answer the user\\'s query: \"The accuracy values of overall model prediction and residual cross-validation for five regions in southeast Tibet and four regions in northwest Yunnan\". Use them as supporting information only in answering this query.\\n', type='text')])\u001b[0m\n",
      "\u001b[33m\n",
      "--- RAG Metadata ---\u001b[0m\n",
      "\u001b[36m{'document_ids': ['docling-processed-doc', 'docling-processed-doc', 'docling-processed-doc', 'docling-processed-doc', 'docling-processed-doc'], 'chunks': [' whether the model overfits the training location or whether it supports common features such as making predictions at unseen locations. We conducted geographical cross-validation for 5 regions in southeast Tibet and 4 regions in northwest Yunnan respectively (Fig. 5), which will produce 5 times and 4 times cross-validation. We conducted two types of geographic cross-validation. In the first, we trained two separate regions, southeast Tibet and northwest Yunnan, and then cross-predicted their respective regions. The second method is to cross-verify the five selected regions in southeast Tibet and four regions in northwest Yunnan. All but one area is trained, and the remaining test area is predicted so that the training data near the test area is not visible during the training period.\\n\\n## 3.6. Evaluation index\\n\\nWe use the root mean square error (RMSE) as the primary error measure to quantify the deviation between the predicted height and the reference height. The mean error (ME) is used to measure the performance of a training model evaluated over an invisible test area. For completeness and to allow comparisons with other studies, we also report the mean absolute error (MAE).\\n\\n<!-- formula-not-decoded -->\\n\\n<!-- formula-not-decoded -->\\n\\n<!-- formula-not-decoded -->\\n\\nwhere 𝑓 𝑥 𝑖 ( ) represents the predicted value of the model set, and 𝑦 𝑖 represents the reference value at sample 𝑖 . According to this definition, a positive ME (mean square error) means that the value predicted by the model is higher than the true value of the reference data.\\n\\nFig. 5. Geographical cross-validation region selection distribution map\\n\\n<!-- image -->\\n\\n## 4. Results and discussion\\n\\n4.1.1. Validation of reference data for GEDI and ICESat-2\\n\\n4.1. Canopy height regression\\n\\nWe use three different combinations of training data as sparse supervised samples to train PRFXception and predict height separately. The data combination types were ICESat-2, GEDI, and ICESat-2 fusion GEDI, respectively, and sample labels were used as reference data to verify the accuracy of high regression. The results show that the canopy height predicted by PRFXception is in good agreement with the reference data. By comparison, it can be seen that the combination effect of ICESat-2 &amp; GEDI is better, 𝑅𝑀𝑆𝐸 =7.56m, 𝑀�', ' ranges from 6.15m to 9.77m, MAE ranges from 5.17m to 8.35m, and ME ranges from -0.55m to 3.75m in southeast Tibet. In northwestern Yunnan, RMSE ranges from 5.43m to 7.19m, MAE ranges from 4.60m to 6.20m, and ME ranges from 3.96m to -0.20m. It can be seen that the overall prediction effect in southeast Tibet is lower than that in northwest Yunnan, which may be due to the greater elevation difference in southeast Tibet. For southeast Tibet, PRFXception showed higher accuracy in RLM area, with lower RMSE, MAE and ME, and better prediction effect. The overall error deviation of PRFXception in northwest Yunnan is low, and the uncertainty in RNM is relatively high, which may be due to the relatively high canopy or elevation of Shangri-La Grand Canyon compared to other regions. There is a low deviation in the RNH area, which is likely to belong to the national wetland Park with a low canopy height, so the accuracy of the model prediction is higher. Geographic cross-validation is able to test models beyond the limitations of their training environment and make predictions across regions. We tested the prediction results of the model trained on the dataset of southeast Tibet in northwest Yunnan, RMSE=10.09m, MAE=8.79m, ME=6.59m. In the whole northwest Yunnan region, the RML, RMJ, RNH and RNM predictions were exceeded by the root-mean-square error, and the mean error (ME) was greater than 10-fold. This may be because the canopy height in northwest Yunnan is mostly low, and the prediction deviation caused by the training model in southeast Tibet is slightly larger. When using Northwest Yunnan as the test set, PRFXception already sees some data from Northwest Yunnan, while section \"4.1\" takes all available data from Southeast Tibet-Northwest Yunnan as the test set. However, we did not observe any significant performance differences. Overall, the experiments in this section show that the proposed model generalizes fairly well to geographic regions not seen in training and does not overfit to specific (training) regions. Through geographic cross-validation, we can find the performance difference of the model in different regions, so', ' prediction deviation caused by the training model in southeast Tibet is slightly larger. When using Northwest Yunnan as the test set, PRFXception already sees some data from Northwest Yunnan, while section \"4.1\" takes all available data from Southeast Tibet-Northwest Yunnan as the test set. However, we did not observe any significant performance differences. Overall, the experiments in this section show that the proposed model generalizes fairly well to geographic regions not seen in training and does not overfit to specific (training) regions. Through geographic cross-validation, we can find the performance difference of the model in different regions, so as to provide a basis for revealing the influence of factors such as climate, soil and vegetation type on the prediction of forest canopy height in different regions. Our model has a strong generalization ability and can provide policymakers with accurate data support across regions in the future, which is important for scientific research and environmental policy formulation.\\n\\n## 4.6. Mapping the height of primary forest canopy in\\n\\nAs described in section 3.3, we trained the proposed PRFXception model and mapped the primary forest canopy height of the fourth world-level giant tree distribution center at 10m spatial resolution. Using Sentinel-2, ICESat-2, GEDI and other multi-source remote sensing data fusion as training data set, the modeling ability of PRFXception to\\n\\n## the fourth world-level giant tree distribution area\\n\\nColumn 1\\n\\nColumn 2\\n\\n|     | Overall model prediction   | Overall model prediction   | Overall model prediction   | Residual cross-validation   | Residual cross-validation   | Residual cross-validation   |\\n|-----|----------------------------|----------------------------|----------------------------|-----------------------------|-----------------------------|-----------------------------|\\n|     | RMSE                       | MAE                        | ME                         | RMSE                        | MAE                         | ME                          |\\n| RDR | 9.77                       | 8.35                       | 3.75                       | 8.14                        | 6.94                        | 0.35                        |\\n| RFU | 8.19                       | 6.96                       | 0.94                       | 14.28                       | 11.74                       | -3.35                       |\\n| RGS | 9.08                       | 7.77                       | 3.39                       | 8.07                        | 6.79                        | -1.75                       |\\n| RGT | 7.40                       | 6.32', '47 |  10.17 |  8.42 |   7.34 |\\n|  2 | RFU |   9.83 |  8.36 | -2.94 |  12.55 | 10.48 |  -8.87 |\\n|  3 | RGS |   8.15 |  6.86 | -1.74 |  13.8  | 11.88 | -10.34 |\\n|  4 | RGT |   7.4  |  6.78 | -2.9  |  14.24 | 15.32 | -14.33 |\\n|  5 | RLM |   7.02 |  5.96 | -3.06 |  10.17 |  8.59 |  -7.39 |\\n\\nseem to have advantages in capturing high canopy areas, which can be concluded from the color distribution and change of pixel values in the map.\\n\\nWe used the model trained by multiple receptive field and single receptive field to predict five different sites in southeast Tibet, and the accuracy of the multi-receptive field model was better than that of the single receptive field model in different regions of southeast Tibet (Table 2). The RMSE of the multi-receptive field in the five regions ranged from 7.02m to 8.26m, the MAE from 5.96m to 8.36m, and the ME from -3.06m to 0.47m. The single receptive field in the five regions had RMSE ranging from 10.17m to 14.24m, MAEranging from 8.42m to 15.32m, and ME ranging from -14.39m to 7.34m. On the whole, the prediction error of multi-receptive field model is lower than that of singlereceptive field model, which emphasizes the potential of PRFXception model in improving the prediction accuracy of\\n\\nFrom Fig. 13, we can see that PRFXception has a more refined spatial rendering mode, because the pyramid receptive field can capture more subtle features, the painted canopy height transitions and changes in different locations are smoother and smoother, and the texture information of the terrain is richer. In addition, details are shown more precisely, such as small forest areas or the edges of the tree canopy. In contrast, the height of', 'RGS), Medog County National Nature Reserve (RGT), and Zayü Nature Reserve (RLM). The selected areas in northwest Yunnan are Gaogongshan National Nature Reserve (RML), Nujiang Grand Canyon (RMJ), Qinghai Hua National Wetland Park (RNH) and Shangri-La Grand Canyon (RNM). We first trained the models of the two overall regions of southeast Tibet and northwest Yunnan, and then predicted the selected subregions respectively (Column 1 of Table 3). After that, we carried out cross-verification in southeast Tibet and northwest Yunnan. Train all but one subregion for prediction so\\n\\nDeep neural networks have a large model capacity, that is, enough trainable parameters to adapt to the details of the training data distribution. To verify whether PRFXception is overfitted in a particular training area or suitable for areas other than the training area, we conducted a geographical training/test segmentation experiment on the study area to study geographical generalization. We used the model trained in southeast Tibet to verify the forest canopy height in northwest Yunnan, so as to evaluate the generalization ability of the model. Geographic cross-validation plays an important role here, not only to reveal the applicability of the model to new regions, but also to help identify and correct the limitations of the model in different geographical conditions. In the experiment, all samples from the test area are used for evaluation, and the model is trained from scratch without seeing any samples from that test area. The geographical generalization results of the fourth world level megalopolis are given in Table 3.\\n\\nthat training data near the test area is not visible during training. The experiment will yield 5x and 4x cross-validation, respectively (Column 2 of Table 3).\\n\\nAscanbeseen from Table 3, on the whole, RMSE ranges from 6.15m to 9.77m, MAE ranges from 5.17m to 8.35m, and ME ranges from -0.55m to 3.75m in southeast Tibet. In northwestern Yunnan, RMSE ranges from 5.43m to 7.19m, MAE ranges from 4.60m to 6.20m, and ME ranges from 3.96m to -0.20m. It can be seen that the overall prediction effect in southeast Tibet is lower than that in northwest Yunnan, which may'], 'scores': [0.6618093252182007, 0.6336410045623779, 0.5971870422363281, 0.5701791048049927, 0.5690022706985474]}\u001b[0m\n",
      "\u001b[35minference> \u001b[0m\u001b[35m\u001b[0m\u001b[35mBased\u001b[0m\u001b[35m on\u001b[0m\u001b[35m the\u001b[0m\u001b[35m provided\u001b[0m\u001b[35m text\u001b[0m\u001b[35m content\u001b[0m\u001b[35m items\u001b[0m\u001b[35m,\u001b[0m\u001b[35m the\u001b[0m\u001b[35m accuracy\u001b[0m\u001b[35m values\u001b[0m\u001b[35m of\u001b[0m\u001b[35m overall\u001b[0m\u001b[35m model\u001b[0m\u001b[35m prediction\u001b[0m\u001b[35m and\u001b[0m\u001b[35m residual\u001b[0m\u001b[35m cross\u001b[0m\u001b[35m-validation\u001b[0m\u001b[35m for\u001b[0m\u001b[35m five\u001b[0m\u001b[35m regions\u001b[0m\u001b[35m in\u001b[0m\u001b[35m southeast\u001b[0m\u001b[35m Tibet\u001b[0m\u001b[35m and\u001b[0m\u001b[35m four\u001b[0m\u001b[35m regions\u001b[0m\u001b[35m in\u001b[0m\u001b[35m northwest\u001b[0m\u001b[35m Y\u001b[0m\u001b[35munn\u001b[0m\u001b[35man\u001b[0m\u001b[35m are\u001b[0m\u001b[35m as\u001b[0m\u001b[35m follows\u001b[0m\u001b[35m:\n",
      "\n",
      "\u001b[0m\u001b[35m**\u001b[0m\u001b[35mSou\u001b[0m\u001b[35mtheast\u001b[0m\u001b[35m Tibet\u001b[0m\u001b[35m:\u001b[0m\u001b[35m**\n",
      "\n",
      "\u001b[0m\u001b[35m-\u001b[0m\u001b[35m Overall\u001b[0m\u001b[35m model\u001b[0m\u001b[35m prediction\u001b[0m\u001b[35m:\n",
      "\u001b[0m\u001b[35m \u001b[0m\u001b[35m -\u001b[0m\u001b[35m RM\u001b[0m\u001b[35mSE\u001b[0m\u001b[35m:\u001b[0m\u001b[35m \u001b[0m\u001b[35m7\u001b[0m\u001b[35m.\u001b[0m\u001b[35m02\u001b[0m\u001b[35mm\u001b[0m\u001b[35m to\u001b[0m\u001b[35m \u001b[0m\u001b[35m8\u001b[0m\u001b[35m.\u001b[0m\u001b[35m26\u001b[0m\u001b[35mm\u001b[0m\u001b[35m\n",
      "\u001b[0m\u001b[35m \u001b[0m\u001b[35m -\u001b[0m\u001b[35m MA\u001b[0m\u001b[35mE\u001b[0m\u001b[35m:\u001b[0m\u001b[35m \u001b[0m\u001b[35m5\u001b[0m\u001b[35m.\u001b[0m\u001b[35m96\u001b[0m\u001b[35mm\u001b[0m\u001b[35m to\u001b[0m\u001b[35m \u001b[0m\u001b[35m8\u001b[0m\u001b[35m.\u001b[0m\u001b[35m36\u001b[0m\u001b[35mm\u001b[0m\u001b[35m\n",
      "\u001b[0m\u001b[35m \u001b[0m\u001b[35m -\u001b[0m\u001b[35m ME\u001b[0m\u001b[35m:\u001b[0m\u001b[35m -\u001b[0m\u001b[35m3\u001b[0m\u001b[35m.\u001b[0m\u001b[35m06\u001b[0m\u001b[35mm\u001b[0m\u001b[35m to\u001b[0m\u001b[35m \u001b[0m\u001b[35m0\u001b[0m\u001b[35m.\u001b[0m\u001b[35m47\u001b[0m\u001b[35mm\u001b[0m\u001b[35m\n",
      "\n",
      "\u001b[0m\u001b[35m-\u001b[0m\u001b[35m Res\u001b[0m\u001b[35midual\u001b[0m\u001b[35m cross\u001b[0m\u001b[35m-validation\u001b[0m\u001b[35m:\n",
      "\u001b[0m\u001b[35m \u001b[0m\u001b[35m -\u001b[0m\u001b[35m RM\u001b[0m\u001b[35mSE\u001b[0m\u001b[35m:\u001b[0m\u001b[35m \u001b[0m\u001b[35m6\u001b[0m\u001b[35m.\u001b[0m\u001b[35m15\u001b[0m\u001b[35mm\u001b[0m\u001b[35m to\u001b[0m\u001b[35m \u001b[0m\u001b[35m9\u001b[0m\u001b[35m.\u001b[0m\u001b[35m77\u001b[0m\u001b[35mm\u001b[0m\u001b[35m\n",
      "\u001b[0m\u001b[35m \u001b[0m\u001b[35m -\u001b[0m\u001b[35m MA\u001b[0m\u001b[35mE\u001b[0m\u001b[35m:\u001b[0m\u001b[35m \u001b[0m\u001b[35m5\u001b[0m\u001b[35m.\u001b[0m\u001b[35m17\u001b[0m\u001b[35mm\u001b[0m\u001b[35m to\u001b[0m\u001b[35m \u001b[0m\u001b[35m8\u001b[0m\u001b[35m.\u001b[0m\u001b[35m35\u001b[0m\u001b[35mm\u001b[0m\u001b[35m\n",
      "\u001b[0m\u001b[35m \u001b[0m\u001b[35m -\u001b[0m\u001b[35m ME\u001b[0m\u001b[35m:\u001b[0m\u001b[35m -\u001b[0m\u001b[35m0\u001b[0m\u001b[35m.\u001b[0m\u001b[35m55\u001b[0m\u001b[35mm\u001b[0m\u001b[35m to\u001b[0m\u001b[35m \u001b[0m\u001b[35m3\u001b[0m\u001b[35m.\u001b[0m\u001b[35m75\u001b[0m\u001b[35mm\u001b[0m\u001b[35m\n",
      "\n",
      "\u001b[0m\u001b[35m**\u001b[0m\u001b[35mNorth\u001b[0m\u001b[35mwest\u001b[0m\u001b[35m Y\u001b[0m\u001b[35munn\u001b[0m\u001b[35man\u001b[0m\u001b[35m:\u001b[0m\u001b[35m**\n",
      "\n",
      "\u001b[0m\u001b[35m-\u001b[0m\u001b[35m Overall\u001b[0m\u001b[35m model\u001b[0m\u001b[35m prediction\u001b[0m\u001b[35m:\n",
      "\u001b[0m\u001b[35m \u001b[0m\u001b[35m -\u001b[0m\u001b[35m RM\u001b[0m\u001b[35mSE\u001b[0m\u001b[35m:\u001b[0m\u001b[35m \u001b[0m\u001b[35m5\u001b[0m\u001b[35m.\u001b[0m\u001b[35m43\u001b[0m\u001b[35mm\u001b[0m\u001b[35m to\u001b[0m\u001b[35m \u001b[0m\u001b[35m7\u001b[0m\u001b[35m.\u001b[0m\u001b[35m19\u001b[0m\u001b[35mm\u001b[0m\u001b[35m\n",
      "\u001b[0m\u001b[35m \u001b[0m\u001b[35m -\u001b[0m\u001b[35m MA\u001b[0m\u001b[35mE\u001b[0m\u001b[35m:\u001b[0m\u001b[35m \u001b[0m\u001b[35m4\u001b[0m\u001b[35m.\u001b[0m\u001b[35m60\u001b[0m\u001b[35mm\u001b[0m\u001b[35m to\u001b[0m\u001b[35m \u001b[0m\u001b[35m6\u001b[0m\u001b[35m.\u001b[0m\u001b[35m20\u001b[0m\u001b[35mm\u001b[0m\u001b[35m\n",
      "\u001b[0m\u001b[35m \u001b[0m\u001b[35m -\u001b[0m\u001b[35m ME\u001b[0m\u001b[35m:\u001b[0m\u001b[35m \u001b[0m\u001b[35m3\u001b[0m\u001b[35m.\u001b[0m\u001b[35m96\u001b[0m\u001b[35mm\u001b[0m\u001b[35m to\u001b[0m\u001b[35m -\u001b[0m\u001b[35m0\u001b[0m\u001b[35m.\u001b[0m\u001b[35m20\u001b[0m\u001b[35mm\u001b[0m\u001b[35m\n",
      "\n",
      "\u001b[0m\u001b[35m-\u001b[0m\u001b[35m Res\u001b[0m\u001b[35midual\u001b[0m\u001b[35m cross\u001b[0m\u001b[35m-validation\u001b[0m\u001b[35m:\n",
      "\u001b[0m\u001b[35m \u001b[0m\u001b[35m -\u001b[0m\u001b[35m RM\u001b[0m\u001b[35mSE\u001b[0m\u001b[35m:\u001b[0m\u001b[35m Not\u001b[0m\u001b[35m provided\u001b[0m\u001b[35m\n",
      "\u001b[0m\u001b[35m \u001b[0m\u001b[35m -\u001b[0m\u001b[35m MA\u001b[0m\u001b[35mE\u001b[0m\u001b[35m:\u001b[0m\u001b[35m Not\u001b[0m\u001b[35m provided\u001b[0m\u001b[35m\n",
      "\u001b[0m\u001b[35m \u001b[0m\u001b[35m -\u001b[0m\u001b[35m ME\u001b[0m\u001b[35m:\u001b[0m\u001b[35m Not\u001b[0m\u001b[35m provided\u001b[0m\u001b[35m\u001b[0m\u001b[34m\n",
      "--- End of RAG Answer ---\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Test queries for the processed document\n",
    "queries = [\n",
    "    \"What is the PRFXception?\",\n",
    "    \"The accuracy values of overall model prediction and residual cross-validation for five regions in southeast Tibet and four regions in northwest Yunnan\"\n",
    "]\n",
    "\n",
    "for prompt in queries:\n",
    "    cprint(f\"\\nUser> {prompt}\", \"blue\")\n",
    "    \n",
    "    # RAG retrieval call - find relevant chunks from the vector database\n",
    "    rag_response = client.tool_runtime.rag_tool.query(\n",
    "        content=prompt, \n",
    "        vector_db_ids=[vector_db_id],\n",
    "        query_config={\n",
    "            \"chunk_template\": \"Result {index}\\nContent: {chunk.content}\\nMetadata: {metadata}\\n\",\n",
    "        },\n",
    "        )\n",
    "\n",
    "    cprint(rag_response)\n",
    "\n",
    "    cprint(f\"\\n--- RAG Metadata ---\", \"yellow\")\n",
    "    cprint(rag_response.metadata, \"cyan\")\n",
    "\n",
    "    # Create messages for the LLM with system prompt\n",
    "    messages = [{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}]\n",
    "\n",
    "    # Combine the user query with retrieved context from RAG\n",
    "    prompt_context = rag_response.content\n",
    "    extended_prompt = f\"Please answer the given query using the context below.\\n\\nCONTEXT:\\n{prompt_context}\\n\\nQUERY:\\n{prompt}\"\n",
    "    messages.append({\"role\": \"user\", \"content\": extended_prompt})\n",
    "\n",
    "    # Get response from the LLM using the enhanced prompt\n",
    "    response = client.inference.chat_completion(\n",
    "        messages=messages,\n",
    "        model_id=model_id,\n",
    "        sampling_params=sampling_params,\n",
    "        stream=stream,\n",
    "    )\n",
    "    \n",
    "    # Print the streaming response\n",
    "    cprint(\"inference> \", color=\"magenta\", end='')\n",
    "    if stream:\n",
    "        for chunk in response:\n",
    "            response_delta = chunk.event.delta\n",
    "            if isinstance(response_delta, TextDelta):\n",
    "                cprint(response_delta.text, color=\"magenta\", end='')\n",
    "            elif isinstance(response_delta, ToolCallDelta):\n",
    "                cprint(response_delta.tool_call, color=\"magenta\", end='')\n",
    "    else:\n",
    "        cprint(response.completion_message.content, color=\"magenta\")\n",
    "\n",
    "    cprint(f\"\\n--- End of RAG Answer ---\", \"blue\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🎉 You've Built a Document Intelligence RAG System!\n",
    "\n",
    "**What you accomplished:**\n",
    "- **🔬 Document Intelligence**: Processed complex academic papers with Docling's advanced capabilities\n",
    "- **📊 Structured Content Extraction**: Preserved tables, formulas, figures, and document hierarchy\n",
    "- **🗄️ Enhanced Vector Storage**: Stored intelligently-processed content in Milvus for semantic search\n",
    "- **🤖 Intelligent Querying**: Built a RAG system that understands complex academic content\n",
    "- **⚡ Production Pipeline**: Created a scalable workflow for real-world educational materials\n",
    "\n",
    "**Key Technical Insights:**\n",
    "- **Document Intelligence vs Basic Extraction**: Docling preserves meaning and structure that simple text extraction would lose\n",
    "- **Three-Phase Processing**: Analysis → Enhancement → RAG Integration creates comprehensive understanding\n",
    "- **Semantic Understanding**: Complex documents become queryable by meaning, not just keywords\n",
    "- **Metadata Enrichment**: Enhanced document metadata enables better retrieval and filtering\n",
    "\n",
    "**Document Intelligence vs Traditional RAG:**\n",
    "| Traditional RAG | Document Intelligence RAG |\n",
    "|-----------------|---------------------------|\n",
    "| ❌ Loses table structure | ✅ Preserves tabular relationships |\n",
    "| ❌ Misses mathematical content | ✅ Handles formulas and equations |\n",
    "| ❌ Ignores document layout | ✅ Understands multi-column layouts |\n",
    "| ❌ Basic text chunks | ✅ Intelligent content structuring |\n",
    "| ❌ Limited metadata | ✅ Rich semantic metadata |\n",
    "\n",
    "**Real-World Applications:**\n",
    "- **📚 Academic Research Assistants**: Query research papers for specific findings and data\n",
    "- **🏫 Educational Content Search**: Find relevant course materials across complex documents  \n",
    "- **📊 Data Extraction**: Automatically extract and query tabular information\n",
    "- **🔬 Scientific Literature Review**: Analyze and compare findings across multiple papers\n",
    "- **📖 Intelligent Document Libraries**: Build searchable repositories of complex materials\n",
    "\n",
    "**Advanced Patterns to Explore:**\n",
    "- **Multi-Document Intelligence**: Process and compare findings across multiple research papers\n",
    "- **Domain-Specific Processing**: Optimize Docling for specific academic fields or document types\n",
    "- **Visual Content Integration**: Enhance with image and chart understanding capabilities\n",
    "- **Collaborative Intelligence**: Enable teams to build and share intelligent document repositories\n",
    "\n",
    "Your document intelligence system can now understand and query the most complex academic content - transforming how educational institutions handle knowledge discovery and research! 🚀"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "app-root",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
