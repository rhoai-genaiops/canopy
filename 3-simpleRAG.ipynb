{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "45fc9086-93aa-4645-8ba2-380c3acbbed9",
   "metadata": {},
   "source": [
    "# ü¶ô LlamaStack & RAG: Building Intelligent Agents\n",
    "\n",
    "This notebook demonstrates **Retrieval-Augmented Generation (RAG)** - a powerful technique that enables AI models to access and reason about external documents and knowledge bases.\n",
    "\n",
    "**What is RAG?**\n",
    "RAG transforms static AI models into dynamic assistants that can:\n",
    "- **Remember** every document you share with them\n",
    "- **Search** through vast libraries of content in milliseconds  \n",
    "- **Reason** about information from multiple sources simultaneously\n",
    "- **Update** their knowledge without retraining the entire model\n",
    "\n",
    "**Why RAG Matters:**\n",
    "Instead of relying only on training data, RAG-enhanced models can reference your specific documents, course materials, and knowledge bases to provide accurate, cited responses.\n",
    "\n",
    "You've already built a vector database - now let's add the intelligent layer that makes RAG truly powerful! üöÄ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6db34e4b-ed29-4007-b760-59543d4caca1",
   "metadata": {},
   "source": [
    "## üèóÔ∏è The LlamaStack RAG Architecture\n",
    "\n",
    "LlamaStack organizes RAG capabilities into **three elegant layers** that work together to create intelligent, knowledge-aware applications:\n",
    "\n",
    "### 1. üóÑÔ∏è Storage Layer (The Foundation)\n",
    "This is where your knowledge lives:\n",
    "- **Vector IO**: Stores document embeddings for semantic search - converts text into mathematical vectors that capture meaning\n",
    "- **KeyValue IO**: Manages structured metadata and simple lookups (document titles, authors, dates)\n",
    "- **Relational IO**: Handles complex queries across structured data (coming soon)\n",
    "\n",
    "### 2. üîß RAG Layer (The Intelligence)\n",
    "This is where documents become searchable knowledge:\n",
    "- **Document Ingestion**: Automatically downloads and processes files, URLs, and content\n",
    "- **Intelligent Chunking**: Splits large documents into optimal pieces (typically 512 tokens) for retrieval\n",
    "- **Semantic Search**: Finds relevant content based on meaning, not just keyword matching\n",
    "\n",
    "### 3. ü§ñ User Layer (The Interface)  \n",
    "This is where users interact with the knowledge:\n",
    "- **Context-Aware Agents**: LLMs that can automatically use RAG tools to answer questions\n",
    "- **Multi-Document Reasoning**: Agents that synthesize information from multiple sources\n",
    "- **Conversational Memory**: Maintains context across interactions while accessing external knowledge\n",
    "\n",
    "**The Magic:** When you ask a question, the system searches the vector database for relevant chunks, then provides those chunks as context to the LLM for generating informed, cited responses.\n",
    "\n",
    "## üì¶ Install Required Packages\n",
    "\n",
    "Install the Python packages needed for this lab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "332e6cec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install -q llama_stack_client fire dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f15080a6-48be-4475-8813-c584701d69bf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Core imports for RAG functionality\n",
    "import uuid  # For generating unique vector database IDs\n",
    "\n",
    "# LlamaStack client and RAG-specific classes\n",
    "from llama_stack_client import RAGDocument  # Represents documents for ingestion\n",
    "from llama_stack_client.types.shared.content_delta import TextDelta, ToolCallDelta  # For streaming responses\n",
    "\n",
    "# Additional utilities for document processing\n",
    "import base64    # For encoding images/binary data if needed\n",
    "import requests  # For fetching documents from URLs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "631e8c70-6f28-440b-b71a-85d4040ffac4",
   "metadata": {},
   "source": [
    "## üîó Connect to LlamaStack\n",
    "\n",
    "Connect to LlamaStack - the AI engine that orchestrates all RAG operations. LlamaStack acts as the central hub that coordinates:\n",
    "- Vector database operations (storage and retrieval)\n",
    "- Document processing and chunking\n",
    "- LLM inference with retrieved context\n",
    "- Agent workflows and tool usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "558909bb-955c-40a3-a0c2-1f4acb0dd62e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard imports for system utilities\n",
    "import os\n",
    "import sys\n",
    "sys.path.append('..')  # Add parent directory to path for custom utilities\n",
    "\n",
    "# Import custom utilities and LlamaStack client\n",
    "from src.utils import step_printer  # For pretty-printing step-by-step progress\n",
    "from termcolor import cprint        # For colorized console output\n",
    "from llama_stack_client import LlamaStackClient  # Main client for all LlamaStack operations\n",
    "\n",
    "# === LlamaStack Connection Setup ===\n",
    "# The base URL points to your LlamaStack server deployment\n",
    "base_url = \"http://llama-stack-service:8321\"\n",
    "\n",
    "# Optional: Configure external search tools (Tavily for web search)\n",
    "# Leave empty for this RAG-focused demo\n",
    "tavily_search_api_key = \"\"\n",
    "if tavily_search_api_key:\n",
    "    provider_data = {\"tavily_search_api_key\": tavily_search_api_key}\n",
    "else:\n",
    "    provider_data = None\n",
    "\n",
    "# Create the LlamaStack client - this is your main interface for all RAG operations\n",
    "client = LlamaStackClient(\n",
    "    base_url=base_url,\n",
    "    provider_data=provider_data  # Additional provider configurations\n",
    ")\n",
    "\n",
    "print(f\"Connected to LlamaStack server\")\n",
    "\n",
    "# === Model Configuration ===\n",
    "# Specify which LLM model to use for generating responses\n",
    "model_id = \"llama32\"  # Using Llama 3.2 model name\n",
    "\n",
    "# === Generation Parameters ===\n",
    "# These control how the model generates responses\n",
    "temperature = 0.0  # 0.0 = deterministic, higher = more creative\n",
    "max_tokens = 512   # Maximum length of generated responses\n",
    "stream = False     # Whether to stream responses token-by-token\n",
    "\n",
    "# Configure the sampling strategy based on temperature\n",
    "if temperature > 0.0:\n",
    "    top_p = 0.95  # Nucleus sampling parameter\n",
    "    strategy = {\"type\": \"top_p\", \"temperature\": temperature, \"top_p\": top_p}\n",
    "else:\n",
    "    strategy = {\"type\": \"greedy\"}  # Always pick most likely token\n",
    "\n",
    "# Package sampling parameters for the inference API\n",
    "sampling_params = {\n",
    "    \"strategy\": strategy,\n",
    "    \"max_tokens\": max_tokens,\n",
    "}\n",
    "\n",
    "# Display configuration for verification\n",
    "print(f\"Model: {model_id}\")\n",
    "print(f\"Sampling Parameters: {sampling_params}\")\n",
    "print(f\"Stream: {stream}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "841eaadf-f5ac-4d7c-bb9d-f039ccd8d9a3",
   "metadata": {},
   "source": [
    "## üóÉÔ∏è Create Vector Database for RAG\n",
    "\n",
    "Set up a vector database where documents will be stored for retrieval. This is the **Storage Layer** of our RAG architecture.\n",
    "\n",
    "**What happens here:**\n",
    "1. **Registration**: Tell LlamaStack about your vector database configuration\n",
    "2. **Embedding Model**: Specify which model converts text to vectors (we use `all-MiniLM-L6-v2`)\n",
    "3. **Dimensions**: Set vector size (384 dimensions for our chosen model)\n",
    "4. **Provider**: Connect to your Milvus database deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c725c2da-05e5-474f-9a44-cf5615557665",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# === STEP 1: Register Vector Database ===\n",
    "\n",
    "# Generate a unique identifier for this vector database instance\n",
    "# Using UUID ensures no conflicts if multiple users run this notebook simultaneously\n",
    "vector_db_id = f\"test_vector_db_{uuid.uuid4()}\"\n",
    "print(f\"üìä Created vector database ID: {vector_db_id}\")\n",
    "\n",
    "# This tells LlamaStack how to connect to and use your vector database\n",
    "client.vector_dbs.register(\n",
    "    vector_db_id=vector_db_id,                    # Unique identifier we created above\n",
    "    embedding_model=\"all-MiniLM-L6-v2\",          # Hugging Face model for text ‚Üí vectors\n",
    "    embedding_dimension=384,                      # Vector size (must match model output)\n",
    "    provider_id=\"milvus\",                         # Use Milvus as the vector database backend\n",
    ")\n",
    "print(f\"‚úÖ Registered vector database with Milvus backend\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87510929-fe4b-428c-8f9e-14d47a03daa2",
   "metadata": {},
   "source": [
    "## üìö Document Ingestion and Processing\n",
    "\n",
    "This is where the **RAG Layer** comes into action! We'll use LlamaStack's RAG Tool to automatically:\n",
    "\n",
    "1. **Download** documents from URLs\n",
    "2. **Process** PDF content and extract text\n",
    "3. **Chunk** large documents into optimal pieces (512 tokens each)\n",
    "4. **Embed** each chunk using the embedding model\n",
    "5. **Store** vectors and metadata in the vector database\n",
    "\n",
    "**Two ways to ingest documents:**\n",
    "- **Direct Vector IO**: Insert pre-processed chunks directly\n",
    "- **RAG Tool** (what we're using): Automatic processing from URLs or files\n",
    "\n",
    "A better way to ingest documents is to use the RAG Tool. This tool allows you to ingest documents from URLs, files, etc. and automatically chunks them into smaller pieces. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d81ffb2-2089-4cb8-adae-f32965f206c7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# === STEP 2: Define Documents to Ingest ===\n",
    "# List of (URL, MIME_TYPE) tuples for documents to process\n",
    "urls = [\n",
    "    (\"https://raw.githubusercontent.com/rhoai-genaiops/deploy-lab/main/university-data/canopy-in-botany.pdf\", \"application/pdf\"),\n",
    "]\n",
    "\n",
    "# === STEP 3: Create RAGDocument Objects ===\n",
    "# RAGDocument is LlamaStack's format for documents to be ingested\n",
    "documents = [\n",
    "    RAGDocument(\n",
    "        document_id=f\"doc-{i}\",                   # Unique ID for this document\n",
    "        content=url,                              # Can be URL, file path, or direct text\n",
    "        mime_type=url_type,                       # Tells LlamaStack how to process the content\n",
    "        metadata={                                # Additional information about the document\n",
    "            \"source_url\": url,                    # Where this document came from\n",
    "            \"document_type\": \"academic_material\",  # Category for filtering/organization\n",
    "        },\n",
    "    )\n",
    "    for i, (url, url_type) in enumerate(urls)\n",
    "]\n",
    "\n",
    "# Display what we're about to ingest\n",
    "print(\"üìñ Ingesting documents into RAG system...\")\n",
    "for i, (url, url_type) in enumerate(urls):\n",
    "    print(f\"  ‚Ä¢ Document {i+1}: {url}\")\n",
    "\n",
    "# === STEP 4: Use RAG Tool for Automatic Processing ===\n",
    "# This is where the magic happens! The RAG tool will:\n",
    "# 1. Download the PDF from the URL\n",
    "# 2. Extract and parse the text content\n",
    "# 3. Split into chunks of 512 tokens each\n",
    "# 4. Generate embeddings for each chunk\n",
    "# 5. Store everything in the vector database\n",
    "try:\n",
    "    client.tool_runtime.rag_tool.insert(\n",
    "        documents=documents,                      # List of RAGDocument objects to process\n",
    "        vector_db_id=vector_db_id,               # Where to store the processed chunks\n",
    "        chunk_size_in_tokens=512,                # Optimal size for retrieval (not too big, not too small)\n",
    "    )\n",
    "    print(\"\\n‚úÖ Document ingestion complete!\")\n",
    "    print(\"üéØ Your documents are now searchable via semantic similarity!\")\n",
    "except Exception as e:\n",
    "    print(f\"\\n‚ùå Document ingestion failed: {e}\")\n",
    "    print(\"üí° This might be due to PDF processing issues. Try with different documents or check the PDF accessibility.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5639413-90d6-42ae-add4-6c89da0297e2",
   "metadata": {},
   "source": [
    "## üîç Testing RAG Retrieval and Generation\n",
    "\n",
    "Now let's test the complete **RAG Pipeline** - this demonstrates how all three layers work together:\n",
    "\n",
    "### The RAG Process:\n",
    "1. **üîç Query Processing**: Convert user question into embeddings\n",
    "2. **üìö Semantic Retrieval**: Find most similar document chunks in vector database  \n",
    "3. **üîó Context Assembly**: Combine user question with retrieved chunks\n",
    "4. **ü§ñ Generation**: LLM generates informed response using both its training and the retrieved context\n",
    "5. **üìñ Citation**: Response includes references to source documents\n",
    "\n",
    "**Why this works better than normal LLMs:**\n",
    "- **Grounded responses**: Answers are based on your specific documents\n",
    "- **Up-to-date**: Add new documents without retraining the model\n",
    "- **Traceable**: Every answer can be traced back to source material\n",
    "- **Accurate**: Reduces hallucination by providing factual context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d39ab00-2a65-4b72-b5ed-4dd61f1204a2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# === Test Queries ===\n",
    "# These questions will test our RAG system's ability to find and synthesize information\n",
    "queries = [\n",
    "    \"What are the types of Canopy?\",        # Tests retrieval of categorical information\n",
    "    \"What is the structure of Canopy?\",     # Tests retrieval of structural/descriptive information\n",
    "]\n",
    "\n",
    "# === RAG Pipeline Testing Loop ===\n",
    "for prompt in queries:\n",
    "    cprint(f\"\\nUser> {prompt}\", \"blue\")\n",
    "    \n",
    "    # === STEP 1: RAG RETRIEVAL ===\n",
    "    # Query the vector database to find relevant document chunks\n",
    "    # This uses semantic similarity - the question gets converted to embeddings\n",
    "    # and matched against document chunk embeddings\n",
    "    rag_response = client.tool_runtime.rag_tool.query(\n",
    "        content=prompt,                              # The user's question\n",
    "        vector_db_ids=[vector_db_id],               # Which vector database(s) to search\n",
    "        query_config={                              # How to format the retrieved results\n",
    "            \"chunk_template\": \"Result {index}\\nContent: {chunk.content}\\nMetadata: {metadata}\\n\",\n",
    "        },\n",
    "    )\n",
    "\n",
    "    # Display the full RAG response structure\n",
    "    cprint(rag_response)\n",
    "\n",
    "    # === STEP 2: EXAMINE RETRIEVED METADATA ===\n",
    "    # The metadata contains information about which documents were matched\n",
    "    # and their relevance scores\n",
    "    cprint(f\"\\n--- RAG Metadata ---\", \"yellow\")\n",
    "    cprint(rag_response.metadata, \"cyan\")\n",
    "\n",
    "    # === STEP 3: PREPARE MESSAGES FOR LLM ===\n",
    "    # Structure the conversation with system prompt and user query\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}\n",
    "    ]\n",
    "\n",
    "    # === STEP 4: CONTEXT INJECTION ===\n",
    "    # This is the key to RAG: we inject the retrieved content as context\n",
    "    # The LLM now has both its training knowledge AND the specific document content\n",
    "    prompt_context = rag_response.content\n",
    "    extended_prompt = f\"Please answer the given query using the context below.\\n\\nCONTEXT:\\n{prompt_context}\\n\\nQUERY:\\n{prompt}\"\n",
    "    messages.append({\"role\": \"user\", \"content\": extended_prompt})\n",
    "\n",
    "    # === STEP 5: LLM GENERATION WITH CONTEXT ===\n",
    "    # The LLM generates a response using both its training and the retrieved context\n",
    "    response = client.inference.chat_completion(\n",
    "        messages=messages,                          # The conversation including context\n",
    "        model_id=model_id,                         # Which model to use for generation\n",
    "        sampling_params=sampling_params,           # How to generate (greedy vs sampling)\n",
    "        stream=True,                               # Stream the response token by token\n",
    "    )\n",
    "\n",
    "    # === STEP 6: DISPLAY GENERATED RESPONSE ===\n",
    "    # Show the final answer that combines the LLM's knowledge with document facts\n",
    "    cprint(\"inference> \", color=\"magenta\", end='')\n",
    "    \n",
    "    # Handle streaming response - tokens arrive one by one\n",
    "    for event in response:\n",
    "        # Some SDKs surface \"delta\" at the top level; others nest under \".event\"\n",
    "        ev = getattr(event, \"event\", event)  # Fall back to event itself\n",
    "        delta = getattr(ev, \"delta\", None)\n",
    "\n",
    "        if delta is None:\n",
    "            # Non-delta events: message_start, message_end, tool_started, heartbeats, etc.\n",
    "            continue\n",
    "\n",
    "        # Extract and display text tokens as they arrive\n",
    "        text = getattr(delta, \"text\", None)\n",
    "        if isinstance(text, str):\n",
    "            cprint(text, color=\"magenta\", end='')\n",
    "            continue\n",
    "\n",
    "        # Handle any tool call tokens (if the model decides to use tools)\n",
    "        tool_call = getattr(delta, \"tool_call\", None)\n",
    "        if tool_call is not None:\n",
    "            cprint(str(tool_call), color=\"magenta\", end='')\n",
    "            continue\n",
    "    \n",
    "    cprint(f\"\\n--- End of RAG Answer ---\", \"blue\")\n",
    "\n",
    "print(\"\\nüéâ RAG Pipeline Complete!\")\n",
    "print(\"üîç Notice how the responses reference specific information from the documents\")\n",
    "print(\"üìö This is the power of RAG: grounded, factual, and citable answers\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bb5b323",
   "metadata": {},
   "source": [
    "## üéâ You've Built a Complete RAG System!\n",
    "\n",
    "**What you accomplished:**\n",
    "- **üóÑÔ∏è Storage Layer**: Registered and configured Milvus vector database with proper embeddings\n",
    "- **üîß RAG Layer**: Used LlamaStack's RAG Tool for automatic document processing and chunking\n",
    "- **ü§ñ User Layer**: Built query processing with context-aware generation and streaming responses\n",
    "- **üìä End-to-End Pipeline**: Demonstrated retrieval ‚Üí context injection ‚Üí generation ‚Üí citation\n",
    "\n",
    "**Key Technical Insights:**\n",
    "- **Semantic Search**: Questions find relevant content by meaning, not just keyword matching\n",
    "- **Document Chunking**: Large documents are split optimally (512 tokens) for precise retrieval\n",
    "- **Context Injection**: The magic happens when retrieved chunks become context for the LLM\n",
    "- **Grounded Generation**: Responses are factual because they reference specific document content\n",
    "\n",
    "**RAG vs Standard LLMs:**\n",
    "| Standard LLM | RAG-Enhanced LLM |\n",
    "|--------------|------------------|\n",
    "| ‚ùå Limited to training data | ‚úÖ Access to your documents |\n",
    "| ‚ùå Can hallucinate facts | ‚úÖ Grounded in real sources |\n",
    "| ‚ùå No citations | ‚úÖ Traceable references |\n",
    "| ‚ùå Static knowledge | ‚úÖ Updatable knowledge base |\n",
    "\n",
    "**Advanced RAG Patterns to Explore:**\n",
    "- **Multi-Document Reasoning**: Synthesize information across multiple sources\n",
    "- **Conversational RAG**: Maintain context across multiple questions\n",
    "- **Hybrid Search**: Combine semantic and keyword search\n",
    "- **Agent Workflows**: Let AI agents decide when and how to search documents\n",
    "\n",
    "Your RAG system can now intelligently answer questions using document knowledge - the foundation for intelligent, knowledge-aware applications! üöÄ"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "app-root",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
