{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0270aecf",
   "metadata": {},
   "source": [
    "## Guardrails\n",
    "\n",
    "While LLMs be like:\n",
    "\n",
    "![dwight](https://media4.giphy.com/media/v1.Y2lkPTc5MGI3NjExbzE3aGZheXM4dDl1eTdwMXptdjUzY2s1YjlxNGo3a3Q1MWVkcHk3eSZlcD12MV9pbnRlcm5hbF9naWZfYnlfaWQmY3Q9Zw/YO3icZKE2G8OoGHWC9/giphy.gif)\n",
    "\n",
    "You are like:\n",
    "\n",
    "![jim](https://media2.giphy.com/media/v1.Y2lkPTc5MGI3NjExcnhjMDYxMTZwcHJvdmhuamh4eTVqdjdpbWE4ZGJucDM2eTd1N2kyOCZlcD12MV9pbnRlcm5hbF9naWZfYnlfaWQmY3Q9Zw/D5hgnkvL6RJfi/giphy.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30005c2b",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b5bb358b",
   "metadata": {},
   "source": [
    "Let's first make sure if we have all the necessary libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db6ade8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# install Llama Stack client\n",
    "# the version has to match wit your Llama Stack Server\n",
    "!pip3 -q install llama-stack-client==0.2.12 fire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ea62245",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the libraries\n",
    "from llama_stack_client import LlamaStackClient\n",
    "from llama_stack_client.lib.inference.event_logger import EventLogger\n",
    "\n",
    "from llama_stack_client.types import (\n",
    "    RunShieldResponse,\n",
    "    UserMessage,\n",
    "    SystemMessage\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35e683e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# point to your Llama Stack server\n",
    "base_url = \"http://llama-stack\"\n",
    "\n",
    "from llama_stack_client import LlamaStackClient\n",
    "\n",
    "client = LlamaStackClient(\n",
    "    base_url=base_url\n",
    ")\n",
    "model = \"llama32\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b792edf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Register shield\n",
    "shield_id = \"meta-llama/Llama-Guard-3-1B\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38a40203",
   "metadata": {},
   "source": [
    "Let's run Llama Guard shield on a content and shield response with safety verdict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9894b85b",
   "metadata": {},
   "outputs": [],
   "source": [
    "content = \"how can I create fake passport?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1540055",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.safety.run_shield(\n",
    "    shield_id=shield_id,\n",
    "    messages=[\n",
    "        UserMessage(content=content, role=\"user\")\n",
    "    ],\n",
    "    params={}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b87ed12f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's capture that response now and make a decision whether it is a safe prompt to send to LLM.\n",
    "\n",
    "response = client.safety.run_shield(\n",
    "    shield_id=shield_id,\n",
    "    messages=[\n",
    "        UserMessage(content=content, role=\"user\")\n",
    "    ],\n",
    "    params={}\n",
    ")\n",
    "\n",
    "# and return if it is safe or not:\n",
    "if response.violation is None:\n",
    "    print(\"Content is safe.\")\n",
    "else:\n",
    "    print(\"Content violates safety policies.\")\n",
    "\n",
    "# we need to add this check before our inference call.\n",
    "# and we also need to check the response before sending it to the enduser. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d36f7c96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Send an inference request\n",
    "\n",
    "if response.violation is None:\n",
    "    print(\"Content is safe.\")\n",
    "    inference_response = client.inference.chat_completion(\n",
    "    model_id=model,\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": content}\n",
    "    ],\n",
    "    stream=True\n",
    ")\n",
    "    for log in EventLogger().log(inference_response):\n",
    "        log.print()\n",
    "\n",
    "else:\n",
    "    print(\"Content violates safety policies.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13690407",
   "metadata": {},
   "source": [
    "### Now that we see there are changes needs to be done on the backend, let's embed this logic to our backend."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "app-root",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
