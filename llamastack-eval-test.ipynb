{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bdf471be",
   "metadata": {},
   "source": [
    "# Llamastack Eval ü¶ô"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b78b9758",
   "metadata": {},
   "source": [
    "We would like to be able to evaluate our models and applications before they go into production.  \n",
    "To do that we can use the llamastack eval endpoint üôå  \n",
    "It allows us to run prompts and expected answers through different evaluations to make sure that the model answers as we expect.  \n",
    "The prompts and expected answers can either be some you custom add, or it can be taken from an evaluation dataset such as this one from HuggingFace: https://huggingface.co/datasets/llamastack/simpleqa  \n",
    "\n",
    "We will be testing two types of evaluations here:\n",
    "- \"subset_of\" which tests if the LLM output is an exact subset of the expected answer \n",
    "- \"llm_as_judge\" which lets an LLM evaluate how similar the LLM output is to the expected answer\n",
    "\n",
    "In here we will both test the raw model to see how performant it is, as well as the backend endpoints as well so that we also evaluate how effective our system prompts are. We will se even more on evaluating the raw model in a later chapter üòâ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9ef944b",
   "metadata": {},
   "source": [
    "## Set-up\n",
    "Let's start by installing the llamastack client and pointing it to our llamastack server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5ba7f6d-9c21-411b-9446-7cd7b996564f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q git+https://github.com/meta-llama/llama-stack.git@release-0.2.12 rich"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dad4681f-cc69-481b-96ae-20805d9d3b48",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_stack_client import LlamaStackClient\n",
    "from rich.pretty import Pretty\n",
    "\n",
    "base_url = \"http://llama-stack.user1-test.svc.cluster.local:80\"\n",
    "client = LlamaStackClient(\n",
    "    base_url=base_url,\n",
    "    timeout=600.0 # Default is 1 min which is far too little for some agentic tests, we set it to 10 min\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d46e1492",
   "metadata": {},
   "source": [
    "# Llamastack Eval endpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec78c9c3",
   "metadata": {},
   "source": [
    "Before we evaluate our backend endpoint, let's just try out llamastack eval and see how it works.  \n",
    "Here we create some `handmade_eval_rows` with the input and the expected answer, but we also add the generated answer already filled out.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c94a9af7",
   "metadata": {},
   "source": [
    "## Subset Of"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc8b469c-2242-4e44-ab20-ad46b220c828",
   "metadata": {},
   "source": [
    "Let's start by setting the `scoring_params` to use the `subset_of` function mentioned earlier and see what it comes back with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "200fe9e1-b9ea-4b6d-b991-117531c870f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "handmade_eval_rows = [\n",
    "    {\n",
    "        \"input_query\": \"What is your favorite food?\",\n",
    "        \"generated_answer\": \"Tapas are my favorites.\",\n",
    "        \"expected_answer\": \"Tapas\",\n",
    "    },\n",
    "    {\n",
    "        \"input_query\": \"What is your favorite food?\",\n",
    "        \"generated_answer\": \"I really like tapas.\",\n",
    "        \"expected_answer\": \"Tapas\",\n",
    "    }\n",
    "]\n",
    "Pretty(handmade_eval_rows)\n",
    "\n",
    "scoring_params = {\n",
    "    \"basic::subset_of\": None,\n",
    "}\n",
    "scoring_response = client.scoring.score(\n",
    "    input_rows=handmade_eval_rows, scoring_functions=scoring_params\n",
    ")\n",
    "Pretty(scoring_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "174c2599",
   "metadata": {},
   "source": [
    "Hmm, we got half of the answers correct ü§î  \n",
    "This is because we expect tapas to be spelled with a big T in front. As mentioned before, the subset_of function expects exact matches within the generated answer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "412e97f3-90b2-47c3-b2ef-49865cb6742b",
   "metadata": {},
   "source": [
    "## LLM as judgeüßë‚Äç‚öñÔ∏è"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c25f3a1e",
   "metadata": {},
   "source": [
    "Now let's try the same thing but with LLM as judge.   \n",
    "Here we are going to feed our eval_rows into an LLM to evaluate how well the generated answer matches the expected answer.  \n",
    "To make sure that the judge LLM does this, we also provide it a `JUDGE_PROMPT` it should follow, as well as a regex of expected scores (`judge_score_regexes`) from the judge.  \n",
    "In our case we let it grade the generated answers from A to E (no F's in this class üôÇ‚Äç‚ÜîÔ∏è), each with its own meaning that you can see in the judge prompt.  \n",
    "We also choose Llama 3.2 to be our judge (as we all know llamas to be the best of judges). This means that when we later later evaluate replies from the backend, we will use the same LLM to generate our answer and judge them, essentially doing a self-judge strategy. This is not always the best, but works pretty well with Llama 3.2 and we don't have any other model to use right now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d830cced-784a-49e5-a780-fbffee2ed03d",
   "metadata": {},
   "outputs": [],
   "source": [
    "handmade_eval_rows = [\n",
    "    {\n",
    "        \"input_query\": \"What is your favorite food?\",\n",
    "        \"generated_answer\": \"Tapas are my favorites.\",\n",
    "        \"expected_answer\": \"Tapas\",\n",
    "    },\n",
    "    {\n",
    "        \"input_query\": \"What is your favorite food?\",\n",
    "        \"generated_answer\": \"I really like tapas.\",\n",
    "        \"expected_answer\": \"Tapas\",\n",
    "    }\n",
    "]\n",
    "Pretty(handmade_eval_rows)\n",
    "\n",
    "judge_model_id = \"llama32\"\n",
    "JUDGE_PROMPT = \"\"\"\n",
    "Given a QUESTION and GENERATED_RESPONSE and EXPECTED_RESPONSE.\n",
    "\n",
    "Compare the factual content of the GENERATED_RESPONSE with the EXPECTED_RESPONSE. Ignore any differences in style, grammar, or punctuation.\n",
    "  The GENERATED_RESPONSE may either be a subset or superset of the EXPECTED_RESPONSE, or it may conflict with it. Determine which case applies. Answer the question by selecting one of the following options:\n",
    "  (A) The GENERATED_RESPONSE is a subset of the EXPECTED_RESPONSE and is fully consistent with it.\n",
    "  (B) The GENERATED_RESPONSE is a superset of the EXPECTED_RESPONSE and is fully consistent with it.\n",
    "  (C) The GENERATED_RESPONSE contains all the same details as the EXPECTED_RESPONSE.\n",
    "  (D) There is a disagreement between the GENERATED_RESPONSE and the EXPECTED_RESPONSE.\n",
    "  (E) The answers differ, but these differences don't matter from the perspective of factuality.\n",
    "\n",
    "Give your answer in the format \"Answer: One of ABCDE, Explanation: \".\n",
    "\n",
    "Your actual task:\n",
    "\n",
    "QUESTION: {input_query}\n",
    "GENERATED_RESPONSE: {generated_answer}\n",
    "EXPECTED_RESPONSE: {expected_answer}\n",
    "\"\"\"\n",
    "\n",
    "scoring_params = {\n",
    "    \"llm-as-judge::base\": {\n",
    "        \"judge_model\": judge_model_id,\n",
    "        \"prompt_template\": JUDGE_PROMPT,\n",
    "        \"type\": \"llm_as_judge\",\n",
    "        \"judge_score_regexes\": [\"Answer: (A|B|C|D|E)\"],\n",
    "    },\n",
    "}\n",
    "\n",
    "scoring_response = client.scoring.score(\n",
    "    input_rows=handmade_eval_rows, scoring_functions=scoring_params\n",
    ")\n",
    "Pretty(scoring_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "360d4cbd",
   "metadata": {},
   "source": [
    "You should have gotten at least a C from the judge, and you can see the reasoning for it in the `judge_feedback` field.  \n",
    "Feel free to try out some other inputs, generated answers, and expected answers üß™"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64c65b29",
   "metadata": {},
   "source": [
    "# Involve the LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e874d58c",
   "metadata": {},
   "source": [
    "So far we have just hardcoded the generated answers but these should be generated from an LLM, otherwise we are just evaluating our human selves.  \n",
    "To do this, let's send some requests to our LLM through llamastack, and then also to our backend and see how that looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da1a3396",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"llama32\"\n",
    "\n",
    "model_eval_rows = [\n",
    "    {\n",
    "        \"input_query\": \"What is your favorite Spanish food?\",\n",
    "        \"expected_answer\": \"Tapas\",\n",
    "    },\n",
    "    {\n",
    "        \"input_query\": \"What is your favorite Turkish food?\",\n",
    "        \"expected_answer\": \"Baklava\",\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27abe1b1",
   "metadata": {},
   "source": [
    "Note how this time we don't have any generated answers yet, those will come from the LLM directly this time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1277bcd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "for eval_row in model_eval_rows:\n",
    "    response = client.inference.chat_completion(\n",
    "        model_id=model_id,\n",
    "        messages=[\n",
    "            {\"role\": \"user\", \"content\": eval_row[\"input_query\"]}\n",
    "        ]\n",
    "    )\n",
    "    eval_row[\"generated_answer\"] = response.completion_message.content\n",
    "\n",
    "Pretty(model_eval_rows)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea77dc63",
   "metadata": {},
   "source": [
    "And now we can just evaluate these answers just like we did before ‚úÖ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "797c2692",
   "metadata": {},
   "outputs": [],
   "source": [
    "scoring_response = client.scoring.score(\n",
    "    input_rows=model_eval_rows, scoring_functions=scoring_params\n",
    ")\n",
    "Pretty(scoring_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d0c9b5d",
   "metadata": {},
   "source": [
    "# Evaluate the Backend"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57f80077",
   "metadata": {},
   "source": [
    "We can also evaluate our backend instead of the model, all we need to do is send the `input queries` to the backend and put the inputs into `generated_answer`.  \n",
    "Since our backend is prompted to summarize text, we add tests that works well for such tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a228d6bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "backend_url = \"http://canopy-backend.user1-canopy.svc.cluster.local:8000\"\n",
    "endpoint_to_test = \"/summarize\"\n",
    "\n",
    "backend_eval_rows = [\n",
    "    {\n",
    "        \"input_query\": \"Llama 3.2 is a state-of-the-art language model that excels in various natural language processing tasks, including summarization, translation, and question answering.\",\n",
    "        \"expected_answer\": \"Llama 3.2 is a top-tier language model for NLP tasks.\",\n",
    "    },\n",
    "    {\n",
    "        \"input_query\": \"Artificial intelligence and machine learning have revolutionized numerous industries in recent years. \\\n",
    "From healthcare diagnostics that can detect diseases earlier than human doctors, to autonomous vehicles that promise safer transportation, \\\n",
    "to recommendation systems that personalize our digital experiences, AI technologies are becoming increasingly sophisticated. \\\n",
    "However, these advances also bring challenges including ethical concerns about bias in algorithms, job displacement due to automation, and the need for robust data privacy protections?\",\n",
    "        \"expected_answer\": \"AI and ML have transformed industries through healthcare diagnostics, autonomous vehicles, and recommendation systems, but also raise concerns about bias, job displacement, and privacy.\",\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f323c083",
   "metadata": {},
   "outputs": [],
   "source": [
    "def send_request(payload, url):\n",
    "    import httpx\n",
    "    import json\n",
    "    full_response = \"\"\n",
    "\n",
    "    with httpx.Client(timeout=None) as client:\n",
    "        with client.stream(\"POST\", url, json=payload) as response:\n",
    "            for line in response.iter_lines():\n",
    "                if line.startswith(\"data: \"):\n",
    "                    try:\n",
    "                        data = json.loads(line[len(\"data: \"):])\n",
    "                        full_response += data.get(\"delta\", \"\")\n",
    "                    except json.JSONDecodeError:\n",
    "                        continue\n",
    "\n",
    "    return full_response\n",
    "\n",
    "def prompt_backend(prompt, backend_url, endpoint_to_test):\n",
    "    from urllib.parse import urljoin\n",
    "    url = urljoin(backend_url, endpoint_to_test)\n",
    "    payload = {\n",
    "        \"prompt\": prompt\n",
    "    }\n",
    "    return send_request(payload, url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "628c33e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "for eval_row in backend_eval_rows:\n",
    "    eval_row[\"generated_answer\"] = prompt_backend(eval_row[\"input_query\"], backend_url, endpoint_to_test)\n",
    "\n",
    "Pretty(backend_eval_rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e814b840",
   "metadata": {},
   "source": [
    "And again, as soon as we have what we want to evaluate in a json format, we can evaluate it with Llamastack."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffb3e838",
   "metadata": {},
   "outputs": [],
   "source": [
    "scoring_response = client.scoring.score(\n",
    "    input_rows=backend_eval_rows, scoring_functions=scoring_params\n",
    ")\n",
    "Pretty(scoring_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d78602af",
   "metadata": {},
   "source": [
    "# Datasets üìñ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "370a44f0",
   "metadata": {},
   "source": [
    "Finally, let's use a dataset with already populated inputs and expected answers and see how well our model and backend does against those."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c40aa068-836d-4ce7-b687-ae0d8aaf7ef7",
   "metadata": {},
   "source": [
    "## SimpleQA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf386e2a",
   "metadata": {},
   "source": [
    "SimpleQA is a (as the name suggests) simple dataset with questions and answers that you can run on your model.  \n",
    "You can find the full dataset, as well as a few others, here: https://huggingface.co/llamastack/datasets  \n",
    "In our case, we don't want to wait to evaluate the full dataset, so we will take 5 examples from the training part of this dataset to test our model on.  \n",
    "Similarily, we will just test on our model this time, partially because our `summarize` endpoint is not prompted for handling QA, and partially because we have already seen how we can evaluate our backend above.  \n",
    "\n",
    "To fetch the dataset we use Llamastack again, where we can register the dataset which allows us to fetch data from it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdea22fe-3ce5-42e1-a6c1-e80b574309e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "simpleqa_dataset_id = \"huggingface::simpleqa\"\n",
    "\n",
    "_ = client.datasets.register(\n",
    "    purpose=\"eval/messages-answer\",\n",
    "    source={\n",
    "        \"type\": \"uri\",\n",
    "        \"uri\": \"huggingface://datasets/llamastack/simpleqa?split=train\",\n",
    "    },\n",
    "    dataset_id=simpleqa_dataset_id,\n",
    ")\n",
    "\n",
    "dataset_eval_rows = client.datasets.iterrows(\n",
    "    dataset_id=simpleqa_dataset_id,\n",
    "    limit=5,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e9e5a99-3bf4-4fe5-8576-3d04ca91b7ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "Pretty(dataset_eval_rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b399035",
   "metadata": {},
   "source": [
    "As you can see, the dataset is already formated with `input_query` and `expected_answer`, and we get some extra information such as `metadata` and `chat_completion_input`.  \n",
    "\n",
    "We could now just input this evaluation set to our model just like we did before, but since we are only evaluating the model we will make use of another Llamastack functionality called `benchmarks`.  \n",
    "This simply passes the dataset through the model and returns the response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8309743-b63d-4275-8ed0-e039d0d57b2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.benchmarks.register(\n",
    "    benchmark_id=\"meta-reference::simpleqa\",\n",
    "    dataset_id=simpleqa_dataset_id,\n",
    "    scoring_functions=[\"llm-as-judge::base\"],\n",
    ")\n",
    "\n",
    "response = client.eval.evaluate_rows(\n",
    "    benchmark_id=\"meta-reference::simpleqa\",\n",
    "    input_rows=dataset_eval_rows.data,\n",
    "    scoring_functions=[\"llm-as-judge::base\"],\n",
    "    benchmark_config={\n",
    "        \"eval_candidate\": {\n",
    "            \"type\": \"model\",\n",
    "            \"model\": model_id,\n",
    "            \"sampling_params\": {\n",
    "                \"strategy\": {\n",
    "                    \"type\": \"greedy\",\n",
    "                },\n",
    "                \"max_tokens\": 4096,\n",
    "                \"repeat_penalty\": 1.0,\n",
    "            },\n",
    "        },\n",
    "    },\n",
    ")\n",
    "Pretty(response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a173fc53",
   "metadata": {},
   "source": [
    "To summarize, we have now used the Llamastack Eval endpoint to evaluate our raw LLM as well as our backend system, both with custom evaluations and with those fetched from a dataset.  \n",
    "With this knowledge, we can now build an evaluation workflow that lets us test our backend and model before it goes into production üëè"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "app-root",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
