{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Tool & LLM Integration Demo\n",
    "\n",
    "Let's start with a very simple example for how tools work.  \n",
    "In this case, we will create the tool here inside the notebook, so that you can see how it works, and then use it both manually and with an LLM.  \n",
    "The tool we will test here is a simple calculator, and although it's simple it's quite effective when combined with LLMs as they often have trouble calculating."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the calculator tool\n",
    "\n",
    "A tool is essentially a function that:\n",
    "- Takes structured input (JSON)\n",
    "- Performs a specific action\n",
    "- Returns structured output (JSON)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from typing import Dict, Any\n",
    "\n",
    "class CalculatorTool:\n",
    "    \"\"\"A simple calculator tool that performs basic arithmetic operations.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.name = \"calculator\"\n",
    "        self.description = \"\"\"Performs basic arithmetic operations (add, subtract, multiply, divide). \n",
    "Expected input format:\n",
    "{\n",
    "    \"operation\": \"add\" | \"subtract\" | \"multiply\" | \"divide\",\n",
    "    \"a\": float,\n",
    "    \"b\": float\n",
    "}\n",
    "\n",
    "Returns:\n",
    "{\n",
    "    \"result\": float\n",
    "}\n",
    "\"\"\"\n",
    "    \n",
    "    def execute(self, tool_input: Dict[str, Any]) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Execute the calculator tool.\n",
    "        \n",
    "        Expected input format:\n",
    "        {\n",
    "            \"operation\": \"add\" | \"subtract\" | \"multiply\" | \"divide\",\n",
    "            \"a\": float,\n",
    "            \"b\": float\n",
    "        }\n",
    "        \n",
    "        Returns:\n",
    "        {\n",
    "            \"result\": float\n",
    "        }\n",
    "        \"\"\"\n",
    "        try:\n",
    "            operation = tool_input.get(\"operation\")\n",
    "            a = float(tool_input.get(\"a\", 0))\n",
    "            b = float(tool_input.get(\"b\", 0))\n",
    "            \n",
    "            if operation == \"add\":\n",
    "                result = a + b\n",
    "            elif operation == \"subtract\":\n",
    "                result = a - b\n",
    "            elif operation == \"multiply\":\n",
    "                result = a * b\n",
    "            elif operation == \"divide\":\n",
    "                if b == 0:\n",
    "                    return {\n",
    "                        \"error\": \"Division by zero is not allowed\",\n",
    "                    }\n",
    "                result = a / b\n",
    "            else:\n",
    "                return {\n",
    "                    \"error\": f\"Unknown operation: {operation}\",\n",
    "                }\n",
    "            \n",
    "            return {\n",
    "                \"result\": result,\n",
    "            }\n",
    "        except Exception as e:\n",
    "            return {\n",
    "                \"error\": str(e)\n",
    "            }\n",
    "\n",
    "# Create an instance of the tool\n",
    "calculator = CalculatorTool()\n",
    "print(f\"Tool created: {calculator.name}\")\n",
    "print(f\"Description: {calculator.description}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the Tool Directly\n",
    "\n",
    "Now let's test our tool by sending a few JSON inputs and receiving JSON outputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 1: Addition\n",
    "input_json = {\n",
    "    \"operation\": \"add\",\n",
    "    \"a\": 15,\n",
    "    \"b\": 27\n",
    "}\n",
    "\n",
    "print(\"Input JSON:\")\n",
    "print(json.dumps(input_json, indent=2))\n",
    "\n",
    "output = calculator.execute(input_json)\n",
    "\n",
    "print(\"\\nOutput JSON:\")\n",
    "print(json.dumps(output, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 2: Multiplication\n",
    "input_json = {\n",
    "    \"operation\": \"multiply\",\n",
    "    \"a\": 8,\n",
    "    \"b\": 7\n",
    "}\n",
    "\n",
    "print(\"Input JSON:\")\n",
    "print(json.dumps(input_json, indent=2))\n",
    "\n",
    "output = calculator.execute(input_json)\n",
    "\n",
    "print(\"\\nOutput JSON:\")\n",
    "print(json.dumps(output, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 3: Division\n",
    "input_json = {\n",
    "    \"operation\": \"divide\",\n",
    "    \"a\": 100,\n",
    "    \"b\": 4\n",
    "}\n",
    "\n",
    "print(\"Input JSON:\")\n",
    "print(json.dumps(input_json, indent=2))\n",
    "\n",
    "output = calculator.execute(input_json)\n",
    "\n",
    "print(\"\\nOutput JSON:\")\n",
    "print(json.dumps(output, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! That's the basics of how a tool works, it's simply a service that accepts json input and responds with json output.  \n",
    "But so far we are calling it ourselves, which is great and all but we can already use calculators, so how do we get the LLM to use it?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLM in the Loop\n",
    "\n",
    "Now let's integrate an LLM to:\n",
    "1. Understand a user's request\n",
    "2. Format the request as proper JSON for the tool\n",
    "3. Call the tool\n",
    "4. Interpret the results for the user and respond\n",
    "\n",
    "First, let's set up the Llama Stack client:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 -q install llama-stack-client==0.3.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_stack_client import LlamaStackClient\n",
    "from llama_stack_client.lib.inference.event_logger import EventLogger\n",
    "\n",
    "# Connect to Llama Stack Server\n",
    "base_url = \"http://llama-stack-service:8321\"\n",
    "client = LlamaStackClient(base_url=base_url)\n",
    "model = \"llama32\"\n",
    "\n",
    "print(f\"Connected to Llama Stack at: {base_url}\")\n",
    "print(f\"Using model: {model}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LLM interprets user request and generates tool call\n",
    "\n",
    "Notice how we can use the tool description in here when we craft our system prompt. This will be very useful later as we add more and more tools - we won't need to manually update the prompt, we just reference the tool's description!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step-by-Step Agent Walkthrough\n",
    "\n",
    "Let's walk through each step individually so you can see the inputs and outputs clearly.\n",
    "\n",
    "**Step 1: Define the user request**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is what the user asks in natural language\n",
    "user_request = \"What is 45 plus 78?\"\n",
    "\n",
    "print(\"USER REQUEST:\")\n",
    "print(user_request)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 2: Create a system prompt using the tool description**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the system prompt dynamically using the tool's description\n",
    "system_prompt = f\"\"\"You are a helpful assistant that converts natural language requests into JSON tool calls.\n",
    "\n",
    "Available tool: {calculator.name}\n",
    "{calculator.description}\n",
    "\n",
    "When the user asks you to perform a calculation, respond with ONLY a JSON object following the format above.\n",
    "Do not include any explanation, just return the JSON object.\n",
    "\"\"\"\n",
    "\n",
    "print(\"SYSTEM PROMPT:\")\n",
    "print(system_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 3: Send the request to the LLM to generate a tool call**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ask the LLM to convert the natural language request into a tool call\n",
    "response = client.chat.completions.create(\n",
    "    model=model,\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": user_request}\n",
    "    ],\n",
    "    stream=False,\n",
    ")\n",
    "\n",
    "# Extract the LLM's response\n",
    "llm_response = response.choices[0].message.content\n",
    "\n",
    "print(\"LLM RESPONSE:\")\n",
    "print(llm_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 4: Parse the LLM response and send it to the tool**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up the response (sometimes LLMs wrap JSON in markdown code blocks)\n",
    "cleaned_response = llm_response.strip()\n",
    "if cleaned_response.startswith(\"```\"):\n",
    "    # Remove markdown code block markers\n",
    "    lines = cleaned_response.split(\"\\n\")\n",
    "    cleaned_response = \"\\n\".join(lines[1:-1])\n",
    "    if cleaned_response.startswith(\"json\"):\n",
    "        cleaned_response = cleaned_response[4:].strip()\n",
    "\n",
    "# Parse the JSON\n",
    "tool_call = json.loads(cleaned_response)\n",
    "\n",
    "tool_result = calculator.execute(tool_call)\n",
    "\n",
    "print(\"TOOL RESULT:\")\n",
    "print(json.dumps(tool_result, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 5: Send the result back to the LLM for interpretation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ask the LLM to answer the user's question with the tool result\n",
    "interpretation_prompt = f\"\"\"User question: {user_request}\n",
    "Tool result: {json.dumps(tool_result)}\n",
    "\n",
    "Answer the user's question directly with the result. Be concise.\"\"\"\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=model,\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant. Answer user questions directly and concisely.\"},\n",
    "        {\"role\": \"user\", \"content\": interpretation_prompt}\n",
    "    ],\n",
    "    stream=True,\n",
    ")\n",
    "\n",
    "print(\"FINAL RESPONSE TO USER:\")\n",
    "print(\"-\" * 60)\n",
    "for log in EventLogger().log(response):\n",
    "    log.print()\n",
    "print(\"-\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We got a reply, great!  \n",
    "As usual, feel free to change the prompts if you would prefer a different tone of the response."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Complete Tool Workflow Function\n",
    "\n",
    "Now that you've seen each step individually, let's wrap it all into a reusable function that does everything in one go!  \n",
    "You can see that we call this function `run_agent`, this is because this is a very simple agentic workflow we have right now where it automatically calls the model multiple (2) times and uses a tool.  \n",
    "I know.. it's not very autonomous yet, but hang in there, we are getting there ;)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_agent(user_request: str):\n",
    "    \"\"\"\n",
    "    Complete agent flow:\n",
    "    1. User makes a natural language request\n",
    "    2. LLM interprets and formats as tool call\n",
    "    3. Tool executes\n",
    "    4. LLM interprets results and responds to user\n",
    "    \"\"\"\n",
    "    print(\"=\"*60)\n",
    "    print(f\"USER REQUEST: {user_request}\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    # Step 1: Build system prompt using tool description\n",
    "    system_prompt = f\"\"\"You are a helpful assistant that converts natural language requests into JSON tool calls.\n",
    "\n",
    "Available tool: {calculator.name}\n",
    "{calculator.description}\n",
    "\n",
    "When the user asks you to perform a calculation, respond with ONLY a JSON object following the format above.\n",
    "Do not include any explanation, just return the JSON object.\n",
    "\"\"\"\n",
    "\n",
    "    # Step 2: LLM formats the tool call\n",
    "    print(\"\\n[Step 1] LLM interpreting request...\")\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_request}\n",
    "        ],\n",
    "        stream=False,\n",
    "    )\n",
    "\n",
    "    llm_response = response.choices[0].message.content\n",
    "\n",
    "    # Step 3: Parse the response\n",
    "    try:\n",
    "        cleaned_response = llm_response.strip()\n",
    "        if cleaned_response.startswith(\"```\"):\n",
    "            lines = cleaned_response.split(\"\\n\")\n",
    "            cleaned_response = \"\\n\".join(lines[1:-1])\n",
    "            if cleaned_response.startswith(\"json\"):\n",
    "                cleaned_response = cleaned_response[4:].strip()\n",
    "\n",
    "        tool_call = json.loads(cleaned_response)\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"Error parsing LLM response: {e}\")\n",
    "        print(f\"LLM response was: {llm_response}\")\n",
    "        return\n",
    "\n",
    "    print(\"\\n[Step 2] Generated Tool Call:\")\n",
    "    print(json.dumps(tool_call, indent=2))\n",
    "\n",
    "    # Step 4: Execute the tool\n",
    "    print(\"\\n[Step 3] Executing tool...\")\n",
    "    tool_result = calculator.execute(tool_call)\n",
    "    print(\"\\n[Step 4] Tool Result:\")\n",
    "    print(json.dumps(tool_result, indent=2))\n",
    "\n",
    "    # Step 5: LLM interprets the result\n",
    "    print(\"\\n[Step 5] LLM interpreting results for user...\")\n",
    "    interpretation_prompt = f\"\"\"User question: {user_request}\n",
    "Tool result: {json.dumps(tool_result)}\n",
    "\n",
    "Answer the user's question directly with the result. Be concise.\"\"\"\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant. Answer user questions directly and concisely.\"},\n",
    "            {\"role\": \"user\", \"content\": interpretation_prompt}\n",
    "        ],\n",
    "        stream=True,\n",
    "    )\n",
    "\n",
    "    print(\"\\n[Step 6] FINAL RESPONSE TO USER:\")\n",
    "    print(\"-\" * 60)\n",
    "    for log in EventLogger().log(response):\n",
    "        log.print()\n",
    "    print(\"-\" * 60)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's try it! ðŸŽ‰\n",
    "\n",
    "Now we can make natural language requests and the LLM will:\n",
    "1. Understand what we want\n",
    "2. Format it as a proper tool call\n",
    "3. Execute the tool\n",
    "4. Explain the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 1: Simple addition\n",
    "run_agent(\"What is 45 plus 78?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 2: Multiplication\n",
    "run_agent(\"Can you multiply 12 by 15 for me?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try our error handling as well, what happens if we divide by 0 for example? ðŸ”¥"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 3: Division\n",
    "run_agent(\"I need to divide 144 by 0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 4: Subtraction\n",
    "run_agent(\"What's 100 minus 37?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this notebook, we learned:\n",
    "\n",
    "1. **Creating a Tool**: A tool is a simple class/function that:\n",
    "   - Takes JSON input with specific parameters\n",
    "   - Performs an action (calculation in our case)\n",
    "   - Returns JSON output with results\n",
    "\n",
    "2. **Using a Tool Directly**: We can call tools directly by providing properly formatted JSON\n",
    "\n",
    "3. **LLM in the Loop**: The LLM acts as an intelligent interface that:\n",
    "   - Interprets user requests\n",
    "   - Formats them as proper tool calls\n",
    "   - Interprets tool results for the user\n",
    "   - Provides text responses\n",
    "\n",
    "This pattern is the foundation of **AI Agents** - systems where LLMs can understand user intent, call appropriate tools, and provide meaningful responses! ðŸš€"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
