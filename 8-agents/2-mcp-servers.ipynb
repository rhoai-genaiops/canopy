{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "12272d32",
   "metadata": {},
   "source": [
    "# MCP Servers: Connecting LLMs to External Services\n",
    "\n",
    "In the previous notebook, we created a simple calculator tool directly in Python. But what if we want to connect to external services, databases, or APIs?\n",
    "\n",
    "This is where **Model Context Protocol (MCP)** comes in. MCP is a standard protocol that allows LLMs to interact with external tools and services in a structured way.\n",
    "\n",
    "In this notebook, we'll:\n",
    "1. Explore the Calendar API and Frontend\n",
    "2. Make direct API calls\n",
    "3. Connect to an MCP Server\n",
    "4. Build an agent that uses MCP tools\n",
    "\n",
    "Let's get started! ðŸš€"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "s9hm4pd10b",
   "metadata": {},
   "source": [
    "## 1. Exploring the Calendar Application\n",
    "\n",
    "First, let's check out the deployed calendar application. It consists of:\n",
    "- **Frontend**: A React web UI for managing calendar events\n",
    "- **API**: A FastAPI backend that stores events in a database\n",
    "- **MCP Server**: An MCP-compliant interface to the API\n",
    "\n",
    "Let's start looking at the API, as the MCP simply will be a communication layer between the LLM and that API."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e0eph1zufc",
   "metadata": {},
   "source": [
    "The Calendar API is a REST API with the following endpoints:\n",
    "\n",
    "| Method | Endpoint | Description |\n",
    "|--------|----------|-------------|\n",
    "| GET | `/schedules` | Get all calendar events |\n",
    "| GET | `/schedules/{id}` | Get a specific event |\n",
    "| POST | `/schedules` | Create a new event |\n",
    "| PUT | `/schedules/{id}` | Update an event |\n",
    "| DELETE | `/schedules/{id}` | Delete an event |\n",
    "\n",
    "Each event has this structure:\n",
    "```json\n",
    "{\n",
    "  \"sid\": \"unique-id\",\n",
    "  \"name\": \"Event Name\",\n",
    "  \"content\": \"Description\",\n",
    "  \"category\": \"Lecture|Lab|Meeting|...\",\n",
    "  \"level\": 1-3,  // Priority\n",
    "  \"status\": 0.0-1.0,  // Completion %\n",
    "  \"start_time\": \"YYYY-MM-DD HH:MM:SS\",\n",
    "  \"end_time\": \"YYYY-MM-DD HH:MM:SS\",\n",
    "  \"creation_time\": \"YYYY-MM-DD HH:MM:SS\"\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9s3g86zivc8",
   "metadata": {},
   "source": [
    "## 3. Making Direct API Calls\n",
    "\n",
    "Let's interact with the Calendar API directly using Python's `requests` library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06ce98be",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Disable some logs\n",
    "import logging\n",
    "\n",
    "logging.getLogger(\"httpx\").setLevel(logging.WARNING)\n",
    "logging.getLogger().setLevel(logging.WARNING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e48892ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "calendar_api_url = \"http://canopy-mcp-calendar-api:8000\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2n0in6bes6q",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Get all events from the /schedules endpoint\n",
    "print(\"ðŸ“‹ Getting all events...\")\n",
    "response = requests.get(f\"{calendar_api_url}/schedules\")\n",
    "events = response.json()\n",
    "\n",
    "\n",
    "# Print the events\n",
    "print(f\"Found {len(events)} events:\\n\")\n",
    "for event in events[:5]:  # Show first 5\n",
    "    print(f\"  â€¢ {event['name']} ({event['category']})\")\n",
    "    print(f\"    ðŸ“… {event['start_time']} â†’ {event['end_time']}\")\n",
    "    print(f\"    âœ… {int(event.get('status', 0) * 100)}% complete\\n\")\n",
    "\n",
    "if len(events) > 5:\n",
    "    print(f\"... and {len(events) - 5} more events\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19f84820",
   "metadata": {},
   "source": [
    "Great, we have some events already, let's add another one!  \n",
    "After you have added it, make sure to check in the calendar UI to verify for yourself that it did get added."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ev0l9t5jk7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new event\n",
    "print(\"Creating a new event...\")\n",
    "\n",
    "event_name = \"AI Workshop: Introduction to Agents\"\n",
    "tomorrow = datetime.now() + timedelta(days=1)\n",
    "start_time = tomorrow.replace(hour=14, minute=0, second=0)\n",
    "end_time = start_time + timedelta(hours=1, minutes=30)\n",
    "\n",
    "new_event = {\n",
    "    \"sid\": f\"notebook-test-{int(datetime.now().timestamp())}\",\n",
    "    \"name\": event_name,\n",
    "    \"content\": \"Learn how to build AI agents using LLMs and tools\",\n",
    "    \"category\": \"Workshop\",\n",
    "    \"level\": 2,\n",
    "    \"status\": 0.0,\n",
    "    \"creation_time\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "    \"start_time\": start_time.strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "    \"end_time\": end_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "}\n",
    "\n",
    "response = requests.post(f\"{calendar_api_url}/schedules\", json=new_event)\n",
    "\n",
    "if response.status_code == 200:\n",
    "    created_event = response.json()\n",
    "    print(f\"âœ… Event created successfully!\")\n",
    "    print(f\"   ID: {created_event['sid']}\")\n",
    "    print(f\"   Name: {created_event['name']}\")\n",
    "    print(f\"   Time: {created_event['start_time']} â†’ {created_event['end_time']}\")\n",
    "else:\n",
    "    print(f\"âŒ Error: {response.status_code} - {response.text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "t5cauo4la19",
   "metadata": {},
   "source": [
    "## 4. Connecting to the MCP Server\n",
    "\n",
    "Now let's connect to the **MCP Server**. Unlike the direct API calls above, MCP provides a standardized way for LLMs to discover and use tools.\n",
    "\n",
    "The MCP server wraps the Calendar API and exposes it as **9 structured tools** that an LLM can use:\n",
    "\n",
    "1. `get_all_events` - Get all events with optional filtering\n",
    "2. `get_event` - Get a specific event by ID\n",
    "3. `create_event` - Create a new event\n",
    "4. `update_event` - Update an existing event\n",
    "5. `delete_event` - Delete an event\n",
    "6. `search_events` - Search by name/content\n",
    "7. `get_upcoming_events` - Get events in the next N days\n",
    "8. `get_events_by_date` - Get events for a specific date\n",
    "9. `get_calendar_statistics` - Get calendar stats\n",
    "\n",
    "Let's connect!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pn9t4imtvs",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install MCP SDK\n",
    "!pip3 install -q mcp httpx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0cvbxr1vs4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mcp import ClientSession\n",
    "from mcp.client.sse import sse_client\n",
    "\n",
    "# Connect to the MCP server\n",
    "mcp_server_url = \"http://canopy-mcp-calendar-mcp-server:8080/sse\"\n",
    "print(f\"ðŸ”Œ Connecting to MCP server at {mcp_server_url}...\")\n",
    "\n",
    "# We'll use this in the next cells\n",
    "mcp_session = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9h6dtkc5f2e",
   "metadata": {},
   "source": [
    "### Listing Available MCP Tools\n",
    "\n",
    "Let's see what tools the MCP server provides. This is one of the key features of MCP - **tool discovery**!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mcvi0thnrk",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "\n",
    "async def list_mcp_tools():\n",
    "    \"\"\"Connect to MCP server and list available tools.\"\"\"\n",
    "    async with sse_client(mcp_server_url) as (read, write):\n",
    "        async with ClientSession(read, write) as session:\n",
    "            await session.initialize()\n",
    "            print(\"âœ… Connected to MCP server!\\n\")\n",
    "            \n",
    "            # List available tools\n",
    "            tools = await session.list_tools() # ðŸ‘ˆ This is how we call the MCP functions and tools\n",
    "            print(f\"ðŸ“‹ Found {len(tools.tools)} MCP tools:\\n\")\n",
    "            print(\"---\")\n",
    "            \n",
    "            for i, tool in enumerate(tools.tools, 1):\n",
    "                print(f\"{i}. {tool.name}\")\n",
    "                print(f\"   {tool.description}\")\n",
    "                print(\"---\")\n",
    "            \n",
    "            return session\n",
    "\n",
    "# Run the async function\n",
    "await list_mcp_tools()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85b0dkdggu3",
   "metadata": {},
   "source": [
    "### Calling MCP Tools Directly\n",
    "\n",
    "Now let's call some MCP tools to see how they work:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9pk664pkoj",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def call_mcp_tool_example():\n",
    "    \"\"\"Example: Call get_upcoming_events tool.\"\"\"\n",
    "    async with sse_client(mcp_server_url) as (read, write):\n",
    "        async with ClientSession(read, write) as session:\n",
    "            await session.initialize()\n",
    "            \n",
    "            # Call the get_upcoming_events tool\n",
    "            print(\"ðŸ“… Calling MCP tool: get_upcoming_events\")\n",
    "            print(\"   Arguments: {'days': 7}\\n\")\n",
    "            \n",
    "            result = await session.call_tool(\n",
    "                \"get_upcoming_events\",  # ðŸ‘ˆ The tool we want to call\n",
    "                arguments={\"days\": 7}   # ðŸ‘ˆ The arguments we want to send the tool (see list in last cell for all arguments)\n",
    "            )\n",
    "            \n",
    "            print(\"Result:\")\n",
    "            print(\"-\" * 60)\n",
    "            for content in result.content:\n",
    "                print(content.text)\n",
    "            print(\"-\" * 60)\n",
    "\n",
    "await call_mcp_tool_example()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0g3glv9660tr",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def search_events_example():\n",
    "    \"\"\"Example: Search for events containing 'lecture'.\"\"\"\n",
    "    async with sse_client(mcp_server_url) as (read, write):\n",
    "        async with ClientSession(read, write) as session:\n",
    "            await session.initialize()\n",
    "            \n",
    "            # Search for events\n",
    "            print(\"ðŸ” Calling MCP tool: search_events\")\n",
    "            print(\"   Arguments: {'query': 'lecture'}\\n\")\n",
    "            \n",
    "            result = await session.call_tool(\n",
    "                \"search_events\",\n",
    "                arguments={\"query\": \"lecture\"}\n",
    "            )\n",
    "            \n",
    "            print(\"Result:\")\n",
    "            print(\"-\" * 60)\n",
    "            for content in result.content:\n",
    "                print(content.text)\n",
    "            print(\"-\" * 60)\n",
    "\n",
    "await search_events_example()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sdq2t5wazi9",
   "metadata": {},
   "source": [
    "## 5. Building an Agent with MCP\n",
    "\n",
    "So far we have simply been sending direct request to either the API or the MCP Server, let's introduce the LLM just like in the previous notebook (`1-intro-to-tools.ipynb`).  \n",
    "\n",
    "This will be similar to what we did with the calculator, but now we're using a real external service through MCP.  \n",
    "However, we are adding one more important twist - we are letting the LLM choose what tools to use!  \n",
    "We can do this fairly simply by just giving the LLM the list of tools available and asking it to choose which is best to use and then create the json input for that specific tool.  \n",
    "This gives us a **lot** more flexibility and agency with our agents ðŸ¤–"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b38be821",
   "metadata": {},
   "source": [
    "### Setting up the LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vxnegm539t",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install -q llama-stack-client==0.3.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a778f946",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_stack_client import LlamaStackClient\n",
    "from llama_stack_client.lib.inference.event_logger import EventLogger\n",
    "\n",
    "# Connect to Llama Stack Server\n",
    "base_url = \"http://llama-stack-service:8321\"\n",
    "llm_client = LlamaStackClient(base_url=base_url)\n",
    "llm_model = \"llama32\"\n",
    "\n",
    "print(f\"Connected to Llama Stack at: {base_url}\")\n",
    "print(f\"Using model: {model}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2kkcyx9o1xq",
   "metadata": {},
   "source": [
    "### Step-by-Step Agent Walkthrough\n",
    "\n",
    "Now let's build an agent that can understand natural language (plain text) requests and automatically call MCP tools!\n",
    "\n",
    "We'll walk through each step individually so you can see the inputs and outputs clearly.\n",
    "\n",
    "**Step 1: Define the user request**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8bpkvvikk",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is what the user asks\n",
    "user_request = \"What events do I have coming up in the next 3 days?\"\n",
    "\n",
    "print(\"USER REQUEST:\")\n",
    "print(user_request)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "z01migktz2d",
   "metadata": {},
   "source": [
    "**Step 2: Get MCP tool descriptions**\n",
    "\n",
    "We need to get the available MCP tools and their descriptions to include in the system prompt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fpb8ts854qn",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def get_mcp_tools_description():\n",
    "    \"\"\"Get descriptions of all available MCP tools.\"\"\"\n",
    "    async with sse_client(mcp_server_url) as (read, write):\n",
    "        async with ClientSession(read, write) as session:\n",
    "            await session.initialize()\n",
    "            tools = await session.list_tools()\n",
    "            \n",
    "            tools_description = \"\"\n",
    "            for tool in tools.tools:\n",
    "                tools_description += f\"- {tool.name}: {tool.description}\\n\"\n",
    "            \n",
    "            return tools_description, session\n",
    "\n",
    "# Get the tool descriptions\n",
    "mcp_tools_description = await get_mcp_tools_description()\n",
    "\n",
    "print(\"AVAILABLE MCP TOOLS:\")\n",
    "print(mcp_tools_description[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dv72s1kcwyr",
   "metadata": {},
   "source": [
    "**Step 3: Create a system prompt with the tool descriptions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tlhk0naidb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the system prompt dynamically using the MCP tool descriptions\n",
    "system_prompt = f\"\"\"You are a helpful assistant that converts natural language requests into MCP tool calls.\n",
    "\n",
    "Available MCP tools:\n",
    "{mcp_tools_description[0]}\n",
    "\n",
    "When the user asks you to do something related to calendar events, respond with a JSON object specifying:\n",
    "1. \"tool_name\": the name of the MCP tool to call\n",
    "2. \"arguments\": a dictionary of arguments for that tool\n",
    "\n",
    "Respond with ONLY the JSON object. Do not include any explanation.\n",
    "\n",
    "Example format:\n",
    "{{\n",
    "    \"tool_name\": \"get_upcoming_events\",\n",
    "    \"arguments\": {{\"days\": 7}}\n",
    "}}\n",
    "\"\"\"\n",
    "\n",
    "print(\"SYSTEM PROMPT:\")\n",
    "print(system_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2xwu65h7gad",
   "metadata": {},
   "source": [
    "**Step 4: Send the request to the LLM to generate a tool call**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "kie2tw5ocg8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ask the LLM to convert the natural language request into an MCP tool call\n",
    "response = llm_client.chat.completions.create(\n",
    "    model=llm_model,\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": user_request}\n",
    "    ],\n",
    "    stream=False,\n",
    ")\n",
    "\n",
    "# Extract the LLM's response\n",
    "llm_response = response.choices[0].message.content\n",
    "\n",
    "print(\"USER REQUEST:\")\n",
    "print(user_request)\n",
    "print()\n",
    "print(\"LLM RESPONSE:\")\n",
    "print(llm_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tx5adfh4dus",
   "metadata": {},
   "source": [
    "**Step 5: Parse the LLM response and call the MCP tool**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "km20e6yhvbl",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up the response (sometimes LLMs wrap JSON in markdown code blocks)\n",
    "cleaned_response = llm_response.strip()\n",
    "if cleaned_response.startswith(\"```\"):\n",
    "    # Remove markdown code block markers\n",
    "    lines = cleaned_response.split(\"\\n\")\n",
    "    cleaned_response = \"\\n\".join(lines[1:-1])\n",
    "    if cleaned_response.startswith(\"json\"):\n",
    "        cleaned_response = cleaned_response[4:].strip()\n",
    "\n",
    "# Parse the JSON\n",
    "tool_call = json.loads(cleaned_response)\n",
    "tool_name = tool_call[\"tool_name\"]\n",
    "tool_arguments = tool_call[\"arguments\"]\n",
    "\n",
    "print(f\"TOOL TO CALL: {tool_name}\")\n",
    "print(f\"ARGUMENTS: {json.dumps(tool_arguments, indent=2)}\")\n",
    "\n",
    "# Now call the MCP tool\n",
    "async def call_mcp_tool(tool_name, arguments):\n",
    "    async with sse_client(mcp_server_url) as (read, write):\n",
    "        async with ClientSession(read, write) as session:\n",
    "            await session.initialize()\n",
    "            result = await session.call_tool(tool_name, arguments=arguments)\n",
    "            return result\n",
    "\n",
    "mcp_result = await call_mcp_tool(tool_name, tool_arguments)\n",
    "\n",
    "print(\"\\nMCP TOOL RESULT:\")\n",
    "print(\"-\" * 60)\n",
    "for content in mcp_result.content:\n",
    "    print(content.text)\n",
    "print(\"-\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dt3mkdvisks",
   "metadata": {},
   "source": [
    "**Step 6: Send the result back to the LLM for interpretation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ljr7bycmivs",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract text from MCP result\n",
    "result_text = \"\\n\".join([content.text for content in mcp_result.content])\n",
    "\n",
    "# Ask the LLM to answer the user's question with the tool result\n",
    "interpretation_prompt = f\"\"\"User question: {user_request}\n",
    "MCP Tool result: {result_text}\n",
    "\n",
    "Answer the user's question directly with the result. Be concise and friendly.\"\"\"\n",
    "\n",
    "response = llm_client.chat.completions.create(\n",
    "    model=llm_model,\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant. Answer user questions directly and concisely.\"},\n",
    "        {\"role\": \"user\", \"content\": interpretation_prompt}\n",
    "    ],\n",
    "    stream=True,\n",
    ")\n",
    "\n",
    "print(\"FINAL RESPONSE TO USER:\")\n",
    "print(\"-\" * 60)\n",
    "for log in EventLogger().log(response):\n",
    "    log.print()\n",
    "print(\"-\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "xifnnewtyx",
   "metadata": {},
   "source": [
    "### Complete MCP Agent Function\n",
    "\n",
    "Now that you've seen each step individually, let's wrap it all into a reusable function that does everything in one go!\n",
    "\n",
    "This is our simple agentic workflow that automatically:\n",
    "1. Interprets user requests\n",
    "2. Calls the appropriate MCP tool\n",
    "3. Returns the results in natural language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "chgnljtr2fi",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def run_mcp_agent(user_request: str):\n",
    "    \"\"\"\n",
    "    Complete MCP agent flow:\n",
    "    1. User makes a natural language request\n",
    "    2. LLM interprets and formats as MCP tool call\n",
    "    3. MCP tool executes\n",
    "    4. LLM interprets results and responds to user\n",
    "    \"\"\"\n",
    "    print(\"=\"*60)\n",
    "    print(f\"USER REQUEST: {user_request}\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    # Step 1: Get MCP tools description\n",
    "    print(\"\\n[Step 1] Getting MCP tools...\")\n",
    "    async with sse_client(mcp_server_url) as (read, write):\n",
    "        async with ClientSession(read, write) as session:\n",
    "            await session.initialize()\n",
    "            tools = await session.list_tools()\n",
    "            \n",
    "            tools_description = \"\"\n",
    "            for tool in tools.tools:\n",
    "                tools_description += f\"- {tool.name}: {tool.description}\\n\"\n",
    "            \n",
    "            # Step 2: Use the system prompt from above\n",
    "            system_prompt = f\"\"\"You are a helpful assistant that converts natural language requests into MCP tool calls.\n",
    "\n",
    "Available MCP tools:\n",
    "{mcp_tools_description[0]}\n",
    "\n",
    "When the user asks you to do something related to calendar events, respond with a JSON object specifying:\n",
    "1. \"tool_name\": the name of the MCP tool to call\n",
    "2. \"arguments\": a dictionary of arguments for that tool\n",
    "\n",
    "Respond with ONLY the JSON object. Do not include any explanation.\n",
    "\n",
    "Example format:\n",
    "{{\n",
    "    \"tool_name\": \"get_upcoming_events\",\n",
    "    \"arguments\": {{\"days\": 7}}\n",
    "}}\n",
    "\"\"\"\n",
    "\n",
    "            # Step 3: LLM formats the tool call\n",
    "            print(\"\\n[Step 2] LLM interpreting request...\")\n",
    "            response = llm_client.chat.completions.create(\n",
    "                model=llm_model,\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": system_prompt},\n",
    "                    {\"role\": \"user\", \"content\": user_request}\n",
    "                ],\n",
    "                stream=False,\n",
    "            )\n",
    "\n",
    "            llm_response = response.choices[0].message.content\n",
    "\n",
    "            # Step 4: Parse the response\n",
    "            try:\n",
    "                cleaned_response = llm_response.strip()\n",
    "                if cleaned_response.startswith(\"```\"):\n",
    "                    lines = cleaned_response.split(\"\\n\")\n",
    "                    cleaned_response = \"\\n\".join(lines[1:-1])\n",
    "                    if cleaned_response.startswith(\"json\"):\n",
    "                        cleaned_response = cleaned_response[4:].strip()\n",
    "\n",
    "                tool_call = json.loads(cleaned_response)\n",
    "                tool_name = tool_call[\"tool_name\"]\n",
    "                tool_arguments = tool_call[\"arguments\"]\n",
    "            except json.JSONDecodeError as e:\n",
    "                print(f\"Error parsing LLM response: {e}\")\n",
    "                print(f\"LLM response was: {llm_response}\")\n",
    "                return\n",
    "\n",
    "            print(\"\\n[Step 3] Generated MCP Tool Call:\")\n",
    "            print(f\"Tool: {tool_name}\")\n",
    "            print(f\"Arguments: {json.dumps(tool_arguments, indent=2)}\")\n",
    "\n",
    "            # Step 5: Execute the MCP tool\n",
    "            print(\"\\n[Step 4] Calling MCP tool...\")\n",
    "            mcp_result = await session.call_tool(tool_name, arguments=tool_arguments)\n",
    "            \n",
    "            result_text = \"\\n\".join([content.text for content in mcp_result.content])\n",
    "            print(\"\\n[Step 5] MCP Tool Result:\")\n",
    "            print(result_text[:200] + \"...\" if len(result_text) > 200 else result_text)\n",
    "\n",
    "            # Step 6: LLM interprets the result\n",
    "            print(\"\\n[Step 6] LLM interpreting results for user...\")\n",
    "            interpretation_prompt = f\"\"\"User question: {user_request}\n",
    "MCP Tool result: {result_text}\n",
    "\n",
    "Answer the user's question directly with the result. Be concise and friendly.\"\"\"\n",
    "\n",
    "            response = llm_client.chat.completions.create(\n",
    "                model=llm_model,\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": \"You are a helpful assistant. Answer user questions directly and concisely.\"},\n",
    "                    {\"role\": \"user\", \"content\": interpretation_prompt}\n",
    "                ],\n",
    "                stream=True,\n",
    "            )\n",
    "\n",
    "            print(\"\\n[Step 7] FINAL RESPONSE TO USER:\")\n",
    "            print(\"-\" * 60)\n",
    "            for log in EventLogger().log(response):\n",
    "                log.print()\n",
    "            print(\"-\" * 60)\n",
    "            print()\n",
    "\n",
    "print(\"MCP Agent function created! Ready to use.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gyrt9x6ytkc",
   "metadata": {},
   "source": [
    "### Let's try it!\n",
    "\n",
    "Now we can make natural language requests and the agent will:\n",
    "1. Understand what we want\n",
    "2. Call the appropriate MCP tool\n",
    "3. Return the results in a friendly format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "kielg5oejnr",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 1: Get upcoming events\n",
    "await run_mcp_agent(\"What events do I have coming up in the next 5 days?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "i2ifdcouhc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 2: Search for specific events\n",
    "await run_mcp_agent(\"Find all events related to 'lecture'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lagpy34hcu9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 3: Get calendar statistics\n",
    "await run_mcp_agent(\"Can you give me statistics about my calendar?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "yr5wchjlrf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 4: Get events for a specific date\n",
    "await run_mcp_agent(\"Show me all events on 2024-12-15\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "app-root",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
