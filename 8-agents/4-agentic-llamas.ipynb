{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "intro",
   "metadata": {},
   "source": [
    "# Building Agents with LangGraph & Llama Stack\n",
    "\n",
    "In the last notebook, you built a ReAct agent **from scratch**. You wrote:\n",
    "- Manual response parsing with regex\n",
    "- Custom iteration loops\n",
    "- State management\n",
    "- Tool execution logic\n",
    "- Error handling\n",
    "\n",
    "That was ~200+ lines of code. Now let's see how **LangGraph** makes this dramatically simpler.\n",
    "\n",
    "## What We'll Build\n",
    "\n",
    "A knowledge-based assistant that:\n",
    "1. Searches documents for answers (RAG)\n",
    "2. Schedules meetings with professors if it can't answer\n",
    "\n",
    "Same capabilities. Way less code."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "setup",
   "metadata": {},
   "source": [
    "## Setup: Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "install",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install -q langgraph==0.2.62 langchain-openai==0.3.7 langchain-core==0.3.28 llama-stack-client==0.3.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "from langchain_core.tools import tool\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from llama_stack_client import LlamaStackClient\n",
    "import json\n",
    "\n",
    "print(\"âœ… All imports successful\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "connect",
   "metadata": {},
   "source": [
    "## Connect to Llama Stack\n",
    "\n",
    "LangGraph connects to Llama Stack through **OpenAI-compatible endpoints**. This means we can use LangChain's `ChatOpenAI` client!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "client",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LangGraph connects via OpenAI-compatible endpoint\n",
    "llm = ChatOpenAI(\n",
    "    base_url=\"http://llama-stack-service:8321/v1/openai/v1\",\n",
    "    model=\"llama32\",\n",
    "    api_key=\"not-needed\",  # Llama Stack doesn't require auth in this setup\n",
    "    temperature=0.1\n",
    ")\n",
    "\n",
    "# Also connect standard client for RAG\n",
    "llama_client = LlamaStackClient(base_url=\"http://llama-stack-service:8321\")\n",
    "\n",
    "print(\"âœ… Connected to Llama Stack\")\n",
    "print(f\"   OpenAI-compatible endpoint: http://llama-stack-service:8321/v1/openai/v1\")\n",
    "print(f\"   Model: llama32\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tools-section",
   "metadata": {},
   "source": [
    "## Define Tools\n",
    "\n",
    "With LangGraph, tools are just Python functions with the `@tool` decorator. No manual JSON schemas!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rag-setup",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, let's get our vector store for RAG\n",
    "try:\n",
    "    vector_stores = llama_client.vector_stores.list()\n",
    "    \n",
    "    if vector_stores.data and len(vector_stores.data) > 0:\n",
    "        vector_store = vector_stores.data[0]\n",
    "        print(f\"âœ… Using vector store: {vector_store.id}\")\n",
    "    else:\n",
    "        vector_store = llama_client.vector_stores.create(\n",
    "            name=\"langgraph_knowledge_base\",\n",
    "            extra_body={\n",
    "                \"embedding_model\": \"all-MiniLM-L6-v2\",\n",
    "                \"embedding_dimension\": 384,\n",
    "                \"provider_id\": \"milvus\",\n",
    "                \"vector_db_id\": \"langgraph_kb\"\n",
    "            }\n",
    "        )\n",
    "        print(f\"âœ… Created vector store: {vector_store.id}\")\n",
    "        print(\"âš ï¸  Note: Ingest documents first (see 5-rag notebooks)\")\n",
    "except Exception as e:\n",
    "    print(f\"âš ï¸  Vector store setup failed: {e}\")\n",
    "    vector_store = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tools",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool\n",
    "def search_knowledge_base(query: str) -> str:\n",
    "    \"\"\"Search through documents to find information. Use this when the user asks about concepts, definitions, or topics.\"\"\"\n",
    "    if not vector_store:\n",
    "        return \"Error: Knowledge base not available. Please set up the vector store first.\"\n",
    "    \n",
    "    try:\n",
    "        results = llama_client.vector_stores.search(\n",
    "            vector_store_id=vector_store.id,\n",
    "            query=query,\n",
    "            max_num_results=3,\n",
    "            search_mode=\"vector\"\n",
    "        )\n",
    "        \n",
    "        if not results.data:\n",
    "            return \"No relevant information found in the knowledge base.\"\n",
    "        \n",
    "        formatted_results = []\n",
    "        for i, result in enumerate(results.data, 1):\n",
    "            content = result.content if hasattr(result, 'content') else str(result)\n",
    "            formatted_results.append(f\"Result {i}: {content}\")\n",
    "        \n",
    "        return \"\\n\\n\".join(formatted_results)\n",
    "    except Exception as e:\n",
    "        return f\"Error searching knowledge base: {str(e)}\"\n",
    "\n",
    "\n",
    "@tool\n",
    "def schedule_professor_meeting(professor_name: str, topic: str, proposed_time: str) -> str:\n",
    "    \"\"\"Schedule a meeting with a professor. Use this when you can't answer a question and need expert help.\n",
    "    \n",
    "    Args:\n",
    "        professor_name: Name of the professor to meet with\n",
    "        topic: What the meeting is about\n",
    "        proposed_time: Suggested meeting time (e.g., 'tomorrow at 2pm', 'next Monday morning')\n",
    "    \"\"\"\n",
    "    # In a real system, this would integrate with a calendar API\n",
    "    return f\"âœ… Meeting scheduled with {professor_name} for '{topic}' at {proposed_time}. You'll receive a calendar invite shortly.\"\n",
    "\n",
    "\n",
    "# List of available tools\n",
    "tools = [search_knowledge_base, schedule_professor_meeting]\n",
    "\n",
    "print(\"âœ… Tools defined:\")\n",
    "for tool_func in tools:\n",
    "    print(f\"   - {tool_func.name}: {tool_func.description[:60]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "agent-section",
   "metadata": {},
   "source": [
    "## Build the Agent\n",
    "\n",
    "This is where the magic happens. With LangGraph, creating a ReAct agent is **one line of code**.\n",
    "\n",
    "No parsing. No loops. No state management. LangGraph handles it all."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "agent",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the ReAct agent - that's it!\n",
    "agent = create_react_agent(\n",
    "    llm,\n",
    "    tools,\n",
    "    checkpointer=MemorySaver()  # Enables conversation memory\n",
    ")\n",
    "\n",
    "print(\"âœ… ReAct agent created!\")\n",
    "print(\"\\nðŸŽ¯ Compare this to the 200+ lines you wrote before...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "what-happened",
   "metadata": {},
   "source": [
    "### What Just Happened?\n",
    "\n",
    "That one function call replaces ALL of this from your previous notebook:\n",
    "- âŒ `parse_react_response()` - Manual regex parsing\n",
    "- âŒ `execute_tool()` - Tool routing logic\n",
    "- âŒ `run_react_agent()` - The entire iteration loop\n",
    "- âŒ Conversation history management\n",
    "- âŒ Error handling and retry logic\n",
    "- âŒ Stopping condition checks\n",
    "\n",
    "LangGraph does **all of this automatically**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "test-section",
   "metadata": {},
   "source": [
    "## Test the Agent\n",
    "\n",
    "Let's see it in action!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "helper",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_agent(question: str, thread_id: str = \"default\"):\n",
    "    \"\"\"Run the agent with a question and display the conversation.\"\"\"\n",
    "    print(\"=\"*70)\n",
    "    print(f\"QUESTION: {question}\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    config = {\"configurable\": {\"thread_id\": thread_id}}\n",
    "    \n",
    "    inputs = {\n",
    "        \"messages\": [\n",
    "            SystemMessage(content=\"You are a helpful assistant. When you can't answer a question from the knowledge base, offer to schedule a meeting with the relevant professor.\"),\n",
    "            HumanMessage(content=question)\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    # Stream the agent's work\n",
    "    for event in agent.stream(inputs, config, stream_mode=\"values\"):\n",
    "        last_message = event[\"messages\"][-1]\n",
    "        \n",
    "        # Display the message\n",
    "        if hasattr(last_message, 'content') and last_message.content:\n",
    "            print(f\"\\n{last_message.type.upper()}: {last_message.content}\")\n",
    "        \n",
    "        # Display tool calls\n",
    "        if hasattr(last_message, 'tool_calls') and last_message.tool_calls:\n",
    "            for tool_call in last_message.tool_calls:\n",
    "                print(f\"\\nðŸ”§ CALLING TOOL: {tool_call['name']}\")\n",
    "                print(f\"   Args: {json.dumps(tool_call['args'], indent=2)}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "\n",
    "print(\"âœ… Helper function ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "example1",
   "metadata": {},
   "source": [
    "### Example 1: Knowledge Base Search\n",
    "\n",
    "The agent should search the knowledge base for this information.\n",
    "\n",
    "**Note**: This requires documents in your vector store. If you haven't done the RAG notebooks (`5-rag/2-intro-to-RAG.ipynb`), the knowledge base will be empty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "test1",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_agent(\"What is the structure of a forest canopy in botany?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "example2",
   "metadata": {},
   "source": [
    "### Example 2: Scheduling a Meeting\n",
    "\n",
    "When the agent can't answer a question, it should offer to schedule a meeting with an expert."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "test2",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_agent(\"I need help with quantum chromodynamics calculations. Can you help?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "example3",
   "metadata": {},
   "source": [
    "### Example 3: Multi-Step Reasoning\n",
    "\n",
    "Watch the agent autonomously decide between searching and scheduling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "test3",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_agent(\"What are agentic workflows, and can I meet with someone to learn more about implementing them?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "comparison",
   "metadata": {},
   "source": [
    "## The Comparison\n",
    "\n",
    "| **From Scratch** | **With LangGraph** |\n",
    "|------------------|--------------------|\n",
    "| 200+ lines of code | ~50 lines of code |\n",
    "| Manual response parsing with regex | Automatic parsing |\n",
    "| Custom iteration loop | Built-in ReAct loop |\n",
    "| Manual state management | Automatic state handling |\n",
    "| Custom tool routing | Automatic tool selection |\n",
    "| Manual conversation history | Built-in memory |\n",
    "| Custom error handling | Built-in error recovery |\n",
    "| Hard to extend | Easy to add new tools |\n",
    "\n",
    "### Key Benefits of LangGraph\n",
    "\n",
    "1. **Declarative**: You define *what* the agent should do, not *how*\n",
    "2. **Production-Ready**: Built-in error handling, retries, and state management\n",
    "3. **Extensible**: Adding new tools is trivial (just add a decorated function)\n",
    "4. **Observable**: Built-in streaming and logging\n",
    "5. **Maintainable**: Much less code to debug and maintain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "try-yourself",
   "metadata": {},
   "source": [
    "## Try It Yourself!\n",
    "\n",
    "Now experiment:\n",
    "1. Ask questions that require knowledge base searches\n",
    "2. Ask questions that should trigger meeting scheduling\n",
    "3. Try to make the agent use both tools in sequence\n",
    "\n",
    "See how the agent autonomously reasons about which tool(s) to use!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "try",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your turn!\n",
    "run_agent(\"YOUR QUESTION HERE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bonus",
   "metadata": {},
   "source": [
    "## Bonus: Adding a New Tool\n",
    "\n",
    "Remember how much work it was to add a new tool to your from-scratch agent?\n",
    "\n",
    "With LangGraph, it's this easy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bonus-tool",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool\n",
    "def get_course_schedule(course_code: str) -> str:\n",
    "    \"\"\"Get the schedule for a specific course.\"\"\"\n",
    "    # Mock data - in reality this would query a course database\n",
    "    schedules = {\n",
    "        \"CS101\": \"Mondays and Wednesdays, 10:00 AM - 11:30 AM\",\n",
    "        \"CS201\": \"Tuesdays and Thursdays, 2:00 PM - 3:30 PM\",\n",
    "        \"MATH301\": \"Mondays, Wednesdays, Fridays, 9:00 AM - 10:00 AM\"\n",
    "    }\n",
    "    return schedules.get(course_code, f\"No schedule found for {course_code}\")\n",
    "\n",
    "# Add it to the tools list\n",
    "tools.append(get_course_schedule)\n",
    "\n",
    "# Recreate the agent with the new tool\n",
    "agent = create_react_agent(llm, tools, checkpointer=MemorySaver())\n",
    "\n",
    "print(\"âœ… New tool added!\")\n",
    "print(\"\\nThat's it. No changes to parsing logic, no changes to the loop, no changes to tool routing.\")\n",
    "print(\"The agent just... works.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "test-new-tool",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the new tool\n",
    "run_agent(\"When is CS101 scheduled?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conclusion",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "1. **Frameworks save massive amounts of time**: 200+ lines â†’ 50 lines\n",
    "2. **Frameworks are production-ready**: Built-in error handling, state management, and observability\n",
    "3. **Frameworks are flexible**: LangGraph works with any OpenAI-compatible endpoint (including Llama Stack)\n",
    "4. **Understanding the fundamentals still matters**: Knowing *how* ReAct works helps you debug when things go wrong\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "In production environments:\n",
    "- Add authentication and authorization\n",
    "- Implement proper error handling and monitoring\n",
    "- Add logging and tracing (LangGraph integrates with LangSmith)\n",
    "- Scale with distributed execution\n",
    "- Implement rate limiting and cost controls\n",
    "\n",
    "All of these are easier with a mature framework like LangGraph!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
