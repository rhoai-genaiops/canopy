{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ£ Document Intelligence with Docling: Unlocking Complex Academic Content\n",
    "\n",
    "This notebook demonstrates **Document Intelligence** - the advanced capability to understand and process complex documents like research papers, academic materials, and structured content that traditional RAG systems struggle with.\n",
    "\n",
    "**The Challenge:**\n",
    "Imagine trying to build an educational AI assistant using only basic text extraction from research papers. You'd lose:\n",
    "- **ðŸ“Š Table data** with crucial research findings\n",
    "- **ðŸ§® Mathematical formulas** and scientific notation  \n",
    "- **ðŸ“ˆ Charts and figures** that provide key insights\n",
    "- **ðŸ›ï¸ Document structure** like sections, references, and metadata\n",
    "- **ðŸ“ Multi-column layouts** common in academic papers\n",
    "\n",
    "**The Solution: Docling**\n",
    "Docling is an advanced document processing toolkit that acts like a brilliant research assistant, understanding the **meaning and structure** of complex academic documents.\n",
    "\n",
    "**What You'll Build:**\n",
    "- **ðŸ”¬ Intelligent Document Processor**: Extract rich content from complex PDFs\n",
    "- **ðŸ“š Enhanced RAG System**: Query tables, formulas, and structured content  \n",
    "- **ðŸŽ¯ Academic AI Assistant**: Answer questions using complete document understanding\n",
    "- **âš¡ Production Pipeline**: Handle real-world educational materials at scale\n",
    "\n",
    "**Why This Matters:**\n",
    "Traditional RAG systems often fail with academic content, missing critical information trapped in tables or losing context from complex layouts. Docling transforms these challenging documents into fully searchable, queryable knowledge.\n",
    "\n",
    "#### Let's build document intelligence that truly understands academic content! ðŸš€"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ“¦ Install Required Packages\n",
    "\n",
    "Install the Python packages needed for this lab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q llama_stack_client==0.2.11 fire==0.7.1 dotenv==0.9.9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ“š Import Libraries for Document Intelligence\n",
    "\n",
    "Import the essential libraries for building our document intelligence RAG system with Docling processing capabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core libraries for document intelligence and RAG\n",
    "import uuid     \n",
    "import requests \n",
    "import base64   \n",
    "import json \n",
    "import os \n",
    "import sys  \n",
    "sys.path.append('..') \n",
    "\n",
    "# LlamaStack client and RAG-specific classes\n",
    "from llama_stack_client import LlamaStackClient  \n",
    "from llama_stack_client import RAGDocument  \n",
    "from llama_stack_client.types.shared.content_delta import TextDelta, ToolCallDelta  \n",
    "\n",
    "# Display and utility imports\n",
    "from src.utils import step_printer \n",
    "from termcolor import cprint        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ”— Connect to LlamaStack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The base URL points to your Llama Stack server deployment\n",
    "base_url = \"http://llama-stack-service:8321\"\n",
    "\n",
    "# Create the Llama Stack client\n",
    "client = LlamaStackClient(\n",
    "    base_url=base_url,\n",
    "    provider_data=None\n",
    ")\n",
    "\n",
    "print(f\"Connected to Llama Stack server\")\n",
    "\n",
    "# Configs for model and sampling\n",
    "model_id = \"llama32\"\n",
    "temperature = 0.0\n",
    "max_tokens = 512\n",
    "stream = False\n",
    "\n",
    "# Configure the sampling strategy based on temperature\n",
    "if temperature > 0.0:\n",
    "    top_p = 0.95  # Nucleus sampling parameter\n",
    "    strategy = {\"type\": \"top_p\", \"temperature\": temperature, \"top_p\": top_p}\n",
    "else:\n",
    "    strategy = {\"type\": \"greedy\"}  # Always pick most likely token\n",
    "\n",
    "# Package sampling parameters for the inference API\n",
    "sampling_params = {\n",
    "    \"strategy\": strategy,\n",
    "    \"max_tokens\": max_tokens,\n",
    "}\n",
    "\n",
    "print(f\"Model: {model_id}\")\n",
    "print(f\"Sampling Parameters: {sampling_params}\")\n",
    "print(f\"Stream: {stream}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ”¬ Docling Processing Function Implementation\n",
    "\n",
    "This function connects to the Docling service and processes documents through the intelligent pipeline we just described.  \n",
    "Let's test Docling's document intelligence on a complex academic paper. We'll use a real research paper that contains tables, mathematical formulas, and figures ðŸ“ˆ\n",
    "\n",
    "> Note: Because Docling is using more advanced document extraction it usually takes a little bit to extract the information.  \n",
    "This particular pdf we use should take 1-2 minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def docling_processing(url):\n",
    "\n",
    "    # Connect to the deployed Docling service in the cluster\n",
    "    api_address = \"http://docling-v0-7-0-predictor.ai501.svc.cluster.local:5001\"\n",
    "    headers = {\"Content-Type\": \"application/json\"}\n",
    "\n",
    "    print(f\"ðŸ”— Connecting to Docling service at {api_address}\")\n",
    "    \n",
    "    # Docling settings for processing\n",
    "    payload = {\n",
    "        \"http_sources\": [{\"url\": url}],\n",
    "        \"options\": {\n",
    "            \"to_formats\": [\"md\"],\n",
    "            \"image_export_mode\": \"placeholder\"\n",
    "        },\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        # Send document to Docling for intelligent analysis\n",
    "        response = requests.post(\n",
    "            f\"{api_address}/v1alpha/convert/source\",\n",
    "            json=payload,\n",
    "            headers=headers,\n",
    "            timeout=180\n",
    "        )\n",
    "        \n",
    "        response.raise_for_status()\n",
    "        \n",
    "        result_data = response.json()\n",
    "        md_content = result_data[\"document\"][\"md_content\"]\n",
    "        \n",
    "        return md_content\n",
    "        \n",
    "    except requests.exceptions.Timeout:\n",
    "        print(f\"â° Processing timeout - complex documents may need more time\")\n",
    "        raise\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"âŒ Docling processing failed: {e}\")\n",
    "        raise\n",
    "    except KeyError as e:\n",
    "        print(f\"âŒ Unexpected response format: {e}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We choose a research paper called \"First Mapping the Canopy Height of Primeval Forests in the Tallest Tree Area of Asia\", fitting for our Canaopy application.\n",
    "url = \"https://arxiv.org/pdf/2404.14661\"\n",
    "\n",
    "md_content = docling_processing(url)\n",
    "\n",
    "print(f\"\\nðŸŽ‰ Document intelligence processing complete!\")\n",
    "print(f\"ðŸ“Š Content preview (first 500 characters):\")\n",
    "print(f\"{'='*60}\")\n",
    "print(md_content[:500] + \"...\" if len(md_content) > 500 else md_content)\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"ðŸ“ˆ Total processed content: {len(md_content)} characters\")\n",
    "print(f\"ðŸ“ Docling has extracted and structured the complete document content!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ“ƒ Using the processed document for RAG\n",
    "\n",
    "We can now add this document to our Milvus database (the in-line one, remember that we are in the experiment namespace and connect to the experiment Llama Stack for now).  \n",
    "Since we have already done this once you will probably recognize some parts of the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to the vector database\n",
    "vector_db_id = f\"test_vector_db_{uuid.uuid4()}\"\n",
    "print(f\"ðŸ“Š Created vector database ID: {vector_db_id}\")\n",
    "\n",
    "client.vector_dbs.register(\n",
    "    vector_db_id=vector_db_id,\n",
    "    embedding_model=\"all-MiniLM-L6-v2\",\n",
    "    embedding_dimension=384,\n",
    "    provider_id=\"milvus\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ingest the document as processed by Docling\n",
    "documents = [\n",
    "    RAGDocument(\n",
    "        document_id=f\"docling-processed-doc\",\n",
    "        content=md_content,\n",
    "        metadata={\n",
    "            \"source_url\": url,\n",
    "            \"processing_method\": \"docling\",\n",
    "            \"document_type\": \"academic_paper\",\n",
    "            \"has_tables\": True,\n",
    "            \"has_formulas\": True,\n",
    "            \"has_figures\": True,\n",
    "        },\n",
    "    )\n",
    "]\n",
    "\n",
    "try:\n",
    "    client.tool_runtime.rag_tool.insert(\n",
    "        documents=documents,\n",
    "        vector_db_id=vector_db_id,\n",
    "        chunk_size_in_tokens=512,\n",
    "    )\n",
    "    print(f\"\\nâœ… Document ingestion complete!\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\nâŒ Document ingestion failed: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have ingested the document (added it into our Vector Database) we can query for it just like we did before!  \n",
    "Feel free to play around with different queries to see what it answers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test queries for the processed document\n",
    "queries = [\n",
    "    \"What is the PRFXception?\",\n",
    "    \"The accuracy values of overall model prediction and residual cross-validation for five regions in southeast Tibet and four regions in northwest Yunnan\"\n",
    "]\n",
    "\n",
    "for prompt in queries:\n",
    "    cprint(f\"\\nUser> {prompt}\", \"blue\")\n",
    "    \n",
    "    # RAG retrieval call - find relevant chunks from the vector database\n",
    "    rag_response = client.tool_runtime.rag_tool.query(\n",
    "        content=prompt, \n",
    "        vector_db_ids=[vector_db_id],\n",
    "        query_config={\n",
    "            \"chunk_template\": \"Result {index}\\nContent: {chunk.content}\\nMetadata: {metadata}\\n\",\n",
    "        },\n",
    "        )\n",
    "\n",
    "    cprint(rag_response)\n",
    "\n",
    "    cprint(f\"\\n--- RAG Metadata ---\", \"yellow\")\n",
    "    cprint(rag_response.metadata, \"cyan\")\n",
    "\n",
    "    # Create messages for the LLM with system prompt\n",
    "    messages = [{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}]\n",
    "\n",
    "    # Combine the user query with retrieved context from RAG\n",
    "    prompt_context = rag_response.content\n",
    "    extended_prompt = f\"Please answer the given query using the context below.\\n\\nCONTEXT:\\n{prompt_context}\\n\\nQUERY:\\n{prompt}\"\n",
    "    messages.append({\"role\": \"user\", \"content\": extended_prompt})\n",
    "\n",
    "    # Get response from the LLM using the enhanced prompt\n",
    "    response = client.inference.chat_completion(\n",
    "        messages=messages,\n",
    "        model_id=model_id,\n",
    "        sampling_params=sampling_params,\n",
    "        stream=stream,\n",
    "    )\n",
    "    \n",
    "    # Print the streaming response\n",
    "    cprint(\"inference> \", color=\"magenta\", end='')\n",
    "    if stream:\n",
    "        for chunk in response:\n",
    "            response_delta = chunk.event.delta\n",
    "            if isinstance(response_delta, TextDelta):\n",
    "                cprint(response_delta.text, color=\"magenta\", end='')\n",
    "            elif isinstance(response_delta, ToolCallDelta):\n",
    "                cprint(response_delta.tool_call, color=\"magenta\", end='')\n",
    "    else:\n",
    "        cprint(response.completion_message.content, color=\"magenta\")\n",
    "\n",
    "    cprint(f\"\\n--- End of RAG Answer ---\", \"blue\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸŽ‰ You have used Docling to enhance your document processing!\n",
    "\n",
    "Your document intelligence system can now understand and query the most complex academic content - transforming how educational institutions handle knowledge discovery and research! ðŸš€  \n",
    "Go back to the instructions to see how we can automate our document ingestion."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "app-root",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
