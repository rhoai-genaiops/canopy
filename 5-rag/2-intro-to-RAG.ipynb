{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "45fc9086-93aa-4645-8ba2-380c3acbbed9",
   "metadata": {},
   "source": [
    "# ü¶ô Llama Stack & RAG: Building Intelligent Agents\n",
    "\n",
    "This notebook demonstrates **Retrieval-Augmented Generation (RAG)** - a powerful technique that enables AI models to access and reason about external documents and knowledge bases.\n",
    "\n",
    "**Why RAG Matters:**\n",
    "Instead of relying only on training data, RAG-enhanced models can reference your specific documents, course materials, and knowledge bases to provide accurate, cited responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "332e6cec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install -q llama_stack_client fire dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f15080a6-48be-4475-8813-c584701d69bf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import uuid\n",
    "\n",
    "from llama_stack_client import RAGDocument, LlamaStackClient\n",
    "from termcolor import cprint\n",
    "\n",
    "import sys\n",
    "sys.path.append('..')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "631e8c70-6f28-440b-b71a-85d4040ffac4",
   "metadata": {},
   "source": [
    "## üîó Connect to Llama Stack\n",
    "\n",
    "Connect to Llama Stack - the AI engine that orchestrates all RAG operations. Llama Stack acts as the central hub that coordinates:\n",
    "- Vector database operations (storage and retrieval)\n",
    "- Document processing and chunking\n",
    "- LLM inference with retrieved context\n",
    "- Agent workflows and tool usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "558909bb-955c-40a3-a0c2-1f4acb0dd62e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The base URL points to your Llama Stack server deployment\n",
    "base_url = \"http://llama-stack-service:8321\"\n",
    "\n",
    "# Create the Llama Stack client\n",
    "client = LlamaStackClient(\n",
    "    base_url=base_url,\n",
    "    provider_data=None\n",
    ")\n",
    "\n",
    "print(f\"Connected to Llama Stack server\")\n",
    "\n",
    "# Configs for model and sampling\n",
    "model_id = \"llama32\"\n",
    "temperature = 0.0\n",
    "max_tokens = 512\n",
    "stream = False\n",
    "\n",
    "# Configure the sampling strategy based on temperature\n",
    "if temperature > 0.0:\n",
    "    top_p = 0.95  # Nucleus sampling parameter\n",
    "    strategy = {\"type\": \"top_p\", \"temperature\": temperature, \"top_p\": top_p}\n",
    "else:\n",
    "    strategy = {\"type\": \"greedy\"}  # Always pick most likely token\n",
    "\n",
    "# Package sampling parameters for the inference API\n",
    "sampling_params = {\n",
    "    \"strategy\": strategy,\n",
    "    \"max_tokens\": max_tokens,\n",
    "}\n",
    "\n",
    "print(f\"Model: {model_id}\")\n",
    "print(f\"Sampling Parameters: {sampling_params}\")\n",
    "print(f\"Stream: {stream}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9867dbd",
   "metadata": {},
   "source": [
    "## Register the Vector Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c725c2da-05e5-474f-9a44-cf5615557665",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Generate a unique identifier to ensures no conflicts if multiple users run this notebook simultaneously\n",
    "vector_db_id = f\"test_vector_db_{uuid.uuid4()}\"\n",
    "print(f\"üìä Created vector database ID: {vector_db_id}\")\n",
    "\n",
    "# This tells Llama Stack how to connect to and use your vector database\n",
    "client.vector_dbs.register(\n",
    "    vector_db_id=vector_db_id,\n",
    "    embedding_model=\"all-MiniLM-L6-v2\",\n",
    "    embedding_dimension=384,                      # Vector size (must match model output)\n",
    "    provider_id=\"milvus\",                         # Use our inline Vector database, as we set it up in Llama Stack\n",
    ")\n",
    "print(f\"‚úÖ Registered vector database with inline backend\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87510929-fe4b-428c-8f9e-14d47a03daa2",
   "metadata": {},
   "source": [
    "## üìö Document Ingestion and Processing\n",
    "\n",
    "This is where the **RAG Layer** comes into action! We'll use Llama Stack's RAG Tool to automatically:\n",
    "\n",
    "1. **Download** documents from URLs\n",
    "2. **Process** PDF content and extract text\n",
    "3. **Chunk** large documents into optimal pieces (512 tokens each)\n",
    "4. **Embed** each chunk using the embedding model\n",
    "5. **Store** vectors and metadata in the vector database\n",
    "\n",
    "**Two ways to ingest documents:**\n",
    "- **Direct Vector IO**: Insert pre-processed chunks directly into your Vector Database\n",
    "- **Llama Stack RAG Tool** (what we're using): Automatic processing from URLs or files\n",
    "\n",
    "We use the RAG Tool because it allows us to easily ingest documents from URLs, files, etc. and automatically chunks them into smaller pieces. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d81ffb2-2089-4cb8-adae-f32965f206c7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# List of URLs (in this case just 1) to process\n",
    "urls = [\n",
    "    (\"https://raw.githubusercontent.com/rhoai-genaiops/deploy-lab/main/university-data/canopy-in-botany.pdf\", \"application/pdf\"),\n",
    "]\n",
    "\n",
    "# RAGDocument is Llama Stack's format for documents to be ingested\n",
    "documents = [\n",
    "    RAGDocument(\n",
    "        document_id=f\"doc-{i}\",\n",
    "        content=url,\n",
    "        mime_type=url_type,\n",
    "        metadata={\n",
    "            \"source_url\": url,\n",
    "            \"document_type\": \"academic_material\",\n",
    "        },\n",
    "    )\n",
    "    for i, (url, url_type) in enumerate(urls)\n",
    "]\n",
    "\n",
    "# Display what we're about to ingest\n",
    "print(\"üìñ Ingesting documents into RAG system...\")\n",
    "for i, (url, url_type) in enumerate(urls):\n",
    "    print(f\"  ‚Ä¢ Document {i+1}: {url}\")\n",
    "\n",
    "# Then we automatically download, chunk, and store our document(s)\n",
    "try:\n",
    "    client.tool_runtime.rag_tool.insert(\n",
    "        documents=documents,\n",
    "        vector_db_id=vector_db_id,\n",
    "        chunk_size_in_tokens=512,\n",
    "    )\n",
    "    print(\"\\n‚úÖ Document ingestion complete!\")\n",
    "    print(\"üéØ Your documents are now searchable via semantic similarity!\")\n",
    "except Exception as e:\n",
    "    print(f\"\\n‚ùå Document ingestion failed: {e}\")\n",
    "    print(\"üí° This might be due to PDF processing issues. Try with different documents or check the PDF accessibility.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcab638b",
   "metadata": {},
   "source": [
    "# Choice time! üôã‚Äç‚ôÇÔ∏è\n",
    "\n",
    "Now that we have injested a document, you can choose to either try out RAG here in the notebook or use the Llama Playground\n",
    "\n",
    "üßæ If you prefer notebook, continue to the next cell.\n",
    "\n",
    "ü¶ô If you wish to use Llama Playground, open it up (here is the route if you have closed it: `https://milvus-test-attu-<USER_NAME>-test.<CLUSTER_DOMAIN>`) and in the left menu choose the `Document Collections` we just injested (there should only be 1). \n",
    "\n",
    "Then try these questions (with and without the document selected):\n",
    "\n",
    "- What are the types of Canopy?\n",
    "- What is the structure of Canopy?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b7295c6",
   "metadata": {},
   "source": [
    "![genaiops-rag-meme.png](genaiops-rag-meme.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cbe147c",
   "metadata": {},
   "outputs": [],
   "source": [
    "queries = [\n",
    "    \"What are the types of Canopy?\",\n",
    "    \"What is the structure of Canopy?\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0cfd7d3",
   "metadata": {},
   "source": [
    "## First, without RAG\n",
    "First, let's test the response without RAG in the picture so we have something to compare with.  \n",
    "Notice that this is very similar to the code we used before to send a prompt to the model, system prompt included and everything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22430160",
   "metadata": {},
   "outputs": [],
   "source": [
    "for prompt in queries:\n",
    "    cprint(f\"\\nUser> {prompt}\", \"blue\")\n",
    "\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}\n",
    "    ]\n",
    "\n",
    "    response = client.inference.chat_completion(\n",
    "        messages=messages,\n",
    "        model_id=model_id,\n",
    "        sampling_params=sampling_params,\n",
    "        stream=True,\n",
    "    )\n",
    "\n",
    "    cprint(\"inference> \", color=\"magenta\", end='')\n",
    "    \n",
    "    # Handle streaming response - tokens arrive one by one\n",
    "    for event in response:\n",
    "        ev = getattr(event, \"event\", event)\n",
    "        delta = getattr(ev, \"delta\", None)\n",
    "\n",
    "        if delta is None:\n",
    "            continue\n",
    "\n",
    "        text = getattr(delta, \"text\", None)\n",
    "        if isinstance(text, str):\n",
    "            cprint(text, color=\"magenta\", end='')\n",
    "            continue\n",
    "\n",
    "        tool_call = getattr(delta, \"tool_call\", None)\n",
    "        if tool_call is not None:\n",
    "            cprint(str(tool_call), color=\"magenta\", end='')\n",
    "            continue"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5639413-90d6-42ae-add4-6c89da0297e2",
   "metadata": {},
   "source": [
    "## üîç Testing RAG Retrieval and Generation\n",
    "\n",
    "Now let's test the complete **RAG Pipeline** - this demonstrates how all three layers work together:\n",
    "\n",
    "### The RAG Process:\n",
    "1. **üîç Query Processing**: Convert user question into embeddings\n",
    "2. **üìö Semantic Retrieval**: Find most similar document chunks in vector database  \n",
    "3. **üîó Context Assembly**: Combine user question with retrieved chunks\n",
    "4. **ü§ñ Generation**: LLM generates informed response using both its training and the retrieved context\n",
    "5. **üìñ Citation**: Response includes references to source documents\n",
    "\n",
    "**Why this works better than normal LLMs:**\n",
    "- **Grounded responses**: Answers are based on your specific documents\n",
    "- **Up-to-date**: Add new documents without retraining the model\n",
    "- **Traceable**: Every answer can be traced back to source material\n",
    "- **Accurate**: Reduces hallucination by providing factual context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d39ab00-2a65-4b72-b5ed-4dd61f1204a2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for prompt in queries:\n",
    "    cprint(f\"\\nUser> {prompt}\", \"blue\")\n",
    "    \n",
    "    # Query the vector database to find relevant document chunks\n",
    "    rag_response = client.tool_runtime.rag_tool.query(\n",
    "        content=prompt,\n",
    "        vector_db_ids=[vector_db_id],\n",
    "        query_config={\n",
    "            \"chunk_template\": \"Result {index}\\nContent: {chunk.content}\\nMetadata: {metadata}\\n\",\n",
    "        },\n",
    "    )\n",
    "\n",
    "    cprint(f\"Text chunks from RAG found: {rag_response}\")\n",
    "    cprint(f\"\\n--- RAG Metadata ---\", \"yellow\")\n",
    "    cprint(rag_response.metadata, \"cyan\")\n",
    "\n",
    "    \n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}\n",
    "    ]\n",
    "\n",
    "    # Now we inject the retrieved content as context into our prompt, this is the key part to make RAG work as this is where the LLM gets the document information\n",
    "    prompt_context = rag_response.content\n",
    "    extended_prompt = f\"Please answer the given query using the context below.\\n\\nCONTEXT:\\n{prompt_context}\\n\\nQUERY:\\n{prompt}\"\n",
    "    messages.append({\"role\": \"user\", \"content\": extended_prompt})\n",
    "\n",
    "    response = client.inference.chat_completion(\n",
    "        messages=messages,\n",
    "        model_id=model_id,\n",
    "        sampling_params=sampling_params,\n",
    "        stream=True,\n",
    "    )\n",
    "\n",
    "    cprint(\"inference> \", color=\"magenta\", end='')\n",
    "    \n",
    "    # Handle streaming response - tokens arrive one by one\n",
    "    for event in response:\n",
    "        ev = getattr(event, \"event\", event)\n",
    "        delta = getattr(ev, \"delta\", None)\n",
    "\n",
    "        if delta is None:\n",
    "            continue\n",
    "\n",
    "        text = getattr(delta, \"text\", None)\n",
    "        if isinstance(text, str):\n",
    "            cprint(text, color=\"magenta\", end='')\n",
    "            continue\n",
    "\n",
    "        tool_call = getattr(delta, \"tool_call\", None)\n",
    "        if tool_call is not None:\n",
    "            cprint(str(tool_call), color=\"magenta\", end='')\n",
    "            continue"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bb5b323",
   "metadata": {},
   "source": [
    "## üéâ You've Built a Complete RAG System!\n",
    "\n",
    "Buuut... it's running in an in-line vector database, with no automation or redundency, and is not connected to our application yet.  \n",
    "Let's go through the steps to move this from a proof of concept to a production ready system!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "app-root",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
