{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "45fc9086-93aa-4645-8ba2-380c3acbbed9",
   "metadata": {},
   "source": [
    "# ü¶ô Llama Stack & RAG: Building Intelligent Agents\n",
    "\n",
    "This notebook demonstrates **Retrieval-Augmented Generation (RAG)** - a powerful technique that enables AI models to access and reason about external documents and knowledge bases.\n",
    "\n",
    "**Why RAG Matters:**\n",
    "Instead of relying only on training data, RAG-enhanced models can reference your specific documents, course materials, and knowledge bases to provide accurate, cited responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "332e6cec",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q llama_stack_client==0.3.0 fire==0.7.1 dotenv==0.9.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f15080a6-48be-4475-8813-c584701d69bf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import uuid\n",
    "\n",
    "from llama_stack_client import RAGDocument, LlamaStackClient\n",
    "from termcolor import cprint\n",
    "\n",
    "import sys\n",
    "sys.path.append('..')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "631e8c70-6f28-440b-b71a-85d4040ffac4",
   "metadata": {},
   "source": [
    "## üîó Connect to Llama Stack\n",
    "\n",
    "Connect to Llama Stack - the AI engine that orchestrates all RAG operations. Llama Stack acts as the central hub that coordinates:\n",
    "- Vector database operations (storage and retrieval)\n",
    "- Document processing and chunking\n",
    "- LLM inference with retrieved context\n",
    "- Agent workflows and tool usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "558909bb-955c-40a3-a0c2-1f4acb0dd62e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The base URL points to your Llama Stack server deployment\n",
    "base_url = \"http://llama-stack-service:8321\"\n",
    "\n",
    "# Create the Llama Stack client\n",
    "client = LlamaStackClient(\n",
    "    base_url=base_url,\n",
    "    provider_data=None\n",
    ")\n",
    "\n",
    "print(f\"Connected to Llama Stack server\")\n",
    "\n",
    "# Configs for model and sampling\n",
    "model_id = \"llama32\"\n",
    "temperature = 0.0\n",
    "max_tokens = 512\n",
    "stream = False\n",
    "\n",
    "# Configure the sampling strategy based on temperature\n",
    "if temperature > 0.0:\n",
    "    top_p = 0.95  # Nucleus sampling parameter\n",
    "    strategy = {\"type\": \"top_p\", \"temperature\": temperature, \"top_p\": top_p}\n",
    "else:\n",
    "    strategy = {\"type\": \"greedy\"}  # Always pick most likely token\n",
    "\n",
    "# Package sampling parameters for the inference API\n",
    "sampling_params = {\n",
    "    \"strategy\": strategy,\n",
    "    \"max_tokens\": max_tokens,\n",
    "}\n",
    "\n",
    "print(f\"Model: {model_id}\")\n",
    "print(f\"Sampling Parameters: {sampling_params}\")\n",
    "print(f\"Stream: {stream}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9867dbd",
   "metadata": {},
   "source": [
    "## Register the Vector Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a89e1b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This tells Llama Stack how to connect to and use your vector database\n",
    "vs = client.vector_stores.create(\n",
    "    name=\"my_citations_db\",\n",
    "    extra_body={\n",
    "        \"embedding_model\": \"all-MiniLM-L6-v2\",\n",
    "        \"embedding_dimension\": 384,\n",
    "        \"provider_id\": \"milvus\",\n",
    "        \"vector_db_id\": \"test\"\n",
    "    }\n",
    ")\n",
    "\n",
    "print(f\"üìä Created vector database with ID: {vs.id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87510929-fe4b-428c-8f9e-14d47a03daa2",
   "metadata": {},
   "source": [
    "## üìö Document Ingestion and Processing\n",
    "\n",
    "This is where the **RAG Layer** comes into action! We'll use Llama Stack's RAG Tool to automatically:\n",
    "\n",
    "1. **Download** documents from URLs\n",
    "2. **Process** PDF content and extract text\n",
    "3. **Chunk** large documents into optimal pieces (512 tokens each)\n",
    "4. **Embed** each chunk using the embedding model\n",
    "5. **Store** vectors and metadata in the vector database\n",
    "\n",
    "**Two ways to ingest documents:**\n",
    "- **Direct Vector IO**: Insert pre-processed chunks directly into your Vector Database\n",
    "- **Llama Stack RAG Tool** (what we're using): Automatic processing from URLs or files\n",
    "\n",
    "We use the RAG Tool because it allows us to easily ingest documents from URLs, files, etc. and automatically chunks them into smaller pieces. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d81ffb2-2089-4cb8-adae-f32965f206c7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# List of URLs (in this case just 1) to process\n",
    "urls = [\n",
    "    \"https://raw.githubusercontent.com/rhoai-genaiops/deploy-lab/main/university-data/canopy-in-botany.pdf\",\n",
    "]\n",
    "\n",
    "# Display what we're about to ingest\n",
    "print(\"üìñ Ingesting documents into RAG system...\")\n",
    "for i, url in enumerate(urls):\n",
    "    print(f\"  ‚Ä¢ Document {i+1}: {url}\")\n",
    "\n",
    "# Download and upload files using the new API\n",
    "import requests\n",
    "from io import BytesIO\n",
    "\n",
    "try:\n",
    "    uploaded_file_ids = []\n",
    "    \n",
    "    for i, url in enumerate(urls):\n",
    "        print(f\"\\nüì• Downloading document from: {url}\")\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        # Create a file-like object from the downloaded content\n",
    "        file_content = BytesIO(response.content)\n",
    "        file_content.name = f\"canopy-in-botany-{i}.pdf\"\n",
    "        \n",
    "        # Upload file using the new files API\n",
    "        uploaded_file = client.files.create(\n",
    "            file=file_content,\n",
    "            purpose=\"assistants\"  # Required purpose parameter\n",
    "        )\n",
    "        \n",
    "        uploaded_file_ids.append(uploaded_file.id)\n",
    "        print(f\"‚úÖ Uploaded file with ID: {uploaded_file.id}\")\n",
    "    \n",
    "    # Add files to the vector store with chunking configuration\n",
    "    for file_id in uploaded_file_ids:\n",
    "        client.vector_stores.files.create(\n",
    "            vector_store_id=vs.id,\n",
    "            file_id=file_id,\n",
    "            chunking_strategy={\n",
    "                \"type\": \"static\",\n",
    "                \"static\": {\n",
    "                    \"max_chunk_size_tokens\": 512,\n",
    "                    \"chunk_overlap_tokens\": 50\n",
    "                }\n",
    "            }\n",
    "        )\n",
    "        print(f\"‚úÖ Added file {file_id} to vector store with chunking\")\n",
    "    \n",
    "    print(\"\\n‚úÖ Document ingestion complete!\")\n",
    "    print(\"üéØ Your documents are now searchable via semantic similarity!\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\n‚ùå Document ingestion failed: {e}\")\n",
    "    print(\"üí° This might be due to PDF processing issues or network connectivity. Try with different documents or check the PDF accessibility.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcab638b",
   "metadata": {},
   "source": [
    "# Choice time! üôã‚Äç‚ôÇÔ∏è\n",
    "\n",
    "Now that we have injested a document, you can choose to either try out RAG here in the notebook or use the Llama Playground\n",
    "\n",
    "üßæ If you prefer notebook, continue to the next cell.\n",
    "\n",
    "ü¶ô If you wish to use Llama Stack Playground, open it up (here is the route if you have closed it: `https://llama-stack-playground-<USER_NAME>-test.<CLUSTER_DOMAIN>`) and in the left menu select the `Document Collections` we just injested (there should only be 1, and named something like `test_vector_db_1234..`). \n",
    "\n",
    "Then try these questions (with and without the document selected):\n",
    "\n",
    "- What are the types of Canopy?\n",
    "- What is the structure of Canopy?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b7295c6",
   "metadata": {},
   "source": [
    "![genaiops-rag-meme.png](genaiops-rag-meme.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cbe147c",
   "metadata": {},
   "outputs": [],
   "source": [
    "queries = [\n",
    "    \"What are the types of Canopy?\",\n",
    "    \"What is the structure of Canopy?\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0cfd7d3",
   "metadata": {},
   "source": [
    "## First, without RAG\n",
    "First, let's test the response without RAG in the picture so we have something to compare with.  \n",
    "Notice that this is very similar to the code we used before to send a prompt to the model, system prompt included and everything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22430160",
   "metadata": {},
   "outputs": [],
   "source": [
    "for prompt in queries:\n",
    "    cprint(f\"\\nUser> {prompt}\", \"blue\")\n",
    "\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "    ]\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=model_id,\n",
    "        messages=messages,\n",
    "        temperature=temperature,\n",
    "        max_tokens=max_tokens,\n",
    "        stream=True,\n",
    "    )\n",
    "\n",
    "    cprint(\"inference> \", color=\"magenta\", end='')\n",
    "    \n",
    "    # Handle streaming response - tokens arrive one by one\n",
    "    for chunk in response:\n",
    "        if hasattr(chunk, 'choices') and chunk.choices:\n",
    "            delta = chunk.choices[0].delta\n",
    "            if hasattr(delta, 'content') and delta.content:\n",
    "                cprint(delta.content, color=\"magenta\", end='')\n",
    "        elif hasattr(chunk, 'content'):\n",
    "            cprint(chunk.content, color=\"magenta\", end='')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5639413-90d6-42ae-add4-6c89da0297e2",
   "metadata": {},
   "source": [
    "## üîç Testing RAG Retrieval and Generation\n",
    "\n",
    "Now let's test the complete **RAG Pipeline** - this demonstrates how all three layers work together:\n",
    "\n",
    "### The RAG Process:\n",
    "1. **üîç Query Processing**: Convert user question into embeddings\n",
    "2. **üìö Semantic Retrieval**: Find most similar document chunks in vector database  \n",
    "3. **üîó Context Assembly**: Combine user question with retrieved chunks\n",
    "4. **ü§ñ Generation**: LLM generates informed response using both its training and the retrieved context\n",
    "5. **üìñ Citation**: Response includes references to source documents\n",
    "\n",
    "**Why this works better than normal LLMs:**\n",
    "- **Grounded responses**: Answers are based on your specific documents\n",
    "- **Up-to-date**: Add new documents without retraining the model\n",
    "- **Traceable**: Every answer can be traced back to source material\n",
    "- **Accurate**: Reduces hallucination by providing factual context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d39ab00-2a65-4b72-b5ed-4dd61f1204a2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for prompt in queries:\n",
    "    cprint(f\"\\nUser> {prompt}\", \"blue\")\n",
    "    \n",
    "    # Query the vector database to find relevant document chunks\n",
    "    search_results = client.vector_stores.search(\n",
    "        vector_store_id=vs.id,\n",
    "        query=prompt,\n",
    "        max_num_results=5,\n",
    "        search_mode=\"vector\"  # Use vector similarity search\n",
    "    )\n",
    "\n",
    "    retrieved_chunks = []\n",
    "    for i, result in enumerate(search_results.data):\n",
    "        chunk_content = result.content if hasattr(result, 'content') else str(result)\n",
    "        metadata = result.metadata if hasattr(result, 'metadata') else {}\n",
    "        retrieved_chunks.append(f\"Result {i+1}\\nContent: {chunk_content}\\nMetadata: {metadata}\")\n",
    "    \n",
    "    rag_response_content = \"\\n\\n\".join(retrieved_chunks)\n",
    "    \n",
    "    cprint(f\"Text chunks from vector search found: {len(search_results.data)} chunks\")\n",
    "    cprint(f\"\\n--- Search Results ---\", \"yellow\")\n",
    "    cprint(rag_response_content[:500] + \"...\" if len(rag_response_content) > 500 else rag_response_content, \"cyan\")\n",
    "\n",
    "    \n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}\n",
    "    ]\n",
    "\n",
    "    # Now we inject the retrieved content as context into our prompt, this is the key part to make RAG work as this is where the LLM gets the document information\n",
    "    extended_prompt = f\"Please answer the given query using the context below.\\n\\nCONTEXT:\\n{rag_response_content}\\n\\nQUERY:\\n{prompt}\"\n",
    "    messages.append({\"role\": \"user\", \"content\": extended_prompt})\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=model_id,\n",
    "        messages=messages,\n",
    "        temperature=temperature,\n",
    "        max_tokens=max_tokens,\n",
    "        stream=True,\n",
    "    )\n",
    "\n",
    "    cprint(\"inference> \", color=\"magenta\", end='')\n",
    "    \n",
    "    # Handle streaming response - tokens arrive one by one\n",
    "    for chunk in response:\n",
    "        if hasattr(chunk, 'choices') and chunk.choices:\n",
    "            delta = chunk.choices[0].delta\n",
    "            if hasattr(delta, 'content') and delta.content:\n",
    "                cprint(delta.content, color=\"magenta\", end='')\n",
    "        elif hasattr(chunk, 'content'):\n",
    "            cprint(chunk.content, color=\"magenta\", end='')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bb5b323",
   "metadata": {},
   "source": [
    "## üéâ You've Built a Complete RAG System!\n",
    "\n",
    "Buuut... it's running in an in-line vector database, with no automation or redundancy, and is not connected to our application yet.  \n",
    "Let's go through the steps to move this from a proof of concept to a production ready system!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "app-root",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
