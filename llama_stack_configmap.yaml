kind: ConfigMap
apiVersion: v1
metadata:
  name: run-config
  namespace: llama-stack
data:
  config.yaml: |-
    version: '2'
    image_name: remote-vllm
    apis:
    - agents
    - inference
    - tool_runtime
    - safety
    - vector_io
    - telemetry
    - eval
    - datasetio
    - scoring
    providers:
      safety:
      - provider_id: llama-guard
        provider_type: inline::llama-guard
        config:
          excluded_categories: []
      - provider_id: prompt-guard
        provider_type: inline::prompt-guard
        config: {}          
      - provider_id: trustyai_fms
        provider_type: remote::trustyai_fms
        config:
          orchestrator_url: https://guardrails-orchestrator-guardrails.apps.cluster-v42lf.v42lf.sandbox239.opentlc.com
          shields:
            email_hap:
              type: content
              confidence_threshold: 0.5
              message_types: ["user", "system"]
              detectors:
                  hap: 
                    detector_params: {}
                  regex:
                    detector_params: 
                      regex: ["email", "ssn", "credit-card", "^hello$"]
            granite:
              type: chat
              confidence_threshold: 0.6
              message_types: ["system", "user", "completion"]
              detector_params:
                temperature: 0.0
                risk_name: "code_execution"
                risk_definition: "The message requests or attempts to execute potentially harmful or unauthorized code that could compromise system security or perform malicious actions."
            
      inference:
      - provider_id: ${env.MODEL_NAME}
        provider_type: remote::vllm
        config:
          url: ${env.MODEL_URL}/v1
          max_tokens: 8192
          api_token: fake
          tls_verify: false
      - provider_id: llama-guard-3-1b
        provider_type: remote::vllm
        config:
          url: https://llamaguard-3-1b-llama-stack.apps.cluster-v42lf.v42lf.sandbox239.opentlc.com/v1
      vector_io:
      - provider_id: milvus
        provider_type: remote::milvus
        config:
          uri: "http://milvus.milvus.svc.cluster.local:19530"
          token: "root:Milvus"
      agents:
      - provider_id: meta-reference
        provider_type: inline::meta-reference
        config:
          persistence_store:
            type: sqlite
            namespace: null
            db_path: ${env.SQLITE_STORE_DIR:~/.llama/distributions/remote-vllm}/persistence_store.db
          responses_store:
            type: sqlite
            namespace: null
            db_path: ${env.SQLITE_STORE_DIR:~/.llama/distributions/remote-vllm}/responses_store.db
      eval:
      - provider_id: meta-reference
        provider_type: inline::meta-reference
        config:
          kvstore:
            type: sqlite
            namespace: null
            db_path: ${env.SQLITE_STORE_DIR:~/.llama/distributions/remote-vllm}/meta_reference_eval.db
      datasetio:
      - provider_id: huggingface
        provider_type: remote::huggingface
        config:
          kvstore:
            type: sqlite
            namespace: null
            db_path: ${env.SQLITE_STORE_DIR:~/.llama/distributions/remote-vllm}/huggingface_datasetio.db
      - provider_id: localfs
        provider_type: inline::localfs
        config:
          kvstore:
            type: sqlite
            namespace: null
            db_path: ${env.SQLITE_STORE_DIR:~/.llama/distributions/remote-vllm}/localfs_datasetio.db
      scoring:
      - provider_id: basic
        provider_type: inline::basic
        config: {}
      - provider_id: llm-as-judge
        provider_type: inline::llm-as-judge
        config: {}
      telemetry:
        - provider_id: meta-reference
          provider_type: inline::meta-reference
          config:
            service_name: ${env.OTEL_SERVICE_NAME:llama-stack}
            sinks: otel_trace,otel_metric
            otel_metric_endpoint: "http://jaeger-collector.llama-stack.svc.cluster.local:4318/v1/metrics"
            otel_trace_endpoint: "http://jaeger-collector.llama-stack.svc.cluster.local:4318/v1/traces"
            sqlite_db_path: ${env.SQLITE_DB_PATH:~/.llama/distributions/remote-vllm/trace_store.db}
    metadata_store:
      type: sqlite
      db_path: ${env.SQLITE_STORE_DIR:~/.llama/distributions/remote-vllm}/registry.db
      namespace: null
    models:
    - metadata: {}
      model_id: ${env.MODEL_NAME}
      provider_id: ${env.MODEL_NAME}
      model_type: llm
    - metadata: {}
      model_id: meta-llama/Llama-Guard-3-1B 
      provider_id: llama-guard-3-1b
      model_type: llm
    shields:
    - provider_id: llama-guard
      shield_id: content_safety
      provider_shield_id: Llama-Guard-3-1B
    - provider_id: prompt-guard
      shield_id: prompt_guard
      provider_shield_id: Prompt-Guard-86M
    - shield_id: email_hap
      provider_id: trustyai_fms
    - shield_id: granite
      provider_id: trustyai_fms
    external_providers_dir: /.llama/llama-stack-provider-trustyai-fms/providers.d
    vector_dbs: []
    datasets: []
    scoring_fns: []
    benchmarks: []
    tool_groups: []