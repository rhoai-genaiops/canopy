{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": [
    "# Introduction to LLM-Compressor ğŸ—œï¸\n",
    "\n",
    "[llm-compressor](https://github.com/vllm-project/llm-compressor) is the production quantization toolkit from the vLLM project. It provides a simple `oneshot` API for applying post-training quantization.\n",
    "\n",
    "In this notebook, you'll:\n",
    "1. Set up llm-compressor\n",
    "2. Quantize a small model using the `oneshot` API\n",
    "3. Compare the file size before and after quantization\n",
    "\n",
    "The example included below is designed to run on CPU rather than GPU.  It's a simple demonstration only of the process involved with compression.  Real world compression for enterprise use cases will involve much larger models and require multiple GPUs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "setup-header",
   "metadata": {},
   "source": [
    "## 0. Setup\n",
    "\n",
    "Install dependencies. This notebook runs on CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "install-deps",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U \"llmcompressor\" \"transformers>=4.46.0\" \"datasets\" accelerate torch --quiet\n",
    "print(\"Setup complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "MODEL_ID = \"Qwen/Qwen2-0.5B-Instruct\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-1",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. The Oneshot API\n",
    "\n",
    "The `oneshot` function is llm-compressor's main interface for quantization:\n",
    "\n",
    "```python\n",
    "oneshot(\n",
    "    model=\"model-name\",           # HuggingFace model ID\n",
    "    dataset=\"dataset-name\",       # Calibration dataset\n",
    "    recipe=recipe,                # Quantization configuration\n",
    "    output_dir=\"./output\",        # Where to save\n",
    "    num_calibration_samples=512,  # Samples for calibration\n",
    "    max_seq_length=4096,          # Sequence length\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-2",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Quantize the Model\n",
    "\n",
    "We'll quantize `Qwen/Qwen2-0.5B-Instruct` using a `GPTQModifier` recipe with W8A8 scheme."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "quantize",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llmcompressor import oneshot\n",
    "from llmcompressor.modifiers.quantization import GPTQModifier\n",
    "\n",
    "OUTPUT_DIR = \"qwen2-0.5b-W8A8\"\n",
    "\n",
    "# Define the quantization recipe\n",
    "recipe = [\n",
    "    GPTQModifier(\n",
    "        scheme=\"W8A8\",        # 8-bit weights and activations\n",
    "        targets=\"Linear\",     # Quantize Linear layers\n",
    "        ignore=[\"lm_head\"],   # Keep output layer in full precision\n",
    "    ),\n",
    "]\n",
    "\n",
    "# Run quantization\n",
    "oneshot(\n",
    "    model=MODEL_ID,\n",
    "    dataset=\"wikitext\",\n",
    "    dataset_config_name=\"wikitext-2-raw-v1\",\n",
    "    recipe=recipe,\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    max_seq_length=64,\n",
    "    num_calibration_samples=8,\n",
    ")\n",
    "\n",
    "print(f\"\\nQuantized model saved to: {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "recipe-explanation",
   "metadata": {},
   "source": [
    "### GPTQModifier Parameters\n",
    "\n",
    "| Parameter | Purpose | Common Values |\n",
    "|-----------|---------|---------------|\n",
    "| `scheme` | Precision format | `W4A16`, `W8A16`, `W8A8` |\n",
    "| `targets` | Layer types to quantize | `\"Linear\"` |\n",
    "| `ignore` | Layers to keep in FP16 | `[\"lm_head\"]` |\n",
    "| `group_size` | Weights per scale factor | `128` (default), `64` |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-3",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Compare File Sizes\n",
    "\n",
    "Let's compare the size of the quantized model to the original."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "size-helpers",
   "metadata": {},
   "outputs": [],
   "source": [
    "def folder_size(path: str) -> int:\n",
    "    \"\"\"Get total size of files in a directory\"\"\"\n",
    "    p = pathlib.Path(path)\n",
    "    if not p.exists():\n",
    "        return 0\n",
    "    return sum(f.stat().st_size for f in p.rglob(\"*\") if f.is_file())\n",
    "\n",
    "def format_size(bytes: int) -> str:\n",
    "    \"\"\"Format bytes as human-readable string\"\"\"\n",
    "    if bytes < 1024:\n",
    "        return f\"{bytes} B\"\n",
    "    elif bytes < 1024**2:\n",
    "        return f\"{bytes/1024:.1f} KB\"\n",
    "    elif bytes < 1024**3:\n",
    "        return f\"{bytes/1024**2:.1f} MB\"\n",
    "    return f\"{bytes/1024**3:.2f} GB\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "compare-size",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get quantized model size\n",
    "size_q = folder_size(OUTPUT_DIR)\n",
    "\n",
    "print(\"Model Size Comparison\")\n",
    "print(\"=\" * 40)\n",
    "print(f\"Original (FP16):    ~1.0 GB\")\n",
    "print(f\"Quantized (W8A8):   {format_size(size_q)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "list-files",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List the quantized model files\n",
    "!ls -lh {OUTPUT_DIR}/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "summary",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary\n",
    "\n",
    "You've learned:\n",
    "\n",
    "âœ… The `oneshot` API is llm-compressor's main interface for quantization\n",
    "\n",
    "âœ… `GPTQModifier` configures the quantization algorithm and scheme\n",
    "\n",
    "âœ… Key parameters: `scheme`, `targets`, `ignore`, `group_size`\n",
    "\n",
    "âœ… Quantization significantly reduces model size\n",
    "\n",
    "**Next:** Continue to [Advanced Quantization](./2-advanced-quantization.ipynb) to explore different schemes and compare trade-offs."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
