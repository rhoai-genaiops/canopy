{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "11d63ed8",
   "metadata": {},
   "source": [
    "# GuideLLM Benchmark ðŸ§ª\n",
    "\n",
    "We want a quick way to **benchmark** an LLM endpoint before putting anything in production.  \n",
    "For that we can use the **`guidellm benchmark`** CLI ðŸ™Œ\n",
    "\n",
    "We'll keep it simple and **emulate tokens** to measure basic performance.  \n",
    "You can tweak parameters later to suit your stack."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cebf82e",
   "metadata": {},
   "source": [
    "## Set-up\n",
    "\n",
    "Make sure `guidellm` is available in this environment.  \n",
    "If not, use the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35ce9010",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q guidellm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1567edb",
   "metadata": {},
   "source": [
    "## Benchmark target ðŸŽ¯\n",
    "\n",
    "Point to your endpoint and model. Keep names consistent with your serving stack."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2de8c779",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Target & Model (edit these) ----\n",
    "target = \"https://llama32-ai501.apps.cluster-tcdr7.tcdr7.sandbox1789.opentlc.com/v1\"  # endpoint URL\n",
    "model = \"llama32\"  # logical model name on the target\n",
    "processor = \"RedHatAI/Llama-3.2-3B-Instruct-quantized.w8a8\"  # serving identifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2a503f0",
   "metadata": {},
   "source": [
    "## Emulated data ðŸ“¦\n",
    "\n",
    "We'll emulate prompts/outputs with fixed token sizes.  \n",
    "Great for **smoke tests** and **baseline throughput**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30ba3643",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Workload (edit this) ----\n",
    "data = {\n",
    "    \"type\": \"emulated\",\n",
    "    \"prompt_tokens\": 512,\n",
    "    \"output_tokens\": 128,\n",
    "}\n",
    "rate_type = \"synchronous\"   # pacing type\n",
    "max_seconds = 60            # total duration (seconds)\n",
    "output_path = \"benchmark-results.yaml\"  # where results are written\n",
    "\n",
    "# Optional passthrough flags, e.g.:\n",
    "# extra_flags = \"--concurrency=4 --headers-file=/tmp/headers.json\"\n",
    "extra_flags = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae4c7fc3",
   "metadata": {},
   "source": [
    "## Preview the command ðŸ‘€\n",
    "\n",
    "Guidellm is ran as a CLI rather than a Python library, so here we compile the command before we run it.  \n",
    "This let's us check the exact CLI before running."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49fc5171",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, shlex\n",
    "\n",
    "def build_guidellm_command(\n",
    "    target: str,\n",
    "    model: str,\n",
    "    processor: str,\n",
    "    data: dict,\n",
    "    output_path: str,\n",
    "    rate_type: str,\n",
    "    max_seconds: int,\n",
    "    extra_flags: str = \"\",\n",
    "):\n",
    "    data_json = json.dumps(data, separators=(\",\", \":\"))\n",
    "    cmd_list = [\n",
    "        \"guidellm\", \"benchmark\",\n",
    "        f\"--target={target}\",\n",
    "        f\"--model={model}\",\n",
    "        f\"--processor={processor}\",\n",
    "        f\"--data={data_json}\",\n",
    "        f\"--output-path={output_path}\",\n",
    "        f\"--rate-type={rate_type}\",\n",
    "        f\"--max-seconds={int(max_seconds)}\",\n",
    "    ]\n",
    "    if extra_flags.strip():\n",
    "        cmd_list.extend(extra_flags.strip().split())\n",
    "\n",
    "    # pretty shell string (quotes + JSON wrapped in single quotes)\n",
    "    data_shell = f\"--data='{data_json}'\"\n",
    "    parts = [\n",
    "        \"guidellm benchmark\",\n",
    "        f\"--target={shlex.quote(target)}\",\n",
    "        f\"--model={shlex.quote(model)}\",\n",
    "        f\"--processor={shlex.quote(processor)}\",\n",
    "        data_shell,\n",
    "        f\"--output-path={shlex.quote(output_path)}\",\n",
    "        f\"--rate-type={shlex.quote(rate_type)}\",\n",
    "        f\"--max-seconds={int(max_seconds)}\",\n",
    "    ]\n",
    "    if extra_flags.strip():\n",
    "        parts.append(extra_flags)\n",
    "\n",
    "    cmd_shell = \" \".join(parts)\n",
    "    return cmd_list, cmd_shell\n",
    "\n",
    "cmd_list, cmd_shell = build_guidellm_command(\n",
    "    target, model, processor, data, output_path, rate_type, max_seconds, extra_flags\n",
    ")\n",
    "\n",
    "print(\"Shell command:\\n\")\n",
    "print(cmd_shell)\n",
    "print(\"\\nSubprocess list:\\n\")\n",
    "print(cmd_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c8e22ff",
   "metadata": {},
   "source": [
    "## Run it â–¶ï¸\n",
    "\n",
    "Execute the benchmark and stream logs here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "417d73bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil, subprocess, sys\n",
    "\n",
    "if shutil.which(\"guidellm\") is None:\n",
    "    raise RuntimeError(\n",
    "        \"The 'guidellm' CLI is not available in this environment. \"\n",
    "        \"Install it in the Set-up section and re-run.\"\n",
    "    )\n",
    "\n",
    "print(\"Starting benchmark...\\n\")\n",
    "process = subprocess.Popen(cmd_list, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, text=True)\n",
    "for line in process.stdout:\n",
    "    sys.stdout.write(line)\n",
    "ret = process.wait()\n",
    "\n",
    "print(f\"\\nDone. Exit code: {ret}\")\n",
    "if ret != 0:\n",
    "    print(\"Non-zero exit code. Review the logs above.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a25bdfc",
   "metadata": {},
   "source": [
    "## Inspect results ðŸ“\n",
    "\n",
    "Quick peek at the output file to confirm success."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de3051bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "p = Path(output_path)\n",
    "if p.exists():\n",
    "    print(f\"Results: {p.resolve()}\")\n",
    "    try:\n",
    "        print(\"\\n--- First 200 lines ---\\n\")\n",
    "        with p.open(\"r\", encoding=\"utf-8\", errors=\"replace\") as f:\n",
    "            for i, line in enumerate(f):\n",
    "                if i >= 200:\n",
    "                    print(\"... (truncated)\")\n",
    "                    break\n",
    "                print(line.rstrip(\"\\n\"))\n",
    "    except Exception as e:\n",
    "        print(f\"Preview error: {e}\")\n",
    "else:\n",
    "    print(f\"Not found: {p}. Double-check --output-path.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51896814",
   "metadata": {},
   "source": [
    "## Tips & tweaks ðŸ’¡\n",
    "\n",
    "Feel free to play around with it some more, here are things you can try:\n",
    "\n",
    "- Increase throughput: `--concurrency=4`  \n",
    "- Limit requests instead of seconds: `--max-requests=N`  \n",
    "- Change format (if supported): `--output-format=yaml|json`  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "app-root",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
