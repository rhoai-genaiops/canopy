{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GuideLLM Canopy Backend Benchmark üöÄ\n",
    "\n",
    "This notebook tests our **Canopy backend** using a forked GuideLLM that supports our Canopy endpoint (technically all endpoints that take text in and responds in streaming like Canopy, but we add a check to see if it's Canopy just to make sure).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set-up\n",
    "\n",
    "First, let's install the modified version of GuideLLM.  \n",
    "You can find it here if you are interested: [https://github.com/RHRolun/guidellm](https://github.com/RHRolun/guidellm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q git+https://github.com/RHRolun/guidellm.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmark Configuration üéØ\n",
    "\n",
    "With native Canopy support, we simply use models prefixed with \"canopy_\" to automatically enable Canopy backend behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Target & Endpoint Configuration ----\n",
    "base_url = \"http://canopy-backend:8000\"\n",
    "endpoint = \"/summarize\"\n",
    "\n",
    "# Complete target URL with endpoint path\n",
    "target = f\"{base_url}{endpoint}\"  # e.g., \"https://...com/summarize\"\n",
    "model = \"canopy_system\"  # Models starting with \"canopy_\" automatically enable Canopy support\n",
    "processor = \"RedHatAI/Llama-3.2-3B-Instruct-quantized.w8a8\"  # Use a real HuggingFace model for tokenization, in this case we use the underlying model of Canopy\n",
    "output_path = \"canopy-benchmark-summarize-endpoint.yaml\"\n",
    "\n",
    "print(f\"Base URL: {base_url}\")\n",
    "print(f\"Endpoint: {endpoint}\")\n",
    "print(f\"Full target: {target}\")\n",
    "print(f\"Processor: {processor}\")\n",
    "print(f\"Output file: {output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Emulated Workload üì¶\n",
    "\n",
    "We'll emulate prompts/outputs with fixed token sizes for a baseline throughput test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Workload Configuration ----\n",
    "data = {\n",
    "    \"type\": \"emulated\",\n",
    "    \"prompt_tokens\": 512,\n",
    "    \"output_tokens\": 128,\n",
    "}\n",
    "rate_type = \"synchronous\"   # pacing type\n",
    "max_seconds = 60            # total duration (seconds)\n",
    "\n",
    "backend_args = {}  # No special backend args needed - Canopy support is built-in\n",
    "extra_flags = \"\"  # Add any extra flags here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Command üëÄ\n",
    "\n",
    "Here we generate the GuideLLM command for our Canopy backend and then print it so we know what we are running.  \n",
    "We will use this command later when we productionize evaluations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, shlex\n",
    "\n",
    "def build_canopy_guidellm_command(\n",
    "    target: str,\n",
    "    model: str,  # canopy_summarize or canopy_info_search\n",
    "    processor: str,\n",
    "    data: dict,\n",
    "    output_path: str,\n",
    "    rate_type: str,\n",
    "    max_seconds: int,\n",
    "    backend_args: dict = None,\n",
    "    extra_flags: str = \"\",\n",
    "):\n",
    "    data_json = json.dumps(data, separators=(\",\", \":\"))\n",
    "    \n",
    "    cmd_list = [\n",
    "        \"guidellm\", \"benchmark\",\n",
    "        f\"--target={target}\",\n",
    "        f\"--model={model}\",  # Use canopy model name (starts with \"canopy_\")\n",
    "        f\"--processor={processor}\",\n",
    "        f\"--backend-type=openai_http\",  # OpenAI backend with native Canopy support\n",
    "        f\"--data={data_json}\",\n",
    "        f\"--output-path={output_path}\",\n",
    "        f\"--rate-type={rate_type}\",\n",
    "        f\"--max-seconds={int(max_seconds)}\",\n",
    "    ]\n",
    "    \n",
    "    # Only add backend-args if we have them\n",
    "    if backend_args:\n",
    "        backend_args_json = json.dumps(backend_args, separators=(\",\", \":\"))\n",
    "        cmd_list.append(f\"--backend-args={backend_args_json}\")\n",
    "    \n",
    "    if extra_flags.strip():\n",
    "        cmd_list.extend(extra_flags.strip().split())\n",
    "\n",
    "    # Pretty shell string\n",
    "    parts = [\n",
    "        \"guidellm benchmark\",\n",
    "        f\"--target={shlex.quote(target)}\",\n",
    "        f\"--model={model}\",\n",
    "        f\"--processor={shlex.quote(processor)}\",\n",
    "        f\"--backend-type=openai_http\",\n",
    "        f\"--data='{data_json}'\",\n",
    "        f\"--output-path={shlex.quote(output_path)}\",\n",
    "        f\"--rate-type={shlex.quote(rate_type)}\",\n",
    "        f\"--max-seconds={int(max_seconds)}\",\n",
    "    ]\n",
    "    \n",
    "    # Only add backend-args to display if we have them\n",
    "    if backend_args:\n",
    "        backend_args_json = json.dumps(backend_args, separators=(\",\", \":\"))\n",
    "        parts.insert(-4, f\"--backend-args='{backend_args_json}'\")  # Insert before data\n",
    "    \n",
    "    if extra_flags.strip():\n",
    "        parts.append(extra_flags)\n",
    "\n",
    "    cmd_shell = \" \".join(parts)\n",
    "    return cmd_list, cmd_shell\n",
    "\n",
    "cmd_list, cmd_shell = build_canopy_guidellm_command(\n",
    "    target, model, processor, data, output_path, rate_type, max_seconds, backend_args, extra_flags\n",
    ")\n",
    "\n",
    "print(f\"Canopy {endpoint} endpoint benchmark command (with native support):\\n\")\n",
    "print(cmd_shell)\n",
    "print(\"\\nSubprocess list:\\n\")\n",
    "print(cmd_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Benchmark ‚ñ∂Ô∏è\n",
    "\n",
    "Execute the benchmark against your Canopy backend and stream logs. This might take ~2 minutes #PleaseHold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil, subprocess, sys, os\n",
    "\n",
    "print(f\"üöÄ Starting canopy {endpoint} endpoint benchmark with native support...\\n\")\n",
    "print(f\"Target: {target}\")\n",
    "print(f\"Endpoint: {endpoint}\")\n",
    "print(f\"Model: {model} (enables native Canopy support)\")\n",
    "print(f\"Duration: {max_seconds}s\\n\")\n",
    "\n",
    "if shutil.which(\"guidellm\") is None:\n",
    "    print(\"‚ö†Ô∏è  guidellm command not found in PATH, trying python module...\")\n",
    "    cmd_prefix = [\"python\", \"-m\", \"guidellm\"]\n",
    "else:\n",
    "    cmd_prefix = [\"guidellm\"]\n",
    "\n",
    "# Run guidellm directly - no wrapper needed with native support\n",
    "process = subprocess.Popen([\n",
    "    *cmd_prefix, \"benchmark\"\n",
    "] + cmd_list[2:], stdout=subprocess.PIPE, stderr=subprocess.STDOUT, text=True)\n",
    "\n",
    "for line in process.stdout:\n",
    "    sys.stdout.write(line)\n",
    "ret = process.wait()\n",
    "\n",
    "print(f\"\\n‚úÖ Benchmark completed. Exit code: {ret}\")\n",
    "if ret != 0:\n",
    "    print(\"‚ùå Non-zero exit code. Review the logs above.\")\n",
    "    print(\"   Common issues:\")\n",
    "    print(\"   - Canopy service not accessible\")\n",
    "    print(\"   - Endpoint feature not enabled\") \n",
    "    print(\"   - Network connectivity issues\")\n",
    "    print(\"   - Model name doesn't start with 'canopy_'\")\n",
    "else:\n",
    "    print(f\"üìä Results saved to: {output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspect Results üìù\n",
    "\n",
    "The results are quite long, but here is a small summary of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "p = Path(output_path)\n",
    "if p.exists():\n",
    "    print(f\"üìã Results: {p.resolve()}\")\n",
    "    print(f\"üìä Benchmarked: Canopy {endpoint} endpoint (native support)\")\n",
    "    print(f\"üéØ Target: {target}\")\n",
    "    print(f\"‚è±Ô∏è  Duration: {max_seconds}s\\n\")\n",
    "    \n",
    "    try:\n",
    "        print(\"--- First 200 lines ---\\n\")\n",
    "        with p.open(\"r\", encoding=\"utf-8\", errors=\"replace\") as f:\n",
    "            for i, line in enumerate(f):\n",
    "                if i >= 200:\n",
    "                    print(\"... (truncated)\")\n",
    "                    break\n",
    "                print(line.rstrip(\"\\n\"))\n",
    "    except Exception as e:\n",
    "        print(f\"Preview error: {e}\")\n",
    "else:\n",
    "    print(f\"‚ùå Results not found: {p}\")\n",
    "    print(\"   Check the benchmark command and logs above.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now, let's learn how to interpret the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding GuideLLM Benchmark Metrics üìä\n",
    "\n",
    "As you can see, GuideLLM produces a **LOT** of outputs, to the point it's difficult to skim through.  \n",
    "Here are a few key performance indicators that are good to look at and what range they should fall into.  \n",
    "If you run the cell below, you can see what results we got, and if you really want to look through all of the metrics yourself you can find them in `canopy-benchmark-summarize-endpoint.yaml`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**üöÄ Time to First Token (TTFT)**\n",
    "- **What it measures**: How long users wait before seeing any response\n",
    "- **Good range**: < 500ms (excellent < 200ms)\n",
    "- **Impact**: User perception of responsiveness\n",
    "\n",
    "**‚ö° Output Tokens per Second** \n",
    "- **What it measures**: Speed of text generation during streaming\n",
    "- **Good range**: 20-100 tokens/sec (depends on model size)\n",
    "- **Impact**: How fast users see text appear\n",
    "\n",
    "**üìà Requests per Second**\n",
    "- **What it measures**: How many complete requests the system handles per second\n",
    "- **Good range**: Varies by use case (0.1-10+ req/sec)\n",
    "- **Impact**: System throughput and user capacity\n",
    "\n",
    "**üéØ Request Latency**\n",
    "- **What it measures**: Total time from request start to completion\n",
    "- **Good range**: < 30 seconds for long responses\n",
    "- **Impact**: Overall user experience\n",
    "\n",
    "**‚öôÔ∏è Inter-Token Latency**\n",
    "- **What it measures**: Consistency of token generation (milliseconds between tokens)\n",
    "- **Good range**: 10-50ms (lower = more consistent)\n",
    "- **Impact**: Smoothness of streaming text\n",
    "\n",
    "**üìä Success Rate**\n",
    "- **What it measures**: Percentage of requests that complete successfully\n",
    "- **Good range**: > 95% (production requirement)\n",
    "- **Impact**: System reliability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from metrics_analyzer import analyze_benchmark_results\n",
    "analyze_benchmark_results(output_path=output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HTML Report üï∏Ô∏è\n",
    "\n",
    "We can also save the results in multiple different formats, here is an example of it as HTML:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from guidellm.benchmark import GenerativeBenchmarksReport\n",
    "\n",
    "report = GenerativeBenchmarksReport.load_file(\n",
    "    path=output_path,\n",
    ")\n",
    "benchmarks = report.benchmarks\n",
    "\n",
    "for benchmark in benchmarks:\n",
    "    print(benchmark.id_)\n",
    "report.save_html(path=\"canopy-benchmark-summarize-endpoint.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then simply download the HTML file by right click > Download and open it locally."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
