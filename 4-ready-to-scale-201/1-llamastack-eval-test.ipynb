{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bdf471be",
   "metadata": {},
   "source": [
    "# Llamastack Eval ü¶ô"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b78b9758",
   "metadata": {},
   "source": [
    "We would like to be able to evaluate our models and applications before they go into production.  \n",
    "To do that we can use the llamastack eval endpoint üôå  \n",
    "It allows us to run prompts and expected answers through different evaluations to make sure that the model answers as we expect.  \n",
    "The prompts and expected answers can either be some you custom add, or it can be taken from an evaluation dataset such as this one from HuggingFace: https://huggingface.co/datasets/llamastack/simpleqa  \n",
    "\n",
    "We will be testing two types of evaluations here:\n",
    "- \"subset_of\" which tests if the LLM output is an exact subset of the expected answer \n",
    "- \"llm_as_judge\" which lets an LLM evaluate how similar the LLM output is to the expected answer\n",
    "\n",
    "In here we will both test the raw model to see how performant it is, as well as the backend endpoints as well so that we also evaluate how effective our system prompts are. We will se even more on evaluating the raw model in a later chapter üòâ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9ef944b",
   "metadata": {},
   "source": [
    "## Set-up\n",
    "Let's start by installing the llamastack client and pointing it to our llamastack server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c5ba7f6d-9c21-411b-9446-7cd7b996564f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install -q git+https://github.com/meta-llama/llama-stack.git@release-0.2.12 rich"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dad4681f-cc69-481b-96ae-20805d9d3b48",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_stack_client import LlamaStackClient\n",
    "from rich.pretty import Pretty\n",
    "\n",
    "base_url = \"http://llama-stack.user1-test.svc.cluster.local:80\"\n",
    "client = LlamaStackClient(\n",
    "    base_url=base_url,\n",
    "    timeout=600.0 # Default is 1 min which is far too little for some agentic tests, we set it to 10 min\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d46e1492",
   "metadata": {},
   "source": [
    "# Llama Stack Eval endpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec78c9c3",
   "metadata": {},
   "source": [
    "Before we evaluate our backend endpoint, let's just try out llamastack eval and see how it works.  \n",
    "First, we can see what scoring functions that Llama Stack Eval provides out-of-the-box."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "32449172",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">[</span>\n",
       "    <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">ScoringFn</span><span style=\"font-weight: bold\">(</span>\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">identifier</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'basic::equality'</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">metadata</span>=<span style=\"font-weight: bold\">{}</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">provider_id</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'basic'</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">return_type</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">ReturnType</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">type</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'number'</span><span style=\"font-weight: bold\">)</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">type</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'scoring_function'</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">description</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'Returns 1.0 if the input is equal to the target, 0.0 otherwise.'</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">params</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">BasicScoringFnParams</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">aggregation_functions</span>=<span style=\"font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">'accuracy'</span><span style=\"font-weight: bold\">]</span>, <span style=\"color: #808000; text-decoration-color: #808000\">type</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'basic'</span><span style=\"font-weight: bold\">)</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">provider_resource_id</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'equality'</span>\n",
       "    <span style=\"font-weight: bold\">)</span>,\n",
       "    <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">ScoringFn</span><span style=\"font-weight: bold\">(</span>\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">identifier</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'basic::subset_of'</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">metadata</span>=<span style=\"font-weight: bold\">{}</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">provider_id</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'basic'</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">return_type</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">ReturnType</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">type</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'number'</span><span style=\"font-weight: bold\">)</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">type</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'scoring_function'</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">description</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'Returns 1.0 if the expected is included in generated, 0.0 otherwise.'</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">params</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">BasicScoringFnParams</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">aggregation_functions</span>=<span style=\"font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">'accuracy'</span><span style=\"font-weight: bold\">]</span>, <span style=\"color: #808000; text-decoration-color: #808000\">type</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'basic'</span><span style=\"font-weight: bold\">)</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">provider_resource_id</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'subset-of'</span>\n",
       "    <span style=\"font-weight: bold\">)</span>,\n",
       "    <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">ScoringFn</span><span style=\"font-weight: bold\">(</span>\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">identifier</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'basic::regex_parser_multiple_choice_answer'</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">metadata</span>=<span style=\"font-weight: bold\">{}</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">provider_id</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'basic'</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">return_type</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">ReturnType</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">type</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'number'</span><span style=\"font-weight: bold\">)</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">type</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'scoring_function'</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">description</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'Extract answer from response matching Answer: [the_answer_letter], and compare with expected </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">result'</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">params</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">RegexParserScoringFnParams</span><span style=\"font-weight: bold\">(</span>\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">aggregation_functions</span>=<span style=\"font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">'accuracy'</span><span style=\"font-weight: bold\">]</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">parsing_regexes</span>=<span style=\"font-weight: bold\">[</span>\n",
       "                <span style=\"color: #008000; text-decoration-color: #008000\">'(?i)The best answer is \\\\s*([A-D]|[ÿ£-ÿØ]|[‡¶Ö]|[‡¶¨]|[‡¶°]|[‡¶¢]|[Ôº°]|[Ôº¢]|[Ôº£]|[Ôº§])'</span>,\n",
       "                <span style=\"color: #008000; text-decoration-color: #008000\">'(?i)Answer\\\\s*:\\\\s*([A-D]|[ÿ£-ÿØ]|[‡¶Ö]|[‡¶¨]|[‡¶°]|[‡¶¢]|[Ôº°]|[Ôº¢]|[Ôº£]|[Ôº§])'</span>,\n",
       "                <span style=\"color: #008000; text-decoration-color: #008000\">'(?i)Answer\\\\s*:\\u200b\\u200b\\u200b\\u200b\\u200b\\u200b\\\\s*([A-D]|[ÿ£-ÿØ]|[‡¶Ö]|[‡¶¨]|[‡¶°]|[‡¶¢]|[Ôº°]|[Ôº¢]|[Ôº£]</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">|[Ôº§])'</span>,\n",
       "                <span style=\"color: #008000; text-decoration-color: #008000\">'(?i)‡¶â‡¶§‡ßç‡¶§‡¶∞\\\\s*:\\\\s*([A-D]|[ÿ£-ÿØ]|[‡¶Ö]|[‡¶¨]|[‡¶°]|[‡¶¢]|[Ôº°]|[Ôº¢]|[Ôº£]|[Ôº§])'</span>,\n",
       "                <span style=\"color: #008000; text-decoration-color: #008000\">'(?i)‡§â‡§§‡•ç‡§§‡§∞\\\\s*:\\\\s*([A-D]|[ÿ£-ÿØ]|[‡¶Ö]|[‡¶¨]|[‡¶°]|[‡¶¢]|[Ôº°]|[Ôº¢]|[Ôº£]|[Ôº§])'</span>,\n",
       "                <span style=\"color: #008000; text-decoration-color: #008000\">'(?i)‡¶â‡¶§‡ßç‡¶§‡¶∞‡¶É\\\\s*([A-D]|[ÿ£-ÿØ]|[‡¶Ö]|[‡¶¨]|[‡¶°]|[‡¶¢]|[Ôº°]|[Ôº¢]|[Ôº£]|[Ôº§])'</span>,\n",
       "                <span style=\"color: #008000; text-decoration-color: #008000\">'(?i)‡¶â‡¶§‡ßç‡¶§‡¶∞\\\\s*:\\\\s*([A-D]|[ÿ£-ÿØ]|[‡¶Ö]|[‡¶¨]|[‡¶°]|[‡¶¢]|[Ôº°]|[Ôº¢]|[Ôº£]|[Ôº§])'</span>,\n",
       "                <span style=\"color: #008000; text-decoration-color: #008000\">'(?i)Antwort\\\\s*:\\\\s*([A-D]|[ÿ£-ÿØ]|[‡¶Ö]|[‡¶¨]|[‡¶°]|[‡¶¢]|[Ôº°]|[Ôº¢]|[Ôº£]|[Ôº§])'</span>,\n",
       "                <span style=\"color: #008000; text-decoration-color: #008000\">'(?i)ÎãµÎ≥Ä\\\\s*:\\\\s*([A-D]|[ÿ£-ÿØ]|[‡¶Ö]|[‡¶¨]|[‡¶°]|[‡¶¢]|[Ôº°]|[Ôº¢]|[Ôº£]|[Ôº§])'</span>,\n",
       "                <span style=\"color: #008000; text-decoration-color: #008000\">'(?i)Ï†ïÎãµ\\\\s*:\\\\s*([A-D]|[ÿ£-ÿØ]|[‡¶Ö]|[‡¶¨]|[‡¶°]|[‡¶¢]|[Ôº°]|[Ôº¢]|[Ôº£]|[Ôº§])'</span>,\n",
       "                <span style=\"color: #008000; text-decoration-color: #008000\">'(?i)Îãµ\\\\s*:\\\\s*([A-D]|[ÿ£-ÿØ]|[‡¶Ö]|[‡¶¨]|[‡¶°]|[‡¶¢]|[Ôº°]|[Ôº¢]|[Ôº£]|[Ôº§])'</span>,\n",
       "                <span style=\"color: #008000; text-decoration-color: #008000\">'(?i)Á≠îÊ°à\\\\s*Ôºö\\\\s*([A-D]|[ÿ£-ÿØ]|[‡¶Ö]|[‡¶¨]|[‡¶°]|[‡¶¢]|[Ôº°]|[Ôº¢]|[Ôº£]|[Ôº§])'</span>,\n",
       "                <span style=\"color: #008000; text-decoration-color: #008000\">'(?i)Á≠îÊ°à\\\\s*:\\\\s*([A-D]|[ÿ£-ÿØ]|[‡¶Ö]|[‡¶¨]|[‡¶°]|[‡¶¢]|[Ôº°]|[Ôº¢]|[Ôº£]|[Ôº§])'</span>,\n",
       "                <span style=\"color: #008000; text-decoration-color: #008000\">'(?i)Á≠î\\\\s*Ôºö\\\\s*([A-D]|[ÿ£-ÿØ]|[‡¶Ö]|[‡¶¨]|[‡¶°]|[‡¶¢]|[Ôº°]|[Ôº¢]|[Ôº£]|[Ôº§])'</span>,\n",
       "                <span style=\"color: #008000; text-decoration-color: #008000\">'(?i)Á≠î\\\\s*:\\\\s*([A-D]|[ÿ£-ÿØ]|[‡¶Ö]|[‡¶¨]|[‡¶°]|[‡¶¢]|[Ôº°]|[Ôº¢]|[Ôº£]|[Ôº§])'</span>,\n",
       "                <span style=\"color: #008000; text-decoration-color: #008000\">'(?i)Á≠îÂ§ç\\\\s*Ôºö\\\\s*([A-D]|[ÿ£-ÿØ]|[‡¶Ö]|[‡¶¨]|[‡¶°]|[‡¶¢]|[Ôº°]|[Ôº¢]|[Ôº£]|[Ôº§])'</span>,\n",
       "                <span style=\"color: #008000; text-decoration-color: #008000\">'(?i)Á≠îÊõ∞\\\\s*Ôºö\\\\s*([A-D]|[ÿ£-ÿØ]|[‡¶Ö]|[‡¶¨]|[‡¶°]|[‡¶¢]|[Ôº°]|[Ôº¢]|[Ôº£]|[Ôº§])'</span>,\n",
       "                <span style=\"color: #008000; text-decoration-color: #008000\">'(?i)ÿßŸÑÿ•ÿ¨ÿßÿ®ÿ©:\\\\s*([A-D]|[ÿ£-ÿØ]|[‡¶Ö]|[‡¶¨]|[‡¶°]|[‡¶¢]|[Ôº°]|[Ôº¢]|[Ôº£]|[Ôº§])'</span>,\n",
       "                <span style=\"color: #008000; text-decoration-color: #008000\">'(?i)ÿßŸÑÿ¨Ÿàÿßÿ®:\\\\s*([A-D]|[ÿ£-ÿØ]|[‡¶Ö]|[‡¶¨]|[‡¶°]|[‡¶¢]|[Ôº°]|[Ôº¢]|[Ôº£]|[Ôº§])'</span>,\n",
       "                <span style=\"color: #008000; text-decoration-color: #008000\">'(?i)ÿ•ÿ¨ÿßÿ®ÿ©:\\\\s*([A-D]|[ÿ£-ÿØ]|[‡¶Ö]|[‡¶¨]|[‡¶°]|[‡¶¢]|[Ôº°]|[Ôº¢]|[Ôº£]|[Ôº§])'</span>,\n",
       "                <span style=\"color: #008000; text-decoration-color: #008000\">'(?i)ÿßŸÑÿ•ÿ¨ÿßÿ®ÿ© ÿßŸÑŸÜŸáÿßÿ¶Ÿäÿ©:\\\\s*([A-D]|[ÿ£-ÿØ]|[‡¶Ö]|[‡¶¨]|[‡¶°]|[‡¶¢]|[Ôº°]|[Ôº¢]|[Ôº£]|[Ôº§])'</span>,\n",
       "                <span style=\"color: #008000; text-decoration-color: #008000\">'(?i)ÿßŸÑÿ•ÿ¨ÿßÿ®ÿ© ÿßŸÑÿµÿ≠Ÿäÿ≠ÿ©:\\\\s*([A-D]|[ÿ£-ÿØ]|[‡¶Ö]|[‡¶¨]|[‡¶°]|[‡¶¢]|[Ôº°]|[Ôº¢]|[Ôº£]|[Ôº§])'</span>,\n",
       "                <span style=\"color: #008000; text-decoration-color: #008000\">'(?i)ÿßŸÑÿ•ÿ¨ÿßÿ®ÿ© ÿßŸÑÿµÿ≠Ÿäÿ≠ÿ© ŸáŸä:\\\\s*([A-D]|[ÿ£-ÿØ]|[‡¶Ö]|[‡¶¨]|[‡¶°]|[‡¶¢]|[Ôº°]|[Ôº¢]|[Ôº£]|[Ôº§])'</span>,\n",
       "                <span style=\"color: #008000; text-decoration-color: #008000\">'(?i)ÿßŸÑÿ•ÿ¨ÿßÿ®ÿ© ŸáŸä:\\\\s*([A-D]|[ÿ£-ÿØ]|[‡¶Ö]|[‡¶¨]|[‡¶°]|[‡¶¢]|[Ôº°]|[Ôº¢]|[Ôº£]|[Ôº§])'</span>,\n",
       "                <span style=\"color: #008000; text-decoration-color: #008000\">'(?i)Respuesta\\\\s*:\\\\s*([A-D]|[ÿ£-ÿØ]|[‡¶Ö]|[‡¶¨]|[‡¶°]|[‡¶¢]|[Ôº°]|[Ôº¢]|[Ôº£]|[Ôº§])'</span>,\n",
       "                <span style=\"color: #008000; text-decoration-color: #008000\">'(?i)Risposta\\\\s*:\\\\s*([A-D]|[ÿ£-ÿØ]|[‡¶Ö]|[‡¶¨]|[‡¶°]|[‡¶¢]|[Ôº°]|[Ôº¢]|[Ôº£]|[Ôº§])'</span>,\n",
       "                <span style=\"color: #008000; text-decoration-color: #008000\">'(?i)Á≠î„Åà\\\\s*:\\\\s*([A-D]|[ÿ£-ÿØ]|[‡¶Ö]|[‡¶¨]|[‡¶°]|[‡¶¢]|[Ôº°]|[Ôº¢]|[Ôº£]|[Ôº§])'</span>,\n",
       "                <span style=\"color: #008000; text-decoration-color: #008000\">'(?i)Á≠î„Åà\\\\s*Ôºö\\\\s*([A-D]|[ÿ£-ÿØ]|[‡¶Ö]|[‡¶¨]|[‡¶°]|[‡¶¢]|[Ôº°]|[Ôº¢]|[Ôº£]|[Ôº§])'</span>,\n",
       "                <span style=\"color: #008000; text-decoration-color: #008000\">'(?i)ÂõûÁ≠î\\\\s*:\\\\s*([A-D]|[ÿ£-ÿØ]|[‡¶Ö]|[‡¶¨]|[‡¶°]|[‡¶¢]|[Ôº°]|[Ôº¢]|[Ôº£]|[Ôº§])'</span>,\n",
       "                <span style=\"color: #008000; text-decoration-color: #008000\">'(?i)ÂõûÁ≠î\\\\s*Ôºö\\\\s*([A-D]|[ÿ£-ÿØ]|[‡¶Ö]|[‡¶¨]|[‡¶°]|[‡¶¢]|[Ôº°]|[Ôº¢]|[Ôº£]|[Ôº§])'</span>,\n",
       "                <span style=\"color: #008000; text-decoration-color: #008000\">'(?i)Ëß£Á≠î\\\\s*:\\\\s*([A-D]|[ÿ£-ÿØ]|[‡¶Ö]|[‡¶¨]|[‡¶°]|[‡¶¢]|[Ôº°]|[Ôº¢]|[Ôº£]|[Ôº§])'</span>,\n",
       "                <span style=\"color: #008000; text-decoration-color: #008000\">'(?i)Jawaban\\\\s*:\\\\s*([A-D]|[ÿ£-ÿØ]|[‡¶Ö]|[‡¶¨]|[‡¶°]|[‡¶¢]|[Ôº°]|[Ôº¢]|[Ôº£]|[Ôº§])'</span>,\n",
       "                <span style=\"color: #008000; text-decoration-color: #008000\">'(?i)R√©ponse\\\\s*:\\\\s*([A-D]|[ÿ£-ÿØ]|[‡¶Ö]|[‡¶¨]|[‡¶°]|[‡¶¢]|[Ôº°]|[Ôº¢]|[Ôº£]|[Ôº§])'</span>,\n",
       "                <span style=\"color: #008000; text-decoration-color: #008000\">'(?i)Resposta\\\\s*:\\\\s*([A-D]|[ÿ£-ÿØ]|[‡¶Ö]|[‡¶¨]|[‡¶°]|[‡¶¢]|[Ôº°]|[Ôº¢]|[Ôº£]|[Ôº§])'</span>,\n",
       "                <span style=\"color: #008000; text-decoration-color: #008000\">'(?i)Jibu\\\\s*:\\\\s*([A-D]|[ÿ£-ÿØ]|[‡¶Ö]|[‡¶¨]|[‡¶°]|[‡¶¢]|[Ôº°]|[Ôº¢]|[Ôº£]|[Ôº§])'</span>,\n",
       "                <span style=\"color: #008000; text-decoration-color: #008000\">'(?i)Idahun\\\\s*:\\\\s*([A-D]|[ÿ£-ÿØ]|[‡¶Ö]|[‡¶¨]|[‡¶°]|[‡¶¢]|[Ôº°]|[Ôº¢]|[Ôº£]|[Ôº§])'</span>,\n",
       "                <span style=\"color: #008000; text-decoration-color: #008000\">'(?i)√åd√°h√πn\\\\s*:\\\\s*([A-D]|[ÿ£-ÿØ]|[‡¶Ö]|[‡¶¨]|[‡¶°]|[‡¶¢]|[Ôº°]|[Ôº¢]|[Ôº£]|[Ôº§])'</span>,\n",
       "                <span style=\"color: #008000; text-decoration-color: #008000\">'(?i)Id√°h√πn\\\\s*:\\\\s*([A-D]|[ÿ£-ÿØ]|[‡¶Ö]|[‡¶¨]|[‡¶°]|[‡¶¢]|[Ôº°]|[Ôº¢]|[Ôº£]|[Ôº§])'</span>,\n",
       "                <span style=\"color: #008000; text-decoration-color: #008000\">'(?i)AÃÄm·ªçÃÄnaÃÄ\\\\s*:\\\\s*([A-D]|[ÿ£-ÿØ]|[‡¶Ö]|[‡¶¨]|[‡¶°]|[‡¶¢]|[Ôº°]|[Ôº¢]|[Ôº£]|[Ôº§])'</span>,\n",
       "                <span style=\"color: #008000; text-decoration-color: #008000\">'(?i)√Äd√°h√πn\\\\s*:\\\\s*([A-D]|[ÿ£-ÿØ]|[‡¶Ö]|[‡¶¨]|[‡¶°]|[‡¶¢]|[Ôº°]|[Ôº¢]|[Ôº£]|[Ôº§])'</span>,\n",
       "                <span style=\"color: #008000; text-decoration-color: #008000\">'(?i)AÃÄnuÃÅgoÃ£\\\\s*:\\\\s*([A-D]|[ÿ£-ÿØ]|[‡¶Ö]|[‡¶¨]|[‡¶°]|[‡¶¢]|[Ôº°]|[Ôº¢]|[Ôº£]|[Ôº§])'</span>,\n",
       "                <span style=\"color: #008000; text-decoration-color: #008000\">'(?i)√Ä·π£√†y√†n\\\\s*:\\\\s*([A-D]|[ÿ£-ÿØ]|[‡¶Ö]|[‡¶¨]|[‡¶°]|[‡¶¢]|[Ôº°]|[Ôº¢]|[Ôº£]|[Ôº§])'</span>\n",
       "            <span style=\"font-weight: bold\">]</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">type</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'regex_parser'</span>\n",
       "        <span style=\"font-weight: bold\">)</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">provider_resource_id</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'regex-parser-multiple-choice-answer'</span>\n",
       "    <span style=\"font-weight: bold\">)</span>,\n",
       "    <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">ScoringFn</span><span style=\"font-weight: bold\">(</span>\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">identifier</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'basic::regex_parser_math_response'</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">metadata</span>=<span style=\"font-weight: bold\">{}</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">provider_id</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'basic'</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">return_type</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">ReturnType</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">type</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'number'</span><span style=\"font-weight: bold\">)</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">type</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'scoring_function'</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">description</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'For math related benchmarks, extract answer from the generated response and expected_answer </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">and see if they match'</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">params</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">RegexParserScoringFnParams</span><span style=\"font-weight: bold\">(</span>\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">aggregation_functions</span>=<span style=\"font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">'accuracy'</span><span style=\"font-weight: bold\">]</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">parsing_regexes</span>=<span style=\"font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">'.*final answer is:?\\\\s*\\\\$\\\\\\\\boxed{(?P&lt;X&gt;.*)}\\\\$'</span><span style=\"font-weight: bold\">]</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">type</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'regex_parser'</span>\n",
       "        <span style=\"font-weight: bold\">)</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">provider_resource_id</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'regex-parser-math-response'</span>\n",
       "    <span style=\"font-weight: bold\">)</span>,\n",
       "    <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">ScoringFn</span><span style=\"font-weight: bold\">(</span>\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">identifier</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'basic::bfcl'</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">metadata</span>=<span style=\"font-weight: bold\">{}</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">provider_id</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'basic'</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">return_type</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">ReturnType</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">type</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'number'</span><span style=\"font-weight: bold\">)</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">type</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'scoring_function'</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">description</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'BFCL complex scoring'</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">params</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">BasicScoringFnParams</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">aggregation_functions</span>=<span style=\"font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">'accuracy'</span><span style=\"font-weight: bold\">]</span>, <span style=\"color: #808000; text-decoration-color: #808000\">type</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'basic'</span><span style=\"font-weight: bold\">)</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">provider_resource_id</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'bfcl'</span>\n",
       "    <span style=\"font-weight: bold\">)</span>,\n",
       "    <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">ScoringFn</span><span style=\"font-weight: bold\">(</span>\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">identifier</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'basic::ifeval'</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">metadata</span>=<span style=\"font-weight: bold\">{}</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">provider_id</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'basic'</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">return_type</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">ReturnType</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">type</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'number'</span><span style=\"font-weight: bold\">)</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">type</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'scoring_function'</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">description</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'Eval intruction follow capacity by checkping how many instructions can be followed in each </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">example'</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">params</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">BasicScoringFnParams</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">aggregation_functions</span>=<span style=\"font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">'weighted_average'</span><span style=\"font-weight: bold\">]</span>, <span style=\"color: #808000; text-decoration-color: #808000\">type</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'basic'</span><span style=\"font-weight: bold\">)</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">provider_resource_id</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'ifeval'</span>\n",
       "    <span style=\"font-weight: bold\">)</span>,\n",
       "    <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">ScoringFn</span><span style=\"font-weight: bold\">(</span>\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">identifier</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'basic::docvqa'</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">metadata</span>=<span style=\"font-weight: bold\">{}</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">provider_id</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'basic'</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">return_type</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">ReturnType</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">type</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'number'</span><span style=\"font-weight: bold\">)</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">type</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'scoring_function'</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">description</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'DocVQA Visual Question &amp; Answer scoring function'</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">params</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">BasicScoringFnParams</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">aggregation_functions</span>=<span style=\"font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">'accuracy'</span><span style=\"font-weight: bold\">]</span>, <span style=\"color: #808000; text-decoration-color: #808000\">type</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'basic'</span><span style=\"font-weight: bold\">)</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">provider_resource_id</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'docvqa'</span>\n",
       "    <span style=\"font-weight: bold\">)</span>,\n",
       "    <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">ScoringFn</span><span style=\"font-weight: bold\">(</span>\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">identifier</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'llm-as-judge::base'</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">metadata</span>=<span style=\"font-weight: bold\">{}</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">provider_id</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'llm-as-judge'</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">return_type</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">ReturnType</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">type</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'number'</span><span style=\"font-weight: bold\">)</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">type</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'scoring_function'</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">description</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'Llm As Judge Scoring Function'</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">params</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">LlmAsJudgeScoringFnParams</span><span style=\"font-weight: bold\">(</span>\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">aggregation_functions</span>=<span style=\"font-weight: bold\">[]</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">judge_model</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'meta-llama/Llama-3.1-405B-Instruct'</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">judge_score_regexes</span>=<span style=\"font-weight: bold\">[]</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">type</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'llm_as_judge'</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">prompt_template</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'Enter custom LLM as Judge Prompt Template'</span>\n",
       "        <span style=\"font-weight: bold\">)</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">provider_resource_id</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'llm-as-judge-base'</span>\n",
       "    <span style=\"font-weight: bold\">)</span>,\n",
       "    <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">ScoringFn</span><span style=\"font-weight: bold\">(</span>\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">identifier</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'llm-as-judge::405b-simpleqa'</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">metadata</span>=<span style=\"font-weight: bold\">{}</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">provider_id</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'llm-as-judge'</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">return_type</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">ReturnType</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">type</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'number'</span><span style=\"font-weight: bold\">)</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">type</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'scoring_function'</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">description</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'Llm As Judge Scoring Function for SimpleQA Benchmark </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">(https://github.com/openai/simple-evals/blob/main/simpleqa_eval.py)'</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">params</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">LlmAsJudgeScoringFnParams</span><span style=\"font-weight: bold\">(</span>\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">aggregation_functions</span>=<span style=\"font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">'categorical_count'</span><span style=\"font-weight: bold\">]</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">judge_model</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'meta-llama/Llama-3.1-405B-Instruct'</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">judge_score_regexes</span>=<span style=\"font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">'(A|B|C)'</span><span style=\"font-weight: bold\">]</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">type</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'llm_as_judge'</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">prompt_template</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'Your job is to look at a question, a gold target, and a predicted answer, and then </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">assign a grade of either [\"CORRECT\", \"INCORRECT\", \"NOT_ATTEMPTED\"].\\nFirst, I will give examples of each grade, and</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">then you will grade a new example.\\nThe following are examples of CORRECT predicted answers.\\n```\\nQuestion: What </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">are the names of Barack Obama\\'s children?\\nGold target: Malia Obama and Sasha Obama\\nPredicted answer 1: sasha and</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">malia obama\\nPredicted answer 2: most people would say Malia and Sasha, but I\\'m not sure and would have to double </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">check\\nPredicted answer 3: Barack Obama has two daughters. Their names are Malia Ann and Natasha Marian, but they </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">are commonly referred to as Malia Obama and Sasha Obama. Malia was born on July 4, 1998, and Sasha was born on June</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">10, 2001.\\n```\\nThese predicted answers are all CORRECT because:\\n    - They fully contain the important </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">information in the gold target.\\n    - They do not contain any information that contradicts the gold target.\\n    -</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Only semantic meaning matters; capitalization, punctuation, grammar, and order don\\'t matter.\\n    - Hedging and </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">guessing are permissible, provided that the gold target is fully included and the response contains no incorrect </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">information or contradictions.\\nThe following are examples of INCORRECT predicted answers.\\n```\\nQuestion: What are</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">the names of Barack Obama\\'s children?\\nGold target: Malia and Sasha\\nPredicted answer 1: Malia.\\nPredicted answer </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">2: Malia, Sasha, and Susan.\\nPredicted answer 3: Barack Obama does not have any children.\\nPredicted answer 4: I </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">think it\\'s either Malia and Sasha. Or it could be Malia and Jackie. Or it could be Joey and Malia.\\nPredicted </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">answer 4: While I don\\'t know their exact names, I can tell you that Barack Obama has three children.\\nPredicted </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">answer 5: It\\'s possible you may mean Betsy and Olivia. However, you should clarify further details with updated </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">references if necessary. Is that the correct answer?\\nPredicted answer 6: It may be the case that Obama\\'s child is</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">named James. However, it\\'s recommended to confirm the most accurate and updated information since this could </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">change over time. This model may not always reflect the most current information.\\n```\\nThese predicted answers are</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">all INCORRECT because:\\n    - A factual statement in the answer contradicts the gold target. Incorrect statements </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">that have some hedging (e.g., \"it is possible that\", \"although i\\'m not sure, i think\") are also considered </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">incorrect.\\nThe following are examples of NOT_ATTEMPTED predicted answers.\\n```\\nQuestion: What are the names of </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Barack Obama\\'s children?\\nGold target: Malia and Sasha\\nPredicted answer 1: I don\\'t know.\\nPredicted answer 2: I </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">need more context about which Obama you are talking about.\\nPredicted answer 3: Without researching the web, I </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">cannot answer this question. However, I can tell you that Barack Obama has two children.\\nPredicted answer 4: </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Barack Obama has two children. I know that one of them is Malia, but I\\'m not sure about the other one.\\n```\\nThese</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">predicted answers are all NOT_ATTEMPTED because:\\n    - The important information in the gold target is not </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">included in the answer.\\n    - No statements in the answer contradict the gold target.\\nAlso note the following </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">things:\\n- For grading questions where the gold target is a number, the predicted answer needs to be correct to the</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">last significant figure in the gold answer. For example, consider a question \"How many citations does the </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Transformer Paper have?\" with gold target \"120k\".\\n    - Predicted answers \"120k\", \"124k\", and 115k\" are all </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">CORRECT.\\n    - Predicted answers \"100k\" and \"113k\" are INCORRECT.\\n    - Predicted answers \"around 100k\" and \"more</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">than 50k\" are considered NOT_ATTEMPTED because they neither confirm nor contradict the gold target.\\n- The gold </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">target may contain more information than the question. In such cases, the predicted answer only needs to contain </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">the information that is in the question.\\n    - For example, consider the question \"What episode did Derek and </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Meredith get legally married in Grey\\'s Anatomy?\" with gold target \"Season 7, Episode 20: White Wedding\". Either </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">\"Season 7, Episode 20\" or \"White Wedding\" would be considered a CORRECT answer.\\n- Do not punish predicted answers </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">if they omit information that would be clearly inferred from the question.\\n    - For example, consider the </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">question \"What city is OpenAI headquartered in?\" and the gold target \"San Francisco, California\". The predicted </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">answer \"San Francisco\" would be considered CORRECT, even though it does not include \"California\".\\n    - Consider </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">the question \"What award did A pretrainer\\'s guide to training data: Measuring the effects of data age, domain </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">coverage, quality, &amp; toxicity win at NAACL \\'24?\", the gold target is \"Outstanding Paper Award\". The predicted </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">answer \"Outstanding Paper\" would be considered CORRECT, because \"award\" is presumed in the question.\\n    - For the</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">question \"What is the height of Jason Wei in meters?\", the gold target is \"1.73 m\". The predicted answer \"1.75\" </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">would be considered CORRECT, because meters is specified in the question.\\n    - For the question \"What is the name</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">of Barack Obama\\'s wife?\", the gold target is \"Michelle Obama\". The predicted answer \"Michelle\" would be considered</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">CORRECT, because the last name can be presumed.\\n- Do not punish for typos in people\\'s name if it\\'s clearly the </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">same name.\\n    - For example, if the gold target is \"Hyung Won Chung\", you can consider the following predicted </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">answers as correct: \"Hyoong Won Choong\", \"Hyungwon Chung\", or \"Hyun Won Chung\".\\nHere is a new example. Simply </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">reply with either CORRECT, INCORRECT, NOT ATTEMPTED. Don\\'t apologize or correct yourself if there was a mistake; </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">we are just trying to grade the answer.\\n```\\nQuestion: {input_query}\\nGold target: {expected_answer}\\nPredicted </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">answer: {generated_answer}\\n```\\nGrade the predicted answer of this new question as one of:\\nA: CORRECT\\nB: </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">INCORRECT\\nC: NOT_ATTEMPTED\\nJust return the letters \"A\", \"B\", or \"C\", with no text around it.'</span>\n",
       "        <span style=\"font-weight: bold\">)</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">provider_resource_id</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'llm-as-judge-405b-simpleqa'</span>\n",
       "    <span style=\"font-weight: bold\">)</span>\n",
       "<span style=\"font-weight: bold\">]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m[\u001b[0m\n",
       "    \u001b[1;35mScoringFn\u001b[0m\u001b[1m(\u001b[0m\n",
       "        \u001b[33midentifier\u001b[0m=\u001b[32m'basic::equality'\u001b[0m,\n",
       "        \u001b[33mmetadata\u001b[0m=\u001b[1m{\u001b[0m\u001b[1m}\u001b[0m,\n",
       "        \u001b[33mprovider_id\u001b[0m=\u001b[32m'basic'\u001b[0m,\n",
       "        \u001b[33mreturn_type\u001b[0m=\u001b[1;35mReturnType\u001b[0m\u001b[1m(\u001b[0m\u001b[33mtype\u001b[0m=\u001b[32m'number'\u001b[0m\u001b[1m)\u001b[0m,\n",
       "        \u001b[33mtype\u001b[0m=\u001b[32m'scoring_function'\u001b[0m,\n",
       "        \u001b[33mdescription\u001b[0m=\u001b[32m'Returns 1.0 if the input is equal to the target, 0.0 otherwise.'\u001b[0m,\n",
       "        \u001b[33mparams\u001b[0m=\u001b[1;35mBasicScoringFnParams\u001b[0m\u001b[1m(\u001b[0m\u001b[33maggregation_functions\u001b[0m=\u001b[1m[\u001b[0m\u001b[32m'accuracy'\u001b[0m\u001b[1m]\u001b[0m, \u001b[33mtype\u001b[0m=\u001b[32m'basic'\u001b[0m\u001b[1m)\u001b[0m,\n",
       "        \u001b[33mprovider_resource_id\u001b[0m=\u001b[32m'equality'\u001b[0m\n",
       "    \u001b[1m)\u001b[0m,\n",
       "    \u001b[1;35mScoringFn\u001b[0m\u001b[1m(\u001b[0m\n",
       "        \u001b[33midentifier\u001b[0m=\u001b[32m'basic::subset_of'\u001b[0m,\n",
       "        \u001b[33mmetadata\u001b[0m=\u001b[1m{\u001b[0m\u001b[1m}\u001b[0m,\n",
       "        \u001b[33mprovider_id\u001b[0m=\u001b[32m'basic'\u001b[0m,\n",
       "        \u001b[33mreturn_type\u001b[0m=\u001b[1;35mReturnType\u001b[0m\u001b[1m(\u001b[0m\u001b[33mtype\u001b[0m=\u001b[32m'number'\u001b[0m\u001b[1m)\u001b[0m,\n",
       "        \u001b[33mtype\u001b[0m=\u001b[32m'scoring_function'\u001b[0m,\n",
       "        \u001b[33mdescription\u001b[0m=\u001b[32m'Returns 1.0 if the expected is included in generated, 0.0 otherwise.'\u001b[0m,\n",
       "        \u001b[33mparams\u001b[0m=\u001b[1;35mBasicScoringFnParams\u001b[0m\u001b[1m(\u001b[0m\u001b[33maggregation_functions\u001b[0m=\u001b[1m[\u001b[0m\u001b[32m'accuracy'\u001b[0m\u001b[1m]\u001b[0m, \u001b[33mtype\u001b[0m=\u001b[32m'basic'\u001b[0m\u001b[1m)\u001b[0m,\n",
       "        \u001b[33mprovider_resource_id\u001b[0m=\u001b[32m'subset-of'\u001b[0m\n",
       "    \u001b[1m)\u001b[0m,\n",
       "    \u001b[1;35mScoringFn\u001b[0m\u001b[1m(\u001b[0m\n",
       "        \u001b[33midentifier\u001b[0m=\u001b[32m'basic::regex_parser_multiple_choice_answer'\u001b[0m,\n",
       "        \u001b[33mmetadata\u001b[0m=\u001b[1m{\u001b[0m\u001b[1m}\u001b[0m,\n",
       "        \u001b[33mprovider_id\u001b[0m=\u001b[32m'basic'\u001b[0m,\n",
       "        \u001b[33mreturn_type\u001b[0m=\u001b[1;35mReturnType\u001b[0m\u001b[1m(\u001b[0m\u001b[33mtype\u001b[0m=\u001b[32m'number'\u001b[0m\u001b[1m)\u001b[0m,\n",
       "        \u001b[33mtype\u001b[0m=\u001b[32m'scoring_function'\u001b[0m,\n",
       "        \u001b[33mdescription\u001b[0m=\u001b[32m'Extract answer from response matching Answer: \u001b[0m\u001b[32m[\u001b[0m\u001b[32mthe_answer_letter\u001b[0m\u001b[32m]\u001b[0m\u001b[32m, and compare with expected \u001b[0m\n",
       "\u001b[32mresult'\u001b[0m,\n",
       "        \u001b[33mparams\u001b[0m=\u001b[1;35mRegexParserScoringFnParams\u001b[0m\u001b[1m(\u001b[0m\n",
       "            \u001b[33maggregation_functions\u001b[0m=\u001b[1m[\u001b[0m\u001b[32m'accuracy'\u001b[0m\u001b[1m]\u001b[0m,\n",
       "            \u001b[33mparsing_regexes\u001b[0m=\u001b[1m[\u001b[0m\n",
       "                \u001b[32m'\u001b[0m\u001b[32m(\u001b[0m\u001b[32m?i\u001b[0m\u001b[32m)\u001b[0m\u001b[32mThe best answer is \\\\s*\u001b[0m\u001b[32m(\u001b[0m\u001b[32m[\u001b[0m\u001b[32mA-D\u001b[0m\u001b[32m]\u001b[0m\u001b[32m|\u001b[0m\u001b[32m[\u001b[0m\u001b[32mÿ£-ÿØ\u001b[0m\u001b[32m]\u001b[0m\u001b[32m|\u001b[0m\u001b[32m[\u001b[0m\u001b[32m‡¶Ö\u001b[0m\u001b[32m]\u001b[0m\u001b[32m|\u001b[0m\u001b[32m[\u001b[0m\u001b[32m‡¶¨\u001b[0m\u001b[32m]\u001b[0m\u001b[32m|\u001b[0m\u001b[32m[\u001b[0m\u001b[32m‡¶°\u001b[0m\u001b[32m]\u001b[0m\u001b[32m|\u001b[0m\u001b[32m[\u001b[0m\u001b[32m‡¶¢\u001b[0m\u001b[32m]\u001b[0m\u001b[32m|\u001b[0m\u001b[32m[\u001b[0m\u001b[32mÔº°\u001b[0m\u001b[32m]\u001b[0m\u001b[32m|\u001b[0m\u001b[32m[\u001b[0m\u001b[32mÔº¢\u001b[0m\u001b[32m]\u001b[0m\u001b[32m|\u001b[0m\u001b[32m[\u001b[0m\u001b[32mÔº£\u001b[0m\u001b[32m]\u001b[0m\u001b[32m|\u001b[0m\u001b[32m[\u001b[0m\u001b[32mÔº§\u001b[0m\u001b[32m]\u001b[0m\u001b[32m)\u001b[0m\u001b[32m'\u001b[0m,\n",
       "                \u001b[32m'\u001b[0m\u001b[32m(\u001b[0m\u001b[32m?i\u001b[0m\u001b[32m)\u001b[0m\u001b[32mAnswer\\\\s*:\\\\s*\u001b[0m\u001b[32m(\u001b[0m\u001b[32m[\u001b[0m\u001b[32mA-D\u001b[0m\u001b[32m]\u001b[0m\u001b[32m|\u001b[0m\u001b[32m[\u001b[0m\u001b[32mÿ£-ÿØ\u001b[0m\u001b[32m]\u001b[0m\u001b[32m|\u001b[0m\u001b[32m[\u001b[0m\u001b[32m‡¶Ö\u001b[0m\u001b[32m]\u001b[0m\u001b[32m|\u001b[0m\u001b[32m[\u001b[0m\u001b[32m‡¶¨\u001b[0m\u001b[32m]\u001b[0m\u001b[32m|\u001b[0m\u001b[32m[\u001b[0m\u001b[32m‡¶°\u001b[0m\u001b[32m]\u001b[0m\u001b[32m|\u001b[0m\u001b[32m[\u001b[0m\u001b[32m‡¶¢\u001b[0m\u001b[32m]\u001b[0m\u001b[32m|\u001b[0m\u001b[32m[\u001b[0m\u001b[32mÔº°\u001b[0m\u001b[32m]\u001b[0m\u001b[32m|\u001b[0m\u001b[32m[\u001b[0m\u001b[32mÔº¢\u001b[0m\u001b[32m]\u001b[0m\u001b[32m|\u001b[0m\u001b[32m[\u001b[0m\u001b[32mÔº£\u001b[0m\u001b[32m]\u001b[0m\u001b[32m|\u001b[0m\u001b[32m[\u001b[0m\u001b[32mÔº§\u001b[0m\u001b[32m]\u001b[0m\u001b[32m)\u001b[0m\u001b[32m'\u001b[0m,\n",
       "                \u001b[32m'\u001b[0m\u001b[32m(\u001b[0m\u001b[32m?i\u001b[0m\u001b[32m)\u001b[0m\u001b[32mAnswer\\\\s*:\\u200b\\u200b\\u200b\\u200b\\u200b\\u200b\\\\s*\u001b[0m\u001b[32m(\u001b[0m\u001b[32m[\u001b[0m\u001b[32mA-D\u001b[0m\u001b[32m]\u001b[0m\u001b[32m|\u001b[0m\u001b[32m[\u001b[0m\u001b[32mÿ£-ÿØ\u001b[0m\u001b[32m]\u001b[0m\u001b[32m|\u001b[0m\u001b[32m[\u001b[0m\u001b[32m‡¶Ö\u001b[0m\u001b[32m]\u001b[0m\u001b[32m|\u001b[0m\u001b[32m[\u001b[0m\u001b[32m‡¶¨\u001b[0m\u001b[32m]\u001b[0m\u001b[32m|\u001b[0m\u001b[32m[\u001b[0m\u001b[32m‡¶°\u001b[0m\u001b[32m]\u001b[0m\u001b[32m|\u001b[0m\u001b[32m[\u001b[0m\u001b[32m‡¶¢\u001b[0m\u001b[32m]\u001b[0m\u001b[32m|\u001b[0m\u001b[32m[\u001b[0m\u001b[32mÔº°\u001b[0m\u001b[32m]\u001b[0m\u001b[32m|\u001b[0m\u001b[32m[\u001b[0m\u001b[32mÔº¢\u001b[0m\u001b[32m]\u001b[0m\u001b[32m|\u001b[0m\u001b[32m[\u001b[0m\u001b[32mÔº£\u001b[0m\u001b[32m]\u001b[0m\n",
       "\u001b[32m|\u001b[0m\u001b[32m[\u001b[0m\u001b[32mÔº§\u001b[0m\u001b[32m]\u001b[0m\u001b[32m)\u001b[0m\u001b[32m'\u001b[0m,\n",
       "                \u001b[32m'\u001b[0m\u001b[32m(\u001b[0m\u001b[32m?i\u001b[0m\u001b[32m)\u001b[0m\u001b[32m‡¶â‡¶§‡ßç‡¶§‡¶∞\\\\s*:\\\\s*\u001b[0m\u001b[32m(\u001b[0m\u001b[32m[\u001b[0m\u001b[32mA-D\u001b[0m\u001b[32m]\u001b[0m\u001b[32m|\u001b[0m\u001b[32m[\u001b[0m\u001b[32mÿ£-ÿØ\u001b[0m\u001b[32m]\u001b[0m\u001b[32m|\u001b[0m\u001b[32m[\u001b[0m\u001b[32m‡¶Ö\u001b[0m\u001b[32m]\u001b[0m\u001b[32m|\u001b[0m\u001b[32m[\u001b[0m\u001b[32m‡¶¨\u001b[0m\u001b[32m]\u001b[0m\u001b[32m|\u001b[0m\u001b[32m[\u001b[0m\u001b[32m‡¶°\u001b[0m\u001b[32m]\u001b[0m\u001b[32m|\u001b[0m\u001b[32m[\u001b[0m\u001b[32m‡¶¢\u001b[0m\u001b[32m]\u001b[0m\u001b[32m|\u001b[0m\u001b[32m[\u001b[0m\u001b[32mÔº°\u001b[0m\u001b[32m]\u001b[0m\u001b[32m|\u001b[0m\u001b[32m[\u001b[0m\u001b[32mÔº¢\u001b[0m\u001b[32m]\u001b[0m\u001b[32m|\u001b[0m\u001b[32m[\u001b[0m\u001b[32mÔº£\u001b[0m\u001b[32m]\u001b[0m\u001b[32m|\u001b[0m\u001b[32m[\u001b[0m\u001b[32mÔº§\u001b[0m\u001b[32m]\u001b[0m\u001b[32m)\u001b[0m\u001b[32m'\u001b[0m,\n",
       "                \u001b[32m'\u001b[0m\u001b[32m(\u001b[0m\u001b[32m?i\u001b[0m\u001b[32m)\u001b[0m\u001b[32m‡§â‡§§‡•ç‡§§‡§∞\\\\s*:\\\\s*\u001b[0m\u001b[32m(\u001b[0m\u001b[32m[\u001b[0m\u001b[32mA-D\u001b[0m\u001b[32m]\u001b[0m\u001b[32m|\u001b[0m\u001b[32m[\u001b[0m\u001b[32mÿ£-ÿØ\u001b[0m\u001b[32m]\u001b[0m\u001b[32m|\u001b[0m\u001b[32m[\u001b[0m\u001b[32m‡¶Ö\u001b[0m\u001b[32m]\u001b[0m\u001b[32m|\u001b[0m\u001b[32m[\u001b[0m\u001b[32m‡¶¨\u001b[0m\u001b[32m]\u001b[0m\u001b[32m|\u001b[0m\u001b[32m[\u001b[0m\u001b[32m‡¶°\u001b[0m\u001b[32m]\u001b[0m\u001b[32m|\u001b[0m\u001b[32m[\u001b[0m\u001b[32m‡¶¢\u001b[0m\u001b[32m]\u001b[0m\u001b[32m|\u001b[0m\u001b[32m[\u001b[0m\u001b[32mÔº°\u001b[0m\u001b[32m]\u001b[0m\u001b[32m|\u001b[0m\u001b[32m[\u001b[0m\u001b[32mÔº¢\u001b[0m\u001b[32m]\u001b[0m\u001b[32m|\u001b[0m\u001b[32m[\u001b[0m\u001b[32mÔº£\u001b[0m\u001b[32m]\u001b[0m\u001b[32m|\u001b[0m\u001b[32m[\u001b[0m\u001b[32mÔº§\u001b[0m\u001b[32m]\u001b[0m\u001b[32m)\u001b[0m\u001b[32m'\u001b[0m,\n",
       "                \u001b[32m'\u001b[0m\u001b[32m(\u001b[0m\u001b[32m?i\u001b[0m\u001b[32m)\u001b[0m\u001b[32m‡¶â‡¶§‡ßç‡¶§‡¶∞‡¶É\\\\s*\u001b[0m\u001b[32m(\u001b[0m\u001b[32m[\u001b[0m\u001b[32mA-D\u001b[0m\u001b[32m]\u001b[0m\u001b[32m|\u001b[0m\u001b[32m[\u001b[0m\u001b[32mÿ£-ÿØ\u001b[0m\u001b[32m]\u001b[0m\u001b[32m|\u001b[0m\u001b[32m[\u001b[0m\u001b[32m‡¶Ö\u001b[0m\u001b[32m]\u001b[0m\u001b[32m|\u001b[0m\u001b[32m[\u001b[0m\u001b[32m‡¶¨\u001b[0m\u001b[32m]\u001b[0m\u001b[32m|\u001b[0m\u001b[32m[\u001b[0m\u001b[32m‡¶°\u001b[0m\u001b[32m]\u001b[0m\u001b[32m|\u001b[0m\u001b[32m[\u001b[0m\u001b[32m‡¶¢\u001b[0m\u001b[32m]\u001b[0m\u001b[32m|\u001b[0m\u001b[32m[\u001b[0m\u001b[32mÔº°\u001b[0m\u001b[32m]\u001b[0m\u001b[32m|\u001b[0m\u001b[32m[\u001b[0m\u001b[32mÔº¢\u001b[0m\u001b[32m]\u001b[0m\u001b[32m|\u001b[0m\u001b[32m[\u001b[0m\u001b[32mÔº£\u001b[0m\u001b[32m]\u001b[0m\u001b[32m|\u001b[0m\u001b[32m[\u001b[0m\u001b[32mÔº§\u001b[0m\u001b[32m]\u001b[0m\u001b[32m)\u001b[0m\u001b[32m'\u001b[0m,\n",
       "                \u001b[32m'\u001b[0m\u001b[32m(\u001b[0m\u001b[32m?i\u001b[0m\u001b[32m)\u001b[0m\u001b[32m‡¶â‡¶§‡ßç‡¶§‡¶∞\\\\s*:\\\\s*\u001b[0m\u001b[32m(\u001b[0m\u001b[32m[\u001b[0m\u001b[32mA-D\u001b[0m\u001b[32m]\u001b[0m\u001b[32m|\u001b[0m\u001b[32m[\u001b[0m\u001b[32mÿ£-ÿØ\u001b[0m\u001b[32m]\u001b[0m\u001b[32m|\u001b[0m\u001b[32m[\u001b[0m\u001b[32m‡¶Ö\u001b[0m\u001b[32m]\u001b[0m\u001b[32m|\u001b[0m\u001b[32m[\u001b[0m\u001b[32m‡¶¨\u001b[0m\u001b[32m]\u001b[0m\u001b[32m|\u001b[0m\u001b[32m[\u001b[0m\u001b[32m‡¶°\u001b[0m\u001b[32m]\u001b[0m\u001b[32m|\u001b[0m\u001b[32m[\u001b[0m\u001b[32m‡¶¢\u001b[0m\u001b[32m]\u001b[0m\u001b[32m|\u001b[0m\u001b[32m[\u001b[0m\u001b[32mÔº°\u001b[0m\u001b[32m]\u001b[0m\u001b[32m|\u001b[0m\u001b[32m[\u001b[0m\u001b[32mÔº¢\u001b[0m\u001b[32m]\u001b[0m\u001b[32m|\u001b[0m\u001b[32m[\u001b[0m\u001b[32mÔº£\u001b[0m\u001b[32m]\u001b[0m\u001b[32m|\u001b[0m\u001b[32m[\u001b[0m\u001b[32mÔº§\u001b[0m\u001b[32m]\u001b[0m\u001b[32m)\u001b[0m\u001b[32m'\u001b[0m,\n",
       "                \u001b[32m'\u001b[0m\u001b[32m(\u001b[0m\u001b[32m?i\u001b[0m\u001b[32m)\u001b[0m\u001b[32mAntwort\\\\s*:\\\\s*\u001b[0m\u001b[32m(\u001b[0m\u001b[32m[\u001b[0m\u001b[32mA-D\u001b[0m\u001b[32m]\u001b[0m\u001b[32m|\u001b[0m\u001b[32m[\u001b[0m\u001b[32mÿ£-ÿØ\u001b[0m\u001b[32m]\u001b[0m\u001b[32m|\u001b[0m\u001b[32m[\u001b[0m\u001b[32m‡¶Ö\u001b[0m\u001b[32m]\u001b[0m\u001b[32m|\u001b[0m\u001b[32m[\u001b[0m\u001b[32m‡¶¨\u001b[0m\u001b[32m]\u001b[0m\u001b[32m|\u001b[0m\u001b[32m[\u001b[0m\u001b[32m‡¶°\u001b[0m\u001b[32m]\u001b[0m\u001b[32m|\u001b[0m\u001b[32m[\u001b[0m\u001b[32m‡¶¢\u001b[0m\u001b[32m]\u001b[0m\u001b[32m|\u001b[0m\u001b[32m[\u001b[0m\u001b[32mÔº°\u001b[0m\u001b[32m]\u001b[0m\u001b[32m|\u001b[0m\u001b[32m[\u001b[0m\u001b[32mÔº¢\u001b[0m\u001b[32m]\u001b[0m\u001b[32m|\u001b[0m\u001b[32m[\u001b[0m\u001b[32mÔº£\u001b[0m\u001b[32m]\u001b[0m\u001b[32m|\u001b[0m\u001b[32m[\u001b[0m\u001b[32mÔº§\u001b[0m\u001b[32m]\u001b[0m\u001b[32m)\u001b[0m\u001b[32m'\u001b[0m,\n",
       "                \u001b[32m'\u001b[0m\u001b[32m(\u001b[0m\u001b[32m?i\u001b[0m\u001b[32m)\u001b[0m\u001b[32mÎãµÎ≥Ä\\\\s*:\\\\s*\u001b[0m\u001b[32m(\u001b[0m\u001b[32m[\u001b[0m\u001b[32mA-D\u001b[0m\u001b[32m]\u001b[0m\u001b[32m|\u001b[0m\u001b[32m[\u001b[0m\u001b[32mÿ£-ÿØ\u001b[0m\u001b[32m]\u001b[0m\u001b[32m|\u001b[0m\u001b[32m[\u001b[0m\u001b[32m‡¶Ö\u001b[0m\u001b[32m]\u001b[0m\u001b[32m|\u001b[0m\u001b[32m[\u001b[0m\u001b[32m‡¶¨\u001b[0m\u001b[32m]\u001b[0m\u001b[32m|\u001b[0m\u001b[32m[\u001b[0m\u001b[32m‡¶°\u001b[0m\u001b[32m]\u001b[0m\u001b[32m|\u001b[0m\u001b[32m[\u001b[0m\u001b[32m‡¶¢\u001b[0m\u001b[32m]\u001b[0m\u001b[32m|\u001b[0m\u001b[32m[\u001b[0m\u001b[32mÔº°\u001b[0m\u001b[32m]\u001b[0m\u001b[32m|\u001b[0m\u001b[32m[\u001b[0m\u001b[32mÔº¢\u001b[0m\u001b[32m]\u001b[0m\u001b[32m|\u001b[0m\u001b[32m[\u001b[0m\u001b[32mÔº£\u001b[0m\u001b[32m]\u001b[0m\u001b[32m|\u001b[0m\u001b[32m[\u001b[0m\u001b[32mÔº§\u001b[0m\u001b[32m]\u001b[0m\u001b[32m)\u001b[0m\u001b[32m'\u001b[0m,\n",
       "                \u001b[32m'\u001b[0m\u001b[32m(\u001b[0m\u001b[32m?i\u001b[0m\u001b[32m)\u001b[0m\u001b[32mÏ†ïÎãµ\\\\s*:\\\\s*\u001b[0m\u001b[32m(\u001b[0m\u001b[32m[\u001b[0m\u001b[32mA-D\u001b[0m\u001b[32m]\u001b[0m\u001b[32m|\u001b[0m\u001b[32m[\u001b[0m\u001b[32mÿ£-ÿØ\u001b[0m\u001b[32m]\u001b[0m\u001b[32m|\u001b[0m\u001b[32m[\u001b[0m\u001b[32m‡¶Ö\u001b[0m\u001b[32m]\u001b[0m\u001b[32m|\u001b[0m\u001b[32m[\u001b[0m\u001b[32m‡¶¨\u001b[0m\u001b[32m]\u001b[0m\u001b[32m|\u001b[0m\u001b[32m[\u001b[0m\u001b[32m‡¶°\u001b[0m\u001b[32m]\u001b[0m\u001b[32m|\u001b[0m\u001b[32m[\u001b[0m\u001b[32m‡¶¢\u001b[0m\u001b[32m]\u001b[0m\u001b[32m|\u001b[0m\u001b[32m[\u001b[0m\u001b[32mÔº°\u001b[0m\u001b[32m]\u001b[0m\u001b[32m|\u001b[0m\u001b[32m[\u001b[0m\u001b[32mÔº¢\u001b[0m\u001b[32m]\u001b[0m\u001b[32m|\u001b[0m\u001b[32m[\u001b[0m\u001b[32mÔº£\u001b[0m\u001b[32m]\u001b[0m\u001b[32m|\u001b[0m\u001b[32m[\u001b[0m\u001b[32mÔº§\u001b[0m\u001b[32m]\u001b[0m\u001b[32m)\u001b[0m\u001b[32m'\u001b[0m,\n",
       "                \u001b[32m'\u001b[0m\u001b[32m(\u001b[0m\u001b[32m?i\u001b[0m\u001b[32m)\u001b[0m\u001b[32mÎãµ\\\\s*:\\\\s*\u001b[0m\u001b[32m(\u001b[0m\u001b[32m[\u001b[0m\u001b[32mA-D\u001b[0m\u001b[32m]\u001b[0m\u001b[32m|\u001b[0m\u001b[32m[\u001b[0m\u001b[32mÿ£-ÿØ\u001b[0m\u001b[32m]\u001b[0m\u001b[32m|\u001b[0m\u001b[32m[\u001b[0m\u001b[32m‡¶Ö\u001b[0m\u001b[32m]\u001b[0m\u001b[32m|\u001b[0m\u001b[32m[\u001b[0m\u001b[32m‡¶¨\u001b[0m\u001b[32m]\u001b[0m\u001b[32m|\u001b[0m\u001b[32m[\u001b[0m\u001b[32m‡¶°\u001b[0m\u001b[32m]\u001b[0m\u001b[32m|\u001b[0m\u001b[32m[\u001b[0m\u001b[32m‡¶¢\u001b[0m\u001b[32m]\u001b[0m\u001b[32m|\u001b[0m\u001b[32m[\u001b[0m\u001b[32mÔº°\u001b[0m\u001b[32m]\u001b[0m\u001b[32m|\u001b[0m\u001b[32m[\u001b[0m\u001b[32mÔº¢\u001b[0m\u001b[32m]\u001b[0m\u001b[32m|\u001b[0m\u001b[32m[\u001b[0m\u001b[32mÔº£\u001b[0m\u001b[32m]\u001b[0m\u001b[32m|\u001b[0m\u001b[32m[\u001b[0m\u001b[32mÔº§\u001b[0m\u001b[32m]\u001b[0m\u001b[32m)\u001b[0m\u001b[32m'\u001b[0m,\n",
       "                \u001b[32m'\u001b[0m\u001b[32m(\u001b[0m\u001b[32m?i\u001b[0m\u001b[32m)\u001b[0m\u001b[32mÁ≠îÊ°à\\\\s*Ôºö\\\\s*\u001b[0m\u001b[32m(\u001b[0m\u001b[32m[\u001b[0m\u001b[32mA-D\u001b[0m\u001b[32m]\u001b[0m\u001b[32m|\u001b[0m\u001b[32m[\u001b[0m\u001b[32mÿ£-ÿØ\u001b[0m\u001b[32m]\u001b[0m\u001b[32m|\u001b[0m\u001b[32m[\u001b[0m\u001b[32m‡¶Ö\u001b[0m\u001b[32m]\u001b[0m\u001b[32m|\u001b[0m\u001b[32m[\u001b[0m\u001b[32m‡¶¨\u001b[0m\u001b[32m]\u001b[0m\u001b[32m|\u001b[0m\u001b[32m[\u001b[0m\u001b[32m‡¶°\u001b[0m\u001b[32m]\u001b[0m\u001b[32m|\u001b[0m\u001b[32m[\u001b[0m\u001b[32m‡¶¢\u001b[0m\u001b[32m]\u001b[0m\u001b[32m|\u001b[0m\u001b[32m[\u001b[0m\u001b[32mÔº°\u001b[0m\u001b[32m]\u001b[0m\u001b[32m|\u001b[0m\u001b[32m[\u001b[0m\u001b[32mÔº¢\u001b[0m\u001b[32m]\u001b[0m\u001b[32m|\u001b[0m\u001b[32m[\u001b[0m\u001b[32mÔº£\u001b[0m\u001b[32m]\u001b[0m\u001b[32m|\u001b[0m\u001b[32m[\u001b[0m\u001b[32mÔº§\u001b[0m\u001b[32m]\u001b[0m\u001b[32m)\u001b[0m\u001b[32m'\u001b[0m,\n",
       "                \u001b[32m'\u001b[0m\u001b[32m(\u001b[0m\u001b[32m?i\u001b[0m\u001b[32m)\u001b[0m\u001b[32mÁ≠îÊ°à\\\\s*:\\\\s*\u001b[0m\u001b[32m(\u001b[0m\u001b[32m[\u001b[0m\u001b[32mA-D\u001b[0m\u001b[32m]\u001b[0m\u001b[32m|\u001b[0m\u001b[32m[\u001b[0m\u001b[32mÿ£-ÿØ\u001b[0m\u001b[32m]\u001b[0m\u001b[32m|\u001b[0m\u001b[32m[\u001b[0m\u001b[32m‡¶Ö\u001b[0m\u001b[32m]\u001b[0m\u001b[32m|\u001b[0m\u001b[32m[\u001b[0m\u001b[32m‡¶¨\u001b[0m\u001b[32m]\u001b[0m\u001b[32m|\u001b[0m\u001b[32m[\u001b[0m\u001b[32m‡¶°\u001b[0m\u001b[32m]\u001b[0m\u001b[32m|\u001b[0m\u001b[32m[\u001b[0m\u001b[32m‡¶¢\u001b[0m\u001b[32m]\u001b[0m\u001b[32m|\u001b[0m\u001b[32m[\u001b[0m\u001b[32mÔº°\u001b[0m\u001b[32m]\u001b[0m\u001b[32m|\u001b[0m\u001b[32m[\u001b[0m\u001b[32mÔº¢\u001b[0m\u001b[32m]\u001b[0m\u001b[32m|\u001b[0m\u001b[32m[\u001b[0m\u001b[32mÔº£\u001b[0m\u001b[32m]\u001b[0m\u001b[32m|\u001b[0m\u001b[32m[\u001b[0m\u001b[32mÔº§\u001b[0m\u001b[32m]\u001b[0m\u001b[32m)\u001b[0m\u001b[32m'\u001b[0m,\n",
       "                \u001b[32m'\u001b[0m\u001b[32m(\u001b[0m\u001b[32m?i\u001b[0m\u001b[32m)\u001b[0m\u001b[32mÁ≠î\\\\s*Ôºö\\\\s*\u001b[0m\u001b[32m(\u001b[0m\u001b[32m[\u001b[0m\u001b[32mA-D\u001b[0m\u001b[32m]\u001b[0m\u001b[32m|\u001b[0m\u001b[32m[\u001b[0m\u001b[32mÿ£-ÿØ\u001b[0m\u001b[32m]\u001b[0m\u001b[32m|\u001b[0m\u001b[32m[\u001b[0m\u001b[32m‡¶Ö\u001b[0m\u001b[32m]\u001b[0m\u001b[32m|\u001b[0m\u001b[32m[\u001b[0m\u001b[32m‡¶¨\u001b[0m\u001b[32m]\u001b[0m\u001b[32m|\u001b[0m\u001b[32m[\u001b[0m\u001b[32m‡¶°\u001b[0m\u001b[32m]\u001b[0m\u001b[32m|\u001b[0m\u001b[32m[\u001b[0m\u001b[32m‡¶¢\u001b[0m\u001b[32m]\u001b[0m\u001b[32m|\u001b[0m\u001b[32m[\u001b[0m\u001b[32mÔº°\u001b[0m\u001b[32m]\u001b[0m\u001b[32m|\u001b[0m\u001b[32m[\u001b[0m\u001b[32mÔº¢\u001b[0m\u001b[32m]\u001b[0m\u001b[32m|\u001b[0m\u001b[32m[\u001b[0m\u001b[32mÔº£\u001b[0m\u001b[32m]\u001b[0m\u001b[32m|\u001b[0m\u001b[32m[\u001b[0m\u001b[32mÔº§\u001b[0m\u001b[32m]\u001b[0m\u001b[32m)\u001b[0m\u001b[32m'\u001b[0m,\n",
       "                \u001b[32m'\u001b[0m\u001b[32m(\u001b[0m\u001b[32m?i\u001b[0m\u001b[32m)\u001b[0m\u001b[32mÁ≠î\\\\s*:\\\\s*\u001b[0m\u001b[32m(\u001b[0m\u001b[32m[\u001b[0m\u001b[32mA-D\u001b[0m\u001b[32m]\u001b[0m\u001b[32m|\u001b[0m\u001b[32m[\u001b[0m\u001b[32mÿ£-ÿØ\u001b[0m\u001b[32m]\u001b[0m\u001b[32m|\u001b[0m\u001b[32m[\u001b[0m\u001b[32m‡¶Ö\u001b[0m\u001b[32m]\u001b[0m\u001b[32m|\u001b[0m\u001b[32m[\u001b[0m\u001b[32m‡¶¨\u001b[0m\u001b[32m]\u001b[0m\u001b[32m|\u001b[0m\u001b[32m[\u001b[0m\u001b[32m‡¶°\u001b[0m\u001b[32m]\u001b[0m\u001b[32m|\u001b[0m\u001b[32m[\u001b[0m\u001b[32m‡¶¢\u001b[0m\u001b[32m]\u001b[0m\u001b[32m|\u001b[0m\u001b[32m[\u001b[0m\u001b[32mÔº°\u001b[0m\u001b[32m]\u001b[0m\u001b[32m|\u001b[0m\u001b[32m[\u001b[0m\u001b[32mÔº¢\u001b[0m\u001b[32m]\u001b[0m\u001b[32m|\u001b[0m\u001b[32m[\u001b[0m\u001b[32mÔº£\u001b[0m\u001b[32m]\u001b[0m\u001b[32m|\u001b[0m\u001b[32m[\u001b[0m\u001b[32mÔº§\u001b[0m\u001b[32m]\u001b[0m\u001b[32m)\u001b[0m\u001b[32m'\u001b[0m,\n",
       "                \u001b[32m'\u001b[0m\u001b[32m(\u001b[0m\u001b[32m?i\u001b[0m\u001b[32m)\u001b[0m\u001b[32mÁ≠îÂ§ç\\\\s*Ôºö\\\\s*\u001b[0m\u001b[32m(\u001b[0m\u001b[32m[\u001b[0m\u001b[32mA-D\u001b[0m\u001b[32m]\u001b[0m\u001b[32m|\u001b[0m\u001b[32m[\u001b[0m\u001b[32mÿ£-ÿØ\u001b[0m\u001b[32m]\u001b[0m\u001b[32m|\u001b[0m\u001b[32m[\u001b[0m\u001b[32m‡¶Ö\u001b[0m\u001b[32m]\u001b[0m\u001b[32m|\u001b[0m\u001b[32m[\u001b[0m\u001b[32m‡¶¨\u001b[0m\u001b[32m]\u001b[0m\u001b[32m|\u001b[0m\u001b[32m[\u001b[0m\u001b[32m‡¶°\u001b[0m\u001b[32m]\u001b[0m\u001b[32m|\u001b[0m\u001b[32m[\u001b[0m\u001b[32m‡¶¢\u001b[0m\u001b[32m]\u001b[0m\u001b[32m|\u001b[0m\u001b[32m[\u001b[0m\u001b[32mÔº°\u001b[0m\u001b[32m]\u001b[0m\u001b[32m|\u001b[0m\u001b[32m[\u001b[0m\u001b[32mÔº¢\u001b[0m\u001b[32m]\u001b[0m\u001b[32m|\u001b[0m\u001b[32m[\u001b[0m\u001b[32mÔº£\u001b[0m\u001b[32m]\u001b[0m\u001b[32m|\u001b[0m\u001b[32m[\u001b[0m\u001b[32mÔº§\u001b[0m\u001b[32m]\u001b[0m\u001b[32m)\u001b[0m\u001b[32m'\u001b[0m,\n",
       "                \u001b[32m'\u001b[0m\u001b[32m(\u001b[0m\u001b[32m?i\u001b[0m\u001b[32m)\u001b[0m\u001b[32mÁ≠îÊõ∞\\\\s*Ôºö\\\\s*\u001b[0m\u001b[32m(\u001b[0m\u001b[32m[\u001b[0m\u001b[32mA-D\u001b[0m\u001b[32m]\u001b[0m\u001b[32m|\u001b[0m\u001b[32m[\u001b[0m\u001b[32mÿ£-ÿØ\u001b[0m\u001b[32m]\u001b[0m\u001b[32m|\u001b[0m\u001b[32m[\u001b[0m\u001b[32m‡¶Ö\u001b[0m\u001b[32m]\u001b[0m\u001b[32m|\u001b[0m\u001b[32m[\u001b[0m\u001b[32m‡¶¨\u001b[0m\u001b[32m]\u001b[0m\u001b[32m|\u001b[0m\u001b[32m[\u001b[0m\u001b[32m‡¶°\u001b[0m\u001b[32m]\u001b[0m\u001b[32m|\u001b[0m\u001b[32m[\u001b[0m\u001b[32m‡¶¢\u001b[0m\u001b[32m]\u001b[0m\u001b[32m|\u001b[0m\u001b[32m[\u001b[0m\u001b[32mÔº°\u001b[0m\u001b[32m]\u001b[0m\u001b[32m|\u001b[0m\u001b[32m[\u001b[0m\u001b[32mÔº¢\u001b[0m\u001b[32m]\u001b[0m\u001b[32m|\u001b[0m\u001b[32m[\u001b[0m\u001b[32mÔº£\u001b[0m\u001b[32m]\u001b[0m\u001b[32m|\u001b[0m\u001b[32m[\u001b[0m\u001b[32mÔº§\u001b[0m\u001b[32m]\u001b[0m\u001b[32m)\u001b[0m\u001b[32m'\u001b[0m,\n",
       "                \u001b[32m'\u001b[0m\u001b[32m(\u001b[0m\u001b[32m?i\u001b[0m\u001b[32m)\u001b[0m\u001b[32mÿßŸÑÿ•ÿ¨ÿßÿ®ÿ©:\\\\s*\u001b[0m\u001b[32m(\u001b[0m\u001b[32m[\u001b[0m\u001b[32mA-D\u001b[0m\u001b[32m]\u001b[0m\u001b[32m|\u001b[0m\u001b[32m[\u001b[0m\u001b[32mÿ£-ÿØ\u001b[0m\u001b[32m]\u001b[0m\u001b[32m|\u001b[0m\u001b[32m[\u001b[0m\u001b[32m‡¶Ö\u001b[0m\u001b[32m]\u001b[0m\u001b[32m|\u001b[0m\u001b[32m[\u001b[0m\u001b[32m‡¶¨\u001b[0m\u001b[32m]\u001b[0m\u001b[32m|\u001b[0m\u001b[32m[\u001b[0m\u001b[32m‡¶°\u001b[0m\u001b[32m]\u001b[0m\u001b[32m|\u001b[0m\u001b[32m[\u001b[0m\u001b[32m‡¶¢\u001b[0m\u001b[32m]\u001b[0m\u001b[32m|\u001b[0m\u001b[32m[\u001b[0m\u001b[32mÔº°\u001b[0m\u001b[32m]\u001b[0m\u001b[32m|\u001b[0m\u001b[32m[\u001b[0m\u001b[32mÔº¢\u001b[0m\u001b[32m]\u001b[0m\u001b[32m|\u001b[0m\u001b[32m[\u001b[0m\u001b[32mÔº£\u001b[0m\u001b[32m]\u001b[0m\u001b[32m|\u001b[0m\u001b[32m[\u001b[0m\u001b[32mÔº§\u001b[0m\u001b[32m]\u001b[0m\u001b[32m)\u001b[0m\u001b[32m'\u001b[0m,\n",
       "                \u001b[32m'\u001b[0m\u001b[32m(\u001b[0m\u001b[32m?i\u001b[0m\u001b[32m)\u001b[0m\u001b[32mÿßŸÑÿ¨Ÿàÿßÿ®:\\\\s*\u001b[0m\u001b[32m(\u001b[0m\u001b[32m[\u001b[0m\u001b[32mA-D\u001b[0m\u001b[32m]\u001b[0m\u001b[32m|\u001b[0m\u001b[32m[\u001b[0m\u001b[32mÿ£-ÿØ\u001b[0m\u001b[32m]\u001b[0m\u001b[32m|\u001b[0m\u001b[32m[\u001b[0m\u001b[32m‡¶Ö\u001b[0m\u001b[32m]\u001b[0m\u001b[32m|\u001b[0m\u001b[32m[\u001b[0m\u001b[32m‡¶¨\u001b[0m\u001b[32m]\u001b[0m\u001b[32m|\u001b[0m\u001b[32m[\u001b[0m\u001b[32m‡¶°\u001b[0m\u001b[32m]\u001b[0m\u001b[32m|\u001b[0m\u001b[32m[\u001b[0m\u001b[32m‡¶¢\u001b[0m\u001b[32m]\u001b[0m\u001b[32m|\u001b[0m\u001b[32m[\u001b[0m\u001b[32mÔº°\u001b[0m\u001b[32m]\u001b[0m\u001b[32m|\u001b[0m\u001b[32m[\u001b[0m\u001b[32mÔº¢\u001b[0m\u001b[32m]\u001b[0m\u001b[32m|\u001b[0m\u001b[32m[\u001b[0m\u001b[32mÔº£\u001b[0m\u001b[32m]\u001b[0m\u001b[32m|\u001b[0m\u001b[32m[\u001b[0m\u001b[32mÔº§\u001b[0m\u001b[32m]\u001b[0m\u001b[32m)\u001b[0m\u001b[32m'\u001b[0m,\n",
       "                \u001b[32m'\u001b[0m\u001b[32m(\u001b[0m\u001b[32m?i\u001b[0m\u001b[32m)\u001b[0m\u001b[32mÿ•ÿ¨ÿßÿ®ÿ©:\\\\s*\u001b[0m\u001b[32m(\u001b[0m\u001b[32m[\u001b[0m\u001b[32mA-D\u001b[0m\u001b[32m]\u001b[0m\u001b[32m|\u001b[0m\u001b[32m[\u001b[0m\u001b[32mÿ£-ÿØ\u001b[0m\u001b[32m]\u001b[0m\u001b[32m|\u001b[0m\u001b[32m[\u001b[0m\u001b[32m‡¶Ö\u001b[0m\u001b[32m]\u001b[0m\u001b[32m|\u001b[0m\u001b[32m[\u001b[0m\u001b[32m‡¶¨\u001b[0m\u001b[32m]\u001b[0m\u001b[32m|\u001b[0m\u001b[32m[\u001b[0m\u001b[32m‡¶°\u001b[0m\u001b[32m]\u001b[0m\u001b[32m|\u001b[0m\u001b[32m[\u001b[0m\u001b[32m‡¶¢\u001b[0m\u001b[32m]\u001b[0m\u001b[32m|\u001b[0m\u001b[32m[\u001b[0m\u001b[32mÔº°\u001b[0m\u001b[32m]\u001b[0m\u001b[32m|\u001b[0m\u001b[32m[\u001b[0m\u001b[32mÔº¢\u001b[0m\u001b[32m]\u001b[0m\u001b[32m|\u001b[0m\u001b[32m[\u001b[0m\u001b[32mÔº£\u001b[0m\u001b[32m]\u001b[0m\u001b[32m|\u001b[0m\u001b[32m[\u001b[0m\u001b[32mÔº§\u001b[0m\u001b[32m]\u001b[0m\u001b[32m)\u001b[0m\u001b[32m'\u001b[0m,\n",
       "                \u001b[32m'\u001b[0m\u001b[32m(\u001b[0m\u001b[32m?i\u001b[0m\u001b[32m)\u001b[0m\u001b[32mÿßŸÑÿ•ÿ¨ÿßÿ®ÿ© ÿßŸÑŸÜŸáÿßÿ¶Ÿäÿ©:\\\\s*\u001b[0m\u001b[32m(\u001b[0m\u001b[32m[\u001b[0m\u001b[32mA-D\u001b[0m\u001b[32m]\u001b[0m\u001b[32m|\u001b[0m\u001b[32m[\u001b[0m\u001b[32mÿ£-ÿØ\u001b[0m\u001b[32m]\u001b[0m\u001b[32m|\u001b[0m\u001b[32m[\u001b[0m\u001b[32m‡¶Ö\u001b[0m\u001b[32m]\u001b[0m\u001b[32m|\u001b[0m\u001b[32m[\u001b[0m\u001b[32m‡¶¨\u001b[0m\u001b[32m]\u001b[0m\u001b[32m|\u001b[0m\u001b[32m[\u001b[0m\u001b[32m‡¶°\u001b[0m\u001b[32m]\u001b[0m\u001b[32m|\u001b[0m\u001b[32m[\u001b[0m\u001b[32m‡¶¢\u001b[0m\u001b[32m]\u001b[0m\u001b[32m|\u001b[0m\u001b[32m[\u001b[0m\u001b[32mÔº°\u001b[0m\u001b[32m]\u001b[0m\u001b[32m|\u001b[0m\u001b[32m[\u001b[0m\u001b[32mÔº¢\u001b[0m\u001b[32m]\u001b[0m\u001b[32m|\u001b[0m\u001b[32m[\u001b[0m\u001b[32mÔº£\u001b[0m\u001b[32m]\u001b[0m\u001b[32m|\u001b[0m\u001b[32m[\u001b[0m\u001b[32mÔº§\u001b[0m\u001b[32m]\u001b[0m\u001b[32m)\u001b[0m\u001b[32m'\u001b[0m,\n",
       "                \u001b[32m'\u001b[0m\u001b[32m(\u001b[0m\u001b[32m?i\u001b[0m\u001b[32m)\u001b[0m\u001b[32mÿßŸÑÿ•ÿ¨ÿßÿ®ÿ© ÿßŸÑÿµÿ≠Ÿäÿ≠ÿ©:\\\\s*\u001b[0m\u001b[32m(\u001b[0m\u001b[32m[\u001b[0m\u001b[32mA-D\u001b[0m\u001b[32m]\u001b[0m\u001b[32m|\u001b[0m\u001b[32m[\u001b[0m\u001b[32mÿ£-ÿØ\u001b[0m\u001b[32m]\u001b[0m\u001b[32m|\u001b[0m\u001b[32m[\u001b[0m\u001b[32m‡¶Ö\u001b[0m\u001b[32m]\u001b[0m\u001b[32m|\u001b[0m\u001b[32m[\u001b[0m\u001b[32m‡¶¨\u001b[0m\u001b[32m]\u001b[0m\u001b[32m|\u001b[0m\u001b[32m[\u001b[0m\u001b[32m‡¶°\u001b[0m\u001b[32m]\u001b[0m\u001b[32m|\u001b[0m\u001b[32m[\u001b[0m\u001b[32m‡¶¢\u001b[0m\u001b[32m]\u001b[0m\u001b[32m|\u001b[0m\u001b[32m[\u001b[0m\u001b[32mÔº°\u001b[0m\u001b[32m]\u001b[0m\u001b[32m|\u001b[0m\u001b[32m[\u001b[0m\u001b[32mÔº¢\u001b[0m\u001b[32m]\u001b[0m\u001b[32m|\u001b[0m\u001b[32m[\u001b[0m\u001b[32mÔº£\u001b[0m\u001b[32m]\u001b[0m\u001b[32m|\u001b[0m\u001b[32m[\u001b[0m\u001b[32mÔº§\u001b[0m\u001b[32m]\u001b[0m\u001b[32m)\u001b[0m\u001b[32m'\u001b[0m,\n",
       "                \u001b[32m'\u001b[0m\u001b[32m(\u001b[0m\u001b[32m?i\u001b[0m\u001b[32m)\u001b[0m\u001b[32mÿßŸÑÿ•ÿ¨ÿßÿ®ÿ© ÿßŸÑÿµÿ≠Ÿäÿ≠ÿ© ŸáŸä:\\\\s*\u001b[0m\u001b[32m(\u001b[0m\u001b[32m[\u001b[0m\u001b[32mA-D\u001b[0m\u001b[32m]\u001b[0m\u001b[32m|\u001b[0m\u001b[32m[\u001b[0m\u001b[32mÿ£-ÿØ\u001b[0m\u001b[32m]\u001b[0m\u001b[32m|\u001b[0m\u001b[32m[\u001b[0m\u001b[32m‡¶Ö\u001b[0m\u001b[32m]\u001b[0m\u001b[32m|\u001b[0m\u001b[32m[\u001b[0m\u001b[32m‡¶¨\u001b[0m\u001b[32m]\u001b[0m\u001b[32m|\u001b[0m\u001b[32m[\u001b[0m\u001b[32m‡¶°\u001b[0m\u001b[32m]\u001b[0m\u001b[32m|\u001b[0m\u001b[32m[\u001b[0m\u001b[32m‡¶¢\u001b[0m\u001b[32m]\u001b[0m\u001b[32m|\u001b[0m\u001b[32m[\u001b[0m\u001b[32mÔº°\u001b[0m\u001b[32m]\u001b[0m\u001b[32m|\u001b[0m\u001b[32m[\u001b[0m\u001b[32mÔº¢\u001b[0m\u001b[32m]\u001b[0m\u001b[32m|\u001b[0m\u001b[32m[\u001b[0m\u001b[32mÔº£\u001b[0m\u001b[32m]\u001b[0m\u001b[32m|\u001b[0m\u001b[32m[\u001b[0m\u001b[32mÔº§\u001b[0m\u001b[32m]\u001b[0m\u001b[32m)\u001b[0m\u001b[32m'\u001b[0m,\n",
       "                \u001b[32m'\u001b[0m\u001b[32m(\u001b[0m\u001b[32m?i\u001b[0m\u001b[32m)\u001b[0m\u001b[32mÿßŸÑÿ•ÿ¨ÿßÿ®ÿ© ŸáŸä:\\\\s*\u001b[0m\u001b[32m(\u001b[0m\u001b[32m[\u001b[0m\u001b[32mA-D\u001b[0m\u001b[32m]\u001b[0m\u001b[32m|\u001b[0m\u001b[32m[\u001b[0m\u001b[32mÿ£-ÿØ\u001b[0m\u001b[32m]\u001b[0m\u001b[32m|\u001b[0m\u001b[32m[\u001b[0m\u001b[32m‡¶Ö\u001b[0m\u001b[32m]\u001b[0m\u001b[32m|\u001b[0m\u001b[32m[\u001b[0m\u001b[32m‡¶¨\u001b[0m\u001b[32m]\u001b[0m\u001b[32m|\u001b[0m\u001b[32m[\u001b[0m\u001b[32m‡¶°\u001b[0m\u001b[32m]\u001b[0m\u001b[32m|\u001b[0m\u001b[32m[\u001b[0m\u001b[32m‡¶¢\u001b[0m\u001b[32m]\u001b[0m\u001b[32m|\u001b[0m\u001b[32m[\u001b[0m\u001b[32mÔº°\u001b[0m\u001b[32m]\u001b[0m\u001b[32m|\u001b[0m\u001b[32m[\u001b[0m\u001b[32mÔº¢\u001b[0m\u001b[32m]\u001b[0m\u001b[32m|\u001b[0m\u001b[32m[\u001b[0m\u001b[32mÔº£\u001b[0m\u001b[32m]\u001b[0m\u001b[32m|\u001b[0m\u001b[32m[\u001b[0m\u001b[32mÔº§\u001b[0m\u001b[32m]\u001b[0m\u001b[32m)\u001b[0m\u001b[32m'\u001b[0m,\n",
       "                \u001b[32m'\u001b[0m\u001b[32m(\u001b[0m\u001b[32m?i\u001b[0m\u001b[32m)\u001b[0m\u001b[32mRespuesta\\\\s*:\\\\s*\u001b[0m\u001b[32m(\u001b[0m\u001b[32m[\u001b[0m\u001b[32mA-D\u001b[0m\u001b[32m]\u001b[0m\u001b[32m|\u001b[0m\u001b[32m[\u001b[0m\u001b[32mÿ£-ÿØ\u001b[0m\u001b[32m]\u001b[0m\u001b[32m|\u001b[0m\u001b[32m[\u001b[0m\u001b[32m‡¶Ö\u001b[0m\u001b[32m]\u001b[0m\u001b[32m|\u001b[0m\u001b[32m[\u001b[0m\u001b[32m‡¶¨\u001b[0m\u001b[32m]\u001b[0m\u001b[32m|\u001b[0m\u001b[32m[\u001b[0m\u001b[32m‡¶°\u001b[0m\u001b[32m]\u001b[0m\u001b[32m|\u001b[0m\u001b[32m[\u001b[0m\u001b[32m‡¶¢\u001b[0m\u001b[32m]\u001b[0m\u001b[32m|\u001b[0m\u001b[32m[\u001b[0m\u001b[32mÔº°\u001b[0m\u001b[32m]\u001b[0m\u001b[32m|\u001b[0m\u001b[32m[\u001b[0m\u001b[32mÔº¢\u001b[0m\u001b[32m]\u001b[0m\u001b[32m|\u001b[0m\u001b[32m[\u001b[0m\u001b[32mÔº£\u001b[0m\u001b[32m]\u001b[0m\u001b[32m|\u001b[0m\u001b[32m[\u001b[0m\u001b[32mÔº§\u001b[0m\u001b[32m]\u001b[0m\u001b[32m)\u001b[0m\u001b[32m'\u001b[0m,\n",
       "                \u001b[32m'\u001b[0m\u001b[32m(\u001b[0m\u001b[32m?i\u001b[0m\u001b[32m)\u001b[0m\u001b[32mRisposta\\\\s*:\\\\s*\u001b[0m\u001b[32m(\u001b[0m\u001b[32m[\u001b[0m\u001b[32mA-D\u001b[0m\u001b[32m]\u001b[0m\u001b[32m|\u001b[0m\u001b[32m[\u001b[0m\u001b[32mÿ£-ÿØ\u001b[0m\u001b[32m]\u001b[0m\u001b[32m|\u001b[0m\u001b[32m[\u001b[0m\u001b[32m‡¶Ö\u001b[0m\u001b[32m]\u001b[0m\u001b[32m|\u001b[0m\u001b[32m[\u001b[0m\u001b[32m‡¶¨\u001b[0m\u001b[32m]\u001b[0m\u001b[32m|\u001b[0m\u001b[32m[\u001b[0m\u001b[32m‡¶°\u001b[0m\u001b[32m]\u001b[0m\u001b[32m|\u001b[0m\u001b[32m[\u001b[0m\u001b[32m‡¶¢\u001b[0m\u001b[32m]\u001b[0m\u001b[32m|\u001b[0m\u001b[32m[\u001b[0m\u001b[32mÔº°\u001b[0m\u001b[32m]\u001b[0m\u001b[32m|\u001b[0m\u001b[32m[\u001b[0m\u001b[32mÔº¢\u001b[0m\u001b[32m]\u001b[0m\u001b[32m|\u001b[0m\u001b[32m[\u001b[0m\u001b[32mÔº£\u001b[0m\u001b[32m]\u001b[0m\u001b[32m|\u001b[0m\u001b[32m[\u001b[0m\u001b[32mÔº§\u001b[0m\u001b[32m]\u001b[0m\u001b[32m)\u001b[0m\u001b[32m'\u001b[0m,\n",
       "                \u001b[32m'\u001b[0m\u001b[32m(\u001b[0m\u001b[32m?i\u001b[0m\u001b[32m)\u001b[0m\u001b[32mÁ≠î„Åà\\\\s*:\\\\s*\u001b[0m\u001b[32m(\u001b[0m\u001b[32m[\u001b[0m\u001b[32mA-D\u001b[0m\u001b[32m]\u001b[0m\u001b[32m|\u001b[0m\u001b[32m[\u001b[0m\u001b[32mÿ£-ÿØ\u001b[0m\u001b[32m]\u001b[0m\u001b[32m|\u001b[0m\u001b[32m[\u001b[0m\u001b[32m‡¶Ö\u001b[0m\u001b[32m]\u001b[0m\u001b[32m|\u001b[0m\u001b[32m[\u001b[0m\u001b[32m‡¶¨\u001b[0m\u001b[32m]\u001b[0m\u001b[32m|\u001b[0m\u001b[32m[\u001b[0m\u001b[32m‡¶°\u001b[0m\u001b[32m]\u001b[0m\u001b[32m|\u001b[0m\u001b[32m[\u001b[0m\u001b[32m‡¶¢\u001b[0m\u001b[32m]\u001b[0m\u001b[32m|\u001b[0m\u001b[32m[\u001b[0m\u001b[32mÔº°\u001b[0m\u001b[32m]\u001b[0m\u001b[32m|\u001b[0m\u001b[32m[\u001b[0m\u001b[32mÔº¢\u001b[0m\u001b[32m]\u001b[0m\u001b[32m|\u001b[0m\u001b[32m[\u001b[0m\u001b[32mÔº£\u001b[0m\u001b[32m]\u001b[0m\u001b[32m|\u001b[0m\u001b[32m[\u001b[0m\u001b[32mÔº§\u001b[0m\u001b[32m]\u001b[0m\u001b[32m)\u001b[0m\u001b[32m'\u001b[0m,\n",
       "                \u001b[32m'\u001b[0m\u001b[32m(\u001b[0m\u001b[32m?i\u001b[0m\u001b[32m)\u001b[0m\u001b[32mÁ≠î„Åà\\\\s*Ôºö\\\\s*\u001b[0m\u001b[32m(\u001b[0m\u001b[32m[\u001b[0m\u001b[32mA-D\u001b[0m\u001b[32m]\u001b[0m\u001b[32m|\u001b[0m\u001b[32m[\u001b[0m\u001b[32mÿ£-ÿØ\u001b[0m\u001b[32m]\u001b[0m\u001b[32m|\u001b[0m\u001b[32m[\u001b[0m\u001b[32m‡¶Ö\u001b[0m\u001b[32m]\u001b[0m\u001b[32m|\u001b[0m\u001b[32m[\u001b[0m\u001b[32m‡¶¨\u001b[0m\u001b[32m]\u001b[0m\u001b[32m|\u001b[0m\u001b[32m[\u001b[0m\u001b[32m‡¶°\u001b[0m\u001b[32m]\u001b[0m\u001b[32m|\u001b[0m\u001b[32m[\u001b[0m\u001b[32m‡¶¢\u001b[0m\u001b[32m]\u001b[0m\u001b[32m|\u001b[0m\u001b[32m[\u001b[0m\u001b[32mÔº°\u001b[0m\u001b[32m]\u001b[0m\u001b[32m|\u001b[0m\u001b[32m[\u001b[0m\u001b[32mÔº¢\u001b[0m\u001b[32m]\u001b[0m\u001b[32m|\u001b[0m\u001b[32m[\u001b[0m\u001b[32mÔº£\u001b[0m\u001b[32m]\u001b[0m\u001b[32m|\u001b[0m\u001b[32m[\u001b[0m\u001b[32mÔº§\u001b[0m\u001b[32m]\u001b[0m\u001b[32m)\u001b[0m\u001b[32m'\u001b[0m,\n",
       "                \u001b[32m'\u001b[0m\u001b[32m(\u001b[0m\u001b[32m?i\u001b[0m\u001b[32m)\u001b[0m\u001b[32mÂõûÁ≠î\\\\s*:\\\\s*\u001b[0m\u001b[32m(\u001b[0m\u001b[32m[\u001b[0m\u001b[32mA-D\u001b[0m\u001b[32m]\u001b[0m\u001b[32m|\u001b[0m\u001b[32m[\u001b[0m\u001b[32mÿ£-ÿØ\u001b[0m\u001b[32m]\u001b[0m\u001b[32m|\u001b[0m\u001b[32m[\u001b[0m\u001b[32m‡¶Ö\u001b[0m\u001b[32m]\u001b[0m\u001b[32m|\u001b[0m\u001b[32m[\u001b[0m\u001b[32m‡¶¨\u001b[0m\u001b[32m]\u001b[0m\u001b[32m|\u001b[0m\u001b[32m[\u001b[0m\u001b[32m‡¶°\u001b[0m\u001b[32m]\u001b[0m\u001b[32m|\u001b[0m\u001b[32m[\u001b[0m\u001b[32m‡¶¢\u001b[0m\u001b[32m]\u001b[0m\u001b[32m|\u001b[0m\u001b[32m[\u001b[0m\u001b[32mÔº°\u001b[0m\u001b[32m]\u001b[0m\u001b[32m|\u001b[0m\u001b[32m[\u001b[0m\u001b[32mÔº¢\u001b[0m\u001b[32m]\u001b[0m\u001b[32m|\u001b[0m\u001b[32m[\u001b[0m\u001b[32mÔº£\u001b[0m\u001b[32m]\u001b[0m\u001b[32m|\u001b[0m\u001b[32m[\u001b[0m\u001b[32mÔº§\u001b[0m\u001b[32m]\u001b[0m\u001b[32m)\u001b[0m\u001b[32m'\u001b[0m,\n",
       "                \u001b[32m'\u001b[0m\u001b[32m(\u001b[0m\u001b[32m?i\u001b[0m\u001b[32m)\u001b[0m\u001b[32mÂõûÁ≠î\\\\s*Ôºö\\\\s*\u001b[0m\u001b[32m(\u001b[0m\u001b[32m[\u001b[0m\u001b[32mA-D\u001b[0m\u001b[32m]\u001b[0m\u001b[32m|\u001b[0m\u001b[32m[\u001b[0m\u001b[32mÿ£-ÿØ\u001b[0m\u001b[32m]\u001b[0m\u001b[32m|\u001b[0m\u001b[32m[\u001b[0m\u001b[32m‡¶Ö\u001b[0m\u001b[32m]\u001b[0m\u001b[32m|\u001b[0m\u001b[32m[\u001b[0m\u001b[32m‡¶¨\u001b[0m\u001b[32m]\u001b[0m\u001b[32m|\u001b[0m\u001b[32m[\u001b[0m\u001b[32m‡¶°\u001b[0m\u001b[32m]\u001b[0m\u001b[32m|\u001b[0m\u001b[32m[\u001b[0m\u001b[32m‡¶¢\u001b[0m\u001b[32m]\u001b[0m\u001b[32m|\u001b[0m\u001b[32m[\u001b[0m\u001b[32mÔº°\u001b[0m\u001b[32m]\u001b[0m\u001b[32m|\u001b[0m\u001b[32m[\u001b[0m\u001b[32mÔº¢\u001b[0m\u001b[32m]\u001b[0m\u001b[32m|\u001b[0m\u001b[32m[\u001b[0m\u001b[32mÔº£\u001b[0m\u001b[32m]\u001b[0m\u001b[32m|\u001b[0m\u001b[32m[\u001b[0m\u001b[32mÔº§\u001b[0m\u001b[32m]\u001b[0m\u001b[32m)\u001b[0m\u001b[32m'\u001b[0m,\n",
       "                \u001b[32m'\u001b[0m\u001b[32m(\u001b[0m\u001b[32m?i\u001b[0m\u001b[32m)\u001b[0m\u001b[32mËß£Á≠î\\\\s*:\\\\s*\u001b[0m\u001b[32m(\u001b[0m\u001b[32m[\u001b[0m\u001b[32mA-D\u001b[0m\u001b[32m]\u001b[0m\u001b[32m|\u001b[0m\u001b[32m[\u001b[0m\u001b[32mÿ£-ÿØ\u001b[0m\u001b[32m]\u001b[0m\u001b[32m|\u001b[0m\u001b[32m[\u001b[0m\u001b[32m‡¶Ö\u001b[0m\u001b[32m]\u001b[0m\u001b[32m|\u001b[0m\u001b[32m[\u001b[0m\u001b[32m‡¶¨\u001b[0m\u001b[32m]\u001b[0m\u001b[32m|\u001b[0m\u001b[32m[\u001b[0m\u001b[32m‡¶°\u001b[0m\u001b[32m]\u001b[0m\u001b[32m|\u001b[0m\u001b[32m[\u001b[0m\u001b[32m‡¶¢\u001b[0m\u001b[32m]\u001b[0m\u001b[32m|\u001b[0m\u001b[32m[\u001b[0m\u001b[32mÔº°\u001b[0m\u001b[32m]\u001b[0m\u001b[32m|\u001b[0m\u001b[32m[\u001b[0m\u001b[32mÔº¢\u001b[0m\u001b[32m]\u001b[0m\u001b[32m|\u001b[0m\u001b[32m[\u001b[0m\u001b[32mÔº£\u001b[0m\u001b[32m]\u001b[0m\u001b[32m|\u001b[0m\u001b[32m[\u001b[0m\u001b[32mÔº§\u001b[0m\u001b[32m]\u001b[0m\u001b[32m)\u001b[0m\u001b[32m'\u001b[0m,\n",
       "                \u001b[32m'\u001b[0m\u001b[32m(\u001b[0m\u001b[32m?i\u001b[0m\u001b[32m)\u001b[0m\u001b[32mJawaban\\\\s*:\\\\s*\u001b[0m\u001b[32m(\u001b[0m\u001b[32m[\u001b[0m\u001b[32mA-D\u001b[0m\u001b[32m]\u001b[0m\u001b[32m|\u001b[0m\u001b[32m[\u001b[0m\u001b[32mÿ£-ÿØ\u001b[0m\u001b[32m]\u001b[0m\u001b[32m|\u001b[0m\u001b[32m[\u001b[0m\u001b[32m‡¶Ö\u001b[0m\u001b[32m]\u001b[0m\u001b[32m|\u001b[0m\u001b[32m[\u001b[0m\u001b[32m‡¶¨\u001b[0m\u001b[32m]\u001b[0m\u001b[32m|\u001b[0m\u001b[32m[\u001b[0m\u001b[32m‡¶°\u001b[0m\u001b[32m]\u001b[0m\u001b[32m|\u001b[0m\u001b[32m[\u001b[0m\u001b[32m‡¶¢\u001b[0m\u001b[32m]\u001b[0m\u001b[32m|\u001b[0m\u001b[32m[\u001b[0m\u001b[32mÔº°\u001b[0m\u001b[32m]\u001b[0m\u001b[32m|\u001b[0m\u001b[32m[\u001b[0m\u001b[32mÔº¢\u001b[0m\u001b[32m]\u001b[0m\u001b[32m|\u001b[0m\u001b[32m[\u001b[0m\u001b[32mÔº£\u001b[0m\u001b[32m]\u001b[0m\u001b[32m|\u001b[0m\u001b[32m[\u001b[0m\u001b[32mÔº§\u001b[0m\u001b[32m]\u001b[0m\u001b[32m)\u001b[0m\u001b[32m'\u001b[0m,\n",
       "                \u001b[32m'\u001b[0m\u001b[32m(\u001b[0m\u001b[32m?i\u001b[0m\u001b[32m)\u001b[0m\u001b[32mR√©ponse\\\\s*:\\\\s*\u001b[0m\u001b[32m(\u001b[0m\u001b[32m[\u001b[0m\u001b[32mA-D\u001b[0m\u001b[32m]\u001b[0m\u001b[32m|\u001b[0m\u001b[32m[\u001b[0m\u001b[32mÿ£-ÿØ\u001b[0m\u001b[32m]\u001b[0m\u001b[32m|\u001b[0m\u001b[32m[\u001b[0m\u001b[32m‡¶Ö\u001b[0m\u001b[32m]\u001b[0m\u001b[32m|\u001b[0m\u001b[32m[\u001b[0m\u001b[32m‡¶¨\u001b[0m\u001b[32m]\u001b[0m\u001b[32m|\u001b[0m\u001b[32m[\u001b[0m\u001b[32m‡¶°\u001b[0m\u001b[32m]\u001b[0m\u001b[32m|\u001b[0m\u001b[32m[\u001b[0m\u001b[32m‡¶¢\u001b[0m\u001b[32m]\u001b[0m\u001b[32m|\u001b[0m\u001b[32m[\u001b[0m\u001b[32mÔº°\u001b[0m\u001b[32m]\u001b[0m\u001b[32m|\u001b[0m\u001b[32m[\u001b[0m\u001b[32mÔº¢\u001b[0m\u001b[32m]\u001b[0m\u001b[32m|\u001b[0m\u001b[32m[\u001b[0m\u001b[32mÔº£\u001b[0m\u001b[32m]\u001b[0m\u001b[32m|\u001b[0m\u001b[32m[\u001b[0m\u001b[32mÔº§\u001b[0m\u001b[32m]\u001b[0m\u001b[32m)\u001b[0m\u001b[32m'\u001b[0m,\n",
       "                \u001b[32m'\u001b[0m\u001b[32m(\u001b[0m\u001b[32m?i\u001b[0m\u001b[32m)\u001b[0m\u001b[32mResposta\\\\s*:\\\\s*\u001b[0m\u001b[32m(\u001b[0m\u001b[32m[\u001b[0m\u001b[32mA-D\u001b[0m\u001b[32m]\u001b[0m\u001b[32m|\u001b[0m\u001b[32m[\u001b[0m\u001b[32mÿ£-ÿØ\u001b[0m\u001b[32m]\u001b[0m\u001b[32m|\u001b[0m\u001b[32m[\u001b[0m\u001b[32m‡¶Ö\u001b[0m\u001b[32m]\u001b[0m\u001b[32m|\u001b[0m\u001b[32m[\u001b[0m\u001b[32m‡¶¨\u001b[0m\u001b[32m]\u001b[0m\u001b[32m|\u001b[0m\u001b[32m[\u001b[0m\u001b[32m‡¶°\u001b[0m\u001b[32m]\u001b[0m\u001b[32m|\u001b[0m\u001b[32m[\u001b[0m\u001b[32m‡¶¢\u001b[0m\u001b[32m]\u001b[0m\u001b[32m|\u001b[0m\u001b[32m[\u001b[0m\u001b[32mÔº°\u001b[0m\u001b[32m]\u001b[0m\u001b[32m|\u001b[0m\u001b[32m[\u001b[0m\u001b[32mÔº¢\u001b[0m\u001b[32m]\u001b[0m\u001b[32m|\u001b[0m\u001b[32m[\u001b[0m\u001b[32mÔº£\u001b[0m\u001b[32m]\u001b[0m\u001b[32m|\u001b[0m\u001b[32m[\u001b[0m\u001b[32mÔº§\u001b[0m\u001b[32m]\u001b[0m\u001b[32m)\u001b[0m\u001b[32m'\u001b[0m,\n",
       "                \u001b[32m'\u001b[0m\u001b[32m(\u001b[0m\u001b[32m?i\u001b[0m\u001b[32m)\u001b[0m\u001b[32mJibu\\\\s*:\\\\s*\u001b[0m\u001b[32m(\u001b[0m\u001b[32m[\u001b[0m\u001b[32mA-D\u001b[0m\u001b[32m]\u001b[0m\u001b[32m|\u001b[0m\u001b[32m[\u001b[0m\u001b[32mÿ£-ÿØ\u001b[0m\u001b[32m]\u001b[0m\u001b[32m|\u001b[0m\u001b[32m[\u001b[0m\u001b[32m‡¶Ö\u001b[0m\u001b[32m]\u001b[0m\u001b[32m|\u001b[0m\u001b[32m[\u001b[0m\u001b[32m‡¶¨\u001b[0m\u001b[32m]\u001b[0m\u001b[32m|\u001b[0m\u001b[32m[\u001b[0m\u001b[32m‡¶°\u001b[0m\u001b[32m]\u001b[0m\u001b[32m|\u001b[0m\u001b[32m[\u001b[0m\u001b[32m‡¶¢\u001b[0m\u001b[32m]\u001b[0m\u001b[32m|\u001b[0m\u001b[32m[\u001b[0m\u001b[32mÔº°\u001b[0m\u001b[32m]\u001b[0m\u001b[32m|\u001b[0m\u001b[32m[\u001b[0m\u001b[32mÔº¢\u001b[0m\u001b[32m]\u001b[0m\u001b[32m|\u001b[0m\u001b[32m[\u001b[0m\u001b[32mÔº£\u001b[0m\u001b[32m]\u001b[0m\u001b[32m|\u001b[0m\u001b[32m[\u001b[0m\u001b[32mÔº§\u001b[0m\u001b[32m]\u001b[0m\u001b[32m)\u001b[0m\u001b[32m'\u001b[0m,\n",
       "                \u001b[32m'\u001b[0m\u001b[32m(\u001b[0m\u001b[32m?i\u001b[0m\u001b[32m)\u001b[0m\u001b[32mIdahun\\\\s*:\\\\s*\u001b[0m\u001b[32m(\u001b[0m\u001b[32m[\u001b[0m\u001b[32mA-D\u001b[0m\u001b[32m]\u001b[0m\u001b[32m|\u001b[0m\u001b[32m[\u001b[0m\u001b[32mÿ£-ÿØ\u001b[0m\u001b[32m]\u001b[0m\u001b[32m|\u001b[0m\u001b[32m[\u001b[0m\u001b[32m‡¶Ö\u001b[0m\u001b[32m]\u001b[0m\u001b[32m|\u001b[0m\u001b[32m[\u001b[0m\u001b[32m‡¶¨\u001b[0m\u001b[32m]\u001b[0m\u001b[32m|\u001b[0m\u001b[32m[\u001b[0m\u001b[32m‡¶°\u001b[0m\u001b[32m]\u001b[0m\u001b[32m|\u001b[0m\u001b[32m[\u001b[0m\u001b[32m‡¶¢\u001b[0m\u001b[32m]\u001b[0m\u001b[32m|\u001b[0m\u001b[32m[\u001b[0m\u001b[32mÔº°\u001b[0m\u001b[32m]\u001b[0m\u001b[32m|\u001b[0m\u001b[32m[\u001b[0m\u001b[32mÔº¢\u001b[0m\u001b[32m]\u001b[0m\u001b[32m|\u001b[0m\u001b[32m[\u001b[0m\u001b[32mÔº£\u001b[0m\u001b[32m]\u001b[0m\u001b[32m|\u001b[0m\u001b[32m[\u001b[0m\u001b[32mÔº§\u001b[0m\u001b[32m]\u001b[0m\u001b[32m)\u001b[0m\u001b[32m'\u001b[0m,\n",
       "                \u001b[32m'\u001b[0m\u001b[32m(\u001b[0m\u001b[32m?i\u001b[0m\u001b[32m)\u001b[0m\u001b[32m√åd√°h√πn\\\\s*:\\\\s*\u001b[0m\u001b[32m(\u001b[0m\u001b[32m[\u001b[0m\u001b[32mA-D\u001b[0m\u001b[32m]\u001b[0m\u001b[32m|\u001b[0m\u001b[32m[\u001b[0m\u001b[32mÿ£-ÿØ\u001b[0m\u001b[32m]\u001b[0m\u001b[32m|\u001b[0m\u001b[32m[\u001b[0m\u001b[32m‡¶Ö\u001b[0m\u001b[32m]\u001b[0m\u001b[32m|\u001b[0m\u001b[32m[\u001b[0m\u001b[32m‡¶¨\u001b[0m\u001b[32m]\u001b[0m\u001b[32m|\u001b[0m\u001b[32m[\u001b[0m\u001b[32m‡¶°\u001b[0m\u001b[32m]\u001b[0m\u001b[32m|\u001b[0m\u001b[32m[\u001b[0m\u001b[32m‡¶¢\u001b[0m\u001b[32m]\u001b[0m\u001b[32m|\u001b[0m\u001b[32m[\u001b[0m\u001b[32mÔº°\u001b[0m\u001b[32m]\u001b[0m\u001b[32m|\u001b[0m\u001b[32m[\u001b[0m\u001b[32mÔº¢\u001b[0m\u001b[32m]\u001b[0m\u001b[32m|\u001b[0m\u001b[32m[\u001b[0m\u001b[32mÔº£\u001b[0m\u001b[32m]\u001b[0m\u001b[32m|\u001b[0m\u001b[32m[\u001b[0m\u001b[32mÔº§\u001b[0m\u001b[32m]\u001b[0m\u001b[32m)\u001b[0m\u001b[32m'\u001b[0m,\n",
       "                \u001b[32m'\u001b[0m\u001b[32m(\u001b[0m\u001b[32m?i\u001b[0m\u001b[32m)\u001b[0m\u001b[32mId√°h√πn\\\\s*:\\\\s*\u001b[0m\u001b[32m(\u001b[0m\u001b[32m[\u001b[0m\u001b[32mA-D\u001b[0m\u001b[32m]\u001b[0m\u001b[32m|\u001b[0m\u001b[32m[\u001b[0m\u001b[32mÿ£-ÿØ\u001b[0m\u001b[32m]\u001b[0m\u001b[32m|\u001b[0m\u001b[32m[\u001b[0m\u001b[32m‡¶Ö\u001b[0m\u001b[32m]\u001b[0m\u001b[32m|\u001b[0m\u001b[32m[\u001b[0m\u001b[32m‡¶¨\u001b[0m\u001b[32m]\u001b[0m\u001b[32m|\u001b[0m\u001b[32m[\u001b[0m\u001b[32m‡¶°\u001b[0m\u001b[32m]\u001b[0m\u001b[32m|\u001b[0m\u001b[32m[\u001b[0m\u001b[32m‡¶¢\u001b[0m\u001b[32m]\u001b[0m\u001b[32m|\u001b[0m\u001b[32m[\u001b[0m\u001b[32mÔº°\u001b[0m\u001b[32m]\u001b[0m\u001b[32m|\u001b[0m\u001b[32m[\u001b[0m\u001b[32mÔº¢\u001b[0m\u001b[32m]\u001b[0m\u001b[32m|\u001b[0m\u001b[32m[\u001b[0m\u001b[32mÔº£\u001b[0m\u001b[32m]\u001b[0m\u001b[32m|\u001b[0m\u001b[32m[\u001b[0m\u001b[32mÔº§\u001b[0m\u001b[32m]\u001b[0m\u001b[32m)\u001b[0m\u001b[32m'\u001b[0m,\n",
       "                \u001b[32m'\u001b[0m\u001b[32m(\u001b[0m\u001b[32m?i\u001b[0m\u001b[32m)\u001b[0m\u001b[32mAÃÄm·ªçÃÄnaÃÄ\\\\s*:\\\\s*\u001b[0m\u001b[32m(\u001b[0m\u001b[32m[\u001b[0m\u001b[32mA-D\u001b[0m\u001b[32m]\u001b[0m\u001b[32m|\u001b[0m\u001b[32m[\u001b[0m\u001b[32mÿ£-ÿØ\u001b[0m\u001b[32m]\u001b[0m\u001b[32m|\u001b[0m\u001b[32m[\u001b[0m\u001b[32m‡¶Ö\u001b[0m\u001b[32m]\u001b[0m\u001b[32m|\u001b[0m\u001b[32m[\u001b[0m\u001b[32m‡¶¨\u001b[0m\u001b[32m]\u001b[0m\u001b[32m|\u001b[0m\u001b[32m[\u001b[0m\u001b[32m‡¶°\u001b[0m\u001b[32m]\u001b[0m\u001b[32m|\u001b[0m\u001b[32m[\u001b[0m\u001b[32m‡¶¢\u001b[0m\u001b[32m]\u001b[0m\u001b[32m|\u001b[0m\u001b[32m[\u001b[0m\u001b[32mÔº°\u001b[0m\u001b[32m]\u001b[0m\u001b[32m|\u001b[0m\u001b[32m[\u001b[0m\u001b[32mÔº¢\u001b[0m\u001b[32m]\u001b[0m\u001b[32m|\u001b[0m\u001b[32m[\u001b[0m\u001b[32mÔº£\u001b[0m\u001b[32m]\u001b[0m\u001b[32m|\u001b[0m\u001b[32m[\u001b[0m\u001b[32mÔº§\u001b[0m\u001b[32m]\u001b[0m\u001b[32m)\u001b[0m\u001b[32m'\u001b[0m,\n",
       "                \u001b[32m'\u001b[0m\u001b[32m(\u001b[0m\u001b[32m?i\u001b[0m\u001b[32m)\u001b[0m\u001b[32m√Äd√°h√πn\\\\s*:\\\\s*\u001b[0m\u001b[32m(\u001b[0m\u001b[32m[\u001b[0m\u001b[32mA-D\u001b[0m\u001b[32m]\u001b[0m\u001b[32m|\u001b[0m\u001b[32m[\u001b[0m\u001b[32mÿ£-ÿØ\u001b[0m\u001b[32m]\u001b[0m\u001b[32m|\u001b[0m\u001b[32m[\u001b[0m\u001b[32m‡¶Ö\u001b[0m\u001b[32m]\u001b[0m\u001b[32m|\u001b[0m\u001b[32m[\u001b[0m\u001b[32m‡¶¨\u001b[0m\u001b[32m]\u001b[0m\u001b[32m|\u001b[0m\u001b[32m[\u001b[0m\u001b[32m‡¶°\u001b[0m\u001b[32m]\u001b[0m\u001b[32m|\u001b[0m\u001b[32m[\u001b[0m\u001b[32m‡¶¢\u001b[0m\u001b[32m]\u001b[0m\u001b[32m|\u001b[0m\u001b[32m[\u001b[0m\u001b[32mÔº°\u001b[0m\u001b[32m]\u001b[0m\u001b[32m|\u001b[0m\u001b[32m[\u001b[0m\u001b[32mÔº¢\u001b[0m\u001b[32m]\u001b[0m\u001b[32m|\u001b[0m\u001b[32m[\u001b[0m\u001b[32mÔº£\u001b[0m\u001b[32m]\u001b[0m\u001b[32m|\u001b[0m\u001b[32m[\u001b[0m\u001b[32mÔº§\u001b[0m\u001b[32m]\u001b[0m\u001b[32m)\u001b[0m\u001b[32m'\u001b[0m,\n",
       "                \u001b[32m'\u001b[0m\u001b[32m(\u001b[0m\u001b[32m?i\u001b[0m\u001b[32m)\u001b[0m\u001b[32mAÃÄnuÃÅgoÃ£\\\\s*:\\\\s*\u001b[0m\u001b[32m(\u001b[0m\u001b[32m[\u001b[0m\u001b[32mA-D\u001b[0m\u001b[32m]\u001b[0m\u001b[32m|\u001b[0m\u001b[32m[\u001b[0m\u001b[32mÿ£-ÿØ\u001b[0m\u001b[32m]\u001b[0m\u001b[32m|\u001b[0m\u001b[32m[\u001b[0m\u001b[32m‡¶Ö\u001b[0m\u001b[32m]\u001b[0m\u001b[32m|\u001b[0m\u001b[32m[\u001b[0m\u001b[32m‡¶¨\u001b[0m\u001b[32m]\u001b[0m\u001b[32m|\u001b[0m\u001b[32m[\u001b[0m\u001b[32m‡¶°\u001b[0m\u001b[32m]\u001b[0m\u001b[32m|\u001b[0m\u001b[32m[\u001b[0m\u001b[32m‡¶¢\u001b[0m\u001b[32m]\u001b[0m\u001b[32m|\u001b[0m\u001b[32m[\u001b[0m\u001b[32mÔº°\u001b[0m\u001b[32m]\u001b[0m\u001b[32m|\u001b[0m\u001b[32m[\u001b[0m\u001b[32mÔº¢\u001b[0m\u001b[32m]\u001b[0m\u001b[32m|\u001b[0m\u001b[32m[\u001b[0m\u001b[32mÔº£\u001b[0m\u001b[32m]\u001b[0m\u001b[32m|\u001b[0m\u001b[32m[\u001b[0m\u001b[32mÔº§\u001b[0m\u001b[32m]\u001b[0m\u001b[32m)\u001b[0m\u001b[32m'\u001b[0m,\n",
       "                \u001b[32m'\u001b[0m\u001b[32m(\u001b[0m\u001b[32m?i\u001b[0m\u001b[32m)\u001b[0m\u001b[32m√Ä·π£√†y√†n\\\\s*:\\\\s*\u001b[0m\u001b[32m(\u001b[0m\u001b[32m[\u001b[0m\u001b[32mA-D\u001b[0m\u001b[32m]\u001b[0m\u001b[32m|\u001b[0m\u001b[32m[\u001b[0m\u001b[32mÿ£-ÿØ\u001b[0m\u001b[32m]\u001b[0m\u001b[32m|\u001b[0m\u001b[32m[\u001b[0m\u001b[32m‡¶Ö\u001b[0m\u001b[32m]\u001b[0m\u001b[32m|\u001b[0m\u001b[32m[\u001b[0m\u001b[32m‡¶¨\u001b[0m\u001b[32m]\u001b[0m\u001b[32m|\u001b[0m\u001b[32m[\u001b[0m\u001b[32m‡¶°\u001b[0m\u001b[32m]\u001b[0m\u001b[32m|\u001b[0m\u001b[32m[\u001b[0m\u001b[32m‡¶¢\u001b[0m\u001b[32m]\u001b[0m\u001b[32m|\u001b[0m\u001b[32m[\u001b[0m\u001b[32mÔº°\u001b[0m\u001b[32m]\u001b[0m\u001b[32m|\u001b[0m\u001b[32m[\u001b[0m\u001b[32mÔº¢\u001b[0m\u001b[32m]\u001b[0m\u001b[32m|\u001b[0m\u001b[32m[\u001b[0m\u001b[32mÔº£\u001b[0m\u001b[32m]\u001b[0m\u001b[32m|\u001b[0m\u001b[32m[\u001b[0m\u001b[32mÔº§\u001b[0m\u001b[32m]\u001b[0m\u001b[32m)\u001b[0m\u001b[32m'\u001b[0m\n",
       "            \u001b[1m]\u001b[0m,\n",
       "            \u001b[33mtype\u001b[0m=\u001b[32m'regex_parser'\u001b[0m\n",
       "        \u001b[1m)\u001b[0m,\n",
       "        \u001b[33mprovider_resource_id\u001b[0m=\u001b[32m'regex-parser-multiple-choice-answer'\u001b[0m\n",
       "    \u001b[1m)\u001b[0m,\n",
       "    \u001b[1;35mScoringFn\u001b[0m\u001b[1m(\u001b[0m\n",
       "        \u001b[33midentifier\u001b[0m=\u001b[32m'basic::regex_parser_math_response'\u001b[0m,\n",
       "        \u001b[33mmetadata\u001b[0m=\u001b[1m{\u001b[0m\u001b[1m}\u001b[0m,\n",
       "        \u001b[33mprovider_id\u001b[0m=\u001b[32m'basic'\u001b[0m,\n",
       "        \u001b[33mreturn_type\u001b[0m=\u001b[1;35mReturnType\u001b[0m\u001b[1m(\u001b[0m\u001b[33mtype\u001b[0m=\u001b[32m'number'\u001b[0m\u001b[1m)\u001b[0m,\n",
       "        \u001b[33mtype\u001b[0m=\u001b[32m'scoring_function'\u001b[0m,\n",
       "        \u001b[33mdescription\u001b[0m=\u001b[32m'For math related benchmarks, extract answer from the generated response and expected_answer \u001b[0m\n",
       "\u001b[32mand see if they match'\u001b[0m,\n",
       "        \u001b[33mparams\u001b[0m=\u001b[1;35mRegexParserScoringFnParams\u001b[0m\u001b[1m(\u001b[0m\n",
       "            \u001b[33maggregation_functions\u001b[0m=\u001b[1m[\u001b[0m\u001b[32m'accuracy'\u001b[0m\u001b[1m]\u001b[0m,\n",
       "            \u001b[33mparsing_regexes\u001b[0m=\u001b[1m[\u001b[0m\u001b[32m'.*final answer is:?\\\\s*\\\\$\\\\\\\\boxed\u001b[0m\u001b[32m{\u001b[0m\u001b[32m(\u001b[0m\u001b[32m?P\u001b[0m\u001b[32m<\u001b[0m\u001b[32mX\u001b[0m\u001b[32m>\u001b[0m\u001b[32m.*\u001b[0m\u001b[32m)\u001b[0m\u001b[32m}\u001b[0m\u001b[32m\\\\$'\u001b[0m\u001b[1m]\u001b[0m,\n",
       "            \u001b[33mtype\u001b[0m=\u001b[32m'regex_parser'\u001b[0m\n",
       "        \u001b[1m)\u001b[0m,\n",
       "        \u001b[33mprovider_resource_id\u001b[0m=\u001b[32m'regex-parser-math-response'\u001b[0m\n",
       "    \u001b[1m)\u001b[0m,\n",
       "    \u001b[1;35mScoringFn\u001b[0m\u001b[1m(\u001b[0m\n",
       "        \u001b[33midentifier\u001b[0m=\u001b[32m'basic::bfcl'\u001b[0m,\n",
       "        \u001b[33mmetadata\u001b[0m=\u001b[1m{\u001b[0m\u001b[1m}\u001b[0m,\n",
       "        \u001b[33mprovider_id\u001b[0m=\u001b[32m'basic'\u001b[0m,\n",
       "        \u001b[33mreturn_type\u001b[0m=\u001b[1;35mReturnType\u001b[0m\u001b[1m(\u001b[0m\u001b[33mtype\u001b[0m=\u001b[32m'number'\u001b[0m\u001b[1m)\u001b[0m,\n",
       "        \u001b[33mtype\u001b[0m=\u001b[32m'scoring_function'\u001b[0m,\n",
       "        \u001b[33mdescription\u001b[0m=\u001b[32m'BFCL complex scoring'\u001b[0m,\n",
       "        \u001b[33mparams\u001b[0m=\u001b[1;35mBasicScoringFnParams\u001b[0m\u001b[1m(\u001b[0m\u001b[33maggregation_functions\u001b[0m=\u001b[1m[\u001b[0m\u001b[32m'accuracy'\u001b[0m\u001b[1m]\u001b[0m, \u001b[33mtype\u001b[0m=\u001b[32m'basic'\u001b[0m\u001b[1m)\u001b[0m,\n",
       "        \u001b[33mprovider_resource_id\u001b[0m=\u001b[32m'bfcl'\u001b[0m\n",
       "    \u001b[1m)\u001b[0m,\n",
       "    \u001b[1;35mScoringFn\u001b[0m\u001b[1m(\u001b[0m\n",
       "        \u001b[33midentifier\u001b[0m=\u001b[32m'basic::ifeval'\u001b[0m,\n",
       "        \u001b[33mmetadata\u001b[0m=\u001b[1m{\u001b[0m\u001b[1m}\u001b[0m,\n",
       "        \u001b[33mprovider_id\u001b[0m=\u001b[32m'basic'\u001b[0m,\n",
       "        \u001b[33mreturn_type\u001b[0m=\u001b[1;35mReturnType\u001b[0m\u001b[1m(\u001b[0m\u001b[33mtype\u001b[0m=\u001b[32m'number'\u001b[0m\u001b[1m)\u001b[0m,\n",
       "        \u001b[33mtype\u001b[0m=\u001b[32m'scoring_function'\u001b[0m,\n",
       "        \u001b[33mdescription\u001b[0m=\u001b[32m'Eval intruction follow capacity by checkping how many instructions can be followed in each \u001b[0m\n",
       "\u001b[32mexample'\u001b[0m,\n",
       "        \u001b[33mparams\u001b[0m=\u001b[1;35mBasicScoringFnParams\u001b[0m\u001b[1m(\u001b[0m\u001b[33maggregation_functions\u001b[0m=\u001b[1m[\u001b[0m\u001b[32m'weighted_average'\u001b[0m\u001b[1m]\u001b[0m, \u001b[33mtype\u001b[0m=\u001b[32m'basic'\u001b[0m\u001b[1m)\u001b[0m,\n",
       "        \u001b[33mprovider_resource_id\u001b[0m=\u001b[32m'ifeval'\u001b[0m\n",
       "    \u001b[1m)\u001b[0m,\n",
       "    \u001b[1;35mScoringFn\u001b[0m\u001b[1m(\u001b[0m\n",
       "        \u001b[33midentifier\u001b[0m=\u001b[32m'basic::docvqa'\u001b[0m,\n",
       "        \u001b[33mmetadata\u001b[0m=\u001b[1m{\u001b[0m\u001b[1m}\u001b[0m,\n",
       "        \u001b[33mprovider_id\u001b[0m=\u001b[32m'basic'\u001b[0m,\n",
       "        \u001b[33mreturn_type\u001b[0m=\u001b[1;35mReturnType\u001b[0m\u001b[1m(\u001b[0m\u001b[33mtype\u001b[0m=\u001b[32m'number'\u001b[0m\u001b[1m)\u001b[0m,\n",
       "        \u001b[33mtype\u001b[0m=\u001b[32m'scoring_function'\u001b[0m,\n",
       "        \u001b[33mdescription\u001b[0m=\u001b[32m'DocVQA Visual Question & Answer scoring function'\u001b[0m,\n",
       "        \u001b[33mparams\u001b[0m=\u001b[1;35mBasicScoringFnParams\u001b[0m\u001b[1m(\u001b[0m\u001b[33maggregation_functions\u001b[0m=\u001b[1m[\u001b[0m\u001b[32m'accuracy'\u001b[0m\u001b[1m]\u001b[0m, \u001b[33mtype\u001b[0m=\u001b[32m'basic'\u001b[0m\u001b[1m)\u001b[0m,\n",
       "        \u001b[33mprovider_resource_id\u001b[0m=\u001b[32m'docvqa'\u001b[0m\n",
       "    \u001b[1m)\u001b[0m,\n",
       "    \u001b[1;35mScoringFn\u001b[0m\u001b[1m(\u001b[0m\n",
       "        \u001b[33midentifier\u001b[0m=\u001b[32m'llm-as-judge::base'\u001b[0m,\n",
       "        \u001b[33mmetadata\u001b[0m=\u001b[1m{\u001b[0m\u001b[1m}\u001b[0m,\n",
       "        \u001b[33mprovider_id\u001b[0m=\u001b[32m'llm-as-judge'\u001b[0m,\n",
       "        \u001b[33mreturn_type\u001b[0m=\u001b[1;35mReturnType\u001b[0m\u001b[1m(\u001b[0m\u001b[33mtype\u001b[0m=\u001b[32m'number'\u001b[0m\u001b[1m)\u001b[0m,\n",
       "        \u001b[33mtype\u001b[0m=\u001b[32m'scoring_function'\u001b[0m,\n",
       "        \u001b[33mdescription\u001b[0m=\u001b[32m'Llm As Judge Scoring Function'\u001b[0m,\n",
       "        \u001b[33mparams\u001b[0m=\u001b[1;35mLlmAsJudgeScoringFnParams\u001b[0m\u001b[1m(\u001b[0m\n",
       "            \u001b[33maggregation_functions\u001b[0m=\u001b[1m[\u001b[0m\u001b[1m]\u001b[0m,\n",
       "            \u001b[33mjudge_model\u001b[0m=\u001b[32m'meta-llama/Llama-3.1-405B-Instruct'\u001b[0m,\n",
       "            \u001b[33mjudge_score_regexes\u001b[0m=\u001b[1m[\u001b[0m\u001b[1m]\u001b[0m,\n",
       "            \u001b[33mtype\u001b[0m=\u001b[32m'llm_as_judge'\u001b[0m,\n",
       "            \u001b[33mprompt_template\u001b[0m=\u001b[32m'Enter custom LLM as Judge Prompt Template'\u001b[0m\n",
       "        \u001b[1m)\u001b[0m,\n",
       "        \u001b[33mprovider_resource_id\u001b[0m=\u001b[32m'llm-as-judge-base'\u001b[0m\n",
       "    \u001b[1m)\u001b[0m,\n",
       "    \u001b[1;35mScoringFn\u001b[0m\u001b[1m(\u001b[0m\n",
       "        \u001b[33midentifier\u001b[0m=\u001b[32m'llm-as-judge::405b-simpleqa'\u001b[0m,\n",
       "        \u001b[33mmetadata\u001b[0m=\u001b[1m{\u001b[0m\u001b[1m}\u001b[0m,\n",
       "        \u001b[33mprovider_id\u001b[0m=\u001b[32m'llm-as-judge'\u001b[0m,\n",
       "        \u001b[33mreturn_type\u001b[0m=\u001b[1;35mReturnType\u001b[0m\u001b[1m(\u001b[0m\u001b[33mtype\u001b[0m=\u001b[32m'number'\u001b[0m\u001b[1m)\u001b[0m,\n",
       "        \u001b[33mtype\u001b[0m=\u001b[32m'scoring_function'\u001b[0m,\n",
       "        \u001b[33mdescription\u001b[0m=\u001b[32m'Llm As Judge Scoring Function for SimpleQA Benchmark \u001b[0m\n",
       "\u001b[32m(\u001b[0m\u001b[32mhttps://github.com/openai/simple-evals/blob/main/simpleqa_eval.py\u001b[0m\u001b[32m)\u001b[0m\u001b[32m'\u001b[0m,\n",
       "        \u001b[33mparams\u001b[0m=\u001b[1;35mLlmAsJudgeScoringFnParams\u001b[0m\u001b[1m(\u001b[0m\n",
       "            \u001b[33maggregation_functions\u001b[0m=\u001b[1m[\u001b[0m\u001b[32m'categorical_count'\u001b[0m\u001b[1m]\u001b[0m,\n",
       "            \u001b[33mjudge_model\u001b[0m=\u001b[32m'meta-llama/Llama-3.1-405B-Instruct'\u001b[0m,\n",
       "            \u001b[33mjudge_score_regexes\u001b[0m=\u001b[1m[\u001b[0m\u001b[32m'\u001b[0m\u001b[32m(\u001b[0m\u001b[32mA|B|C\u001b[0m\u001b[32m)\u001b[0m\u001b[32m'\u001b[0m\u001b[1m]\u001b[0m,\n",
       "            \u001b[33mtype\u001b[0m=\u001b[32m'llm_as_judge'\u001b[0m,\n",
       "            \u001b[33mprompt_template\u001b[0m=\u001b[32m'Your job is to look at a question, a gold target, and a predicted answer, and then \u001b[0m\n",
       "\u001b[32massign a grade of either \u001b[0m\u001b[32m[\u001b[0m\u001b[32m\"CORRECT\", \"INCORRECT\", \"NOT_ATTEMPTED\"\u001b[0m\u001b[32m]\u001b[0m\u001b[32m.\\nFirst, I will give examples of each grade, and\u001b[0m\n",
       "\u001b[32mthen you will grade a new example.\\nThe following are examples of CORRECT predicted answers.\\n```\\nQuestion: What \u001b[0m\n",
       "\u001b[32mare the names of Barack Obama\\'s children?\\nGold target: Malia Obama and Sasha Obama\\nPredicted answer 1: sasha and\u001b[0m\n",
       "\u001b[32mmalia obama\\nPredicted answer 2: most people would say Malia and Sasha, but I\\'m not sure and would have to double \u001b[0m\n",
       "\u001b[32mcheck\\nPredicted answer 3: Barack Obama has two daughters. Their names are Malia Ann and Natasha Marian, but they \u001b[0m\n",
       "\u001b[32mare commonly referred to as Malia Obama and Sasha Obama. Malia was born on July 4, 1998, and Sasha was born on June\u001b[0m\n",
       "\u001b[32m10, 2001.\\n```\\nThese predicted answers are all CORRECT because:\\n    - They fully contain the important \u001b[0m\n",
       "\u001b[32minformation in the gold target.\\n    - They do not contain any information that contradicts the gold target.\\n    -\u001b[0m\n",
       "\u001b[32mOnly semantic meaning matters; capitalization, punctuation, grammar, and order don\\'t matter.\\n    - Hedging and \u001b[0m\n",
       "\u001b[32mguessing are permissible, provided that the gold target is fully included and the response contains no incorrect \u001b[0m\n",
       "\u001b[32minformation or contradictions.\\nThe following are examples of INCORRECT predicted answers.\\n```\\nQuestion: What are\u001b[0m\n",
       "\u001b[32mthe names of Barack Obama\\'s children?\\nGold target: Malia and Sasha\\nPredicted answer 1: Malia.\\nPredicted answer \u001b[0m\n",
       "\u001b[32m2: Malia, Sasha, and Susan.\\nPredicted answer 3: Barack Obama does not have any children.\\nPredicted answer 4: I \u001b[0m\n",
       "\u001b[32mthink it\\'s either Malia and Sasha. Or it could be Malia and Jackie. Or it could be Joey and Malia.\\nPredicted \u001b[0m\n",
       "\u001b[32manswer 4: While I don\\'t know their exact names, I can tell you that Barack Obama has three children.\\nPredicted \u001b[0m\n",
       "\u001b[32manswer 5: It\\'s possible you may mean Betsy and Olivia. However, you should clarify further details with updated \u001b[0m\n",
       "\u001b[32mreferences if necessary. Is that the correct answer?\\nPredicted answer 6: It may be the case that Obama\\'s child is\u001b[0m\n",
       "\u001b[32mnamed James. However, it\\'s recommended to confirm the most accurate and updated information since this could \u001b[0m\n",
       "\u001b[32mchange over time. This model may not always reflect the most current information.\\n```\\nThese predicted answers are\u001b[0m\n",
       "\u001b[32mall INCORRECT because:\\n    - A factual statement in the answer contradicts the gold target. Incorrect statements \u001b[0m\n",
       "\u001b[32mthat have some hedging \u001b[0m\u001b[32m(\u001b[0m\u001b[32me.g., \"it is possible that\", \"although i\\'m not sure, i think\"\u001b[0m\u001b[32m)\u001b[0m\u001b[32m are also considered \u001b[0m\n",
       "\u001b[32mincorrect.\\nThe following are examples of NOT_ATTEMPTED predicted answers.\\n```\\nQuestion: What are the names of \u001b[0m\n",
       "\u001b[32mBarack Obama\\'s children?\\nGold target: Malia and Sasha\\nPredicted answer 1: I don\\'t know.\\nPredicted answer 2: I \u001b[0m\n",
       "\u001b[32mneed more context about which Obama you are talking about.\\nPredicted answer 3: Without researching the web, I \u001b[0m\n",
       "\u001b[32mcannot answer this question. However, I can tell you that Barack Obama has two children.\\nPredicted answer 4: \u001b[0m\n",
       "\u001b[32mBarack Obama has two children. I know that one of them is Malia, but I\\'m not sure about the other one.\\n```\\nThese\u001b[0m\n",
       "\u001b[32mpredicted answers are all NOT_ATTEMPTED because:\\n    - The important information in the gold target is not \u001b[0m\n",
       "\u001b[32mincluded in the answer.\\n    - No statements in the answer contradict the gold target.\\nAlso note the following \u001b[0m\n",
       "\u001b[32mthings:\\n- For grading questions where the gold target is a number, the predicted answer needs to be correct to the\u001b[0m\n",
       "\u001b[32mlast significant figure in the gold answer. For example, consider a question \"How many citations does the \u001b[0m\n",
       "\u001b[32mTransformer Paper have?\" with gold target \"120k\".\\n    - Predicted answers \"120k\", \"124k\", and 115k\" are all \u001b[0m\n",
       "\u001b[32mCORRECT.\\n    - Predicted answers \"100k\" and \"113k\" are INCORRECT.\\n    - Predicted answers \"around 100k\" and \"more\u001b[0m\n",
       "\u001b[32mthan 50k\" are considered NOT_ATTEMPTED because they neither confirm nor contradict the gold target.\\n- The gold \u001b[0m\n",
       "\u001b[32mtarget may contain more information than the question. In such cases, the predicted answer only needs to contain \u001b[0m\n",
       "\u001b[32mthe information that is in the question.\\n    - For example, consider the question \"What episode did Derek and \u001b[0m\n",
       "\u001b[32mMeredith get legally married in Grey\\'s Anatomy?\" with gold target \"Season 7, Episode 20: White Wedding\". Either \u001b[0m\n",
       "\u001b[32m\"Season 7, Episode 20\" or \"White Wedding\" would be considered a CORRECT answer.\\n- Do not punish predicted answers \u001b[0m\n",
       "\u001b[32mif they omit information that would be clearly inferred from the question.\\n    - For example, consider the \u001b[0m\n",
       "\u001b[32mquestion \"What city is OpenAI headquartered in?\" and the gold target \"San Francisco, California\". The predicted \u001b[0m\n",
       "\u001b[32manswer \"San Francisco\" would be considered CORRECT, even though it does not include \"California\".\\n    - Consider \u001b[0m\n",
       "\u001b[32mthe question \"What award did A pretrainer\\'s guide to training data: Measuring the effects of data age, domain \u001b[0m\n",
       "\u001b[32mcoverage, quality, & toxicity win at NAACL \\'24?\", the gold target is \"Outstanding Paper Award\". The predicted \u001b[0m\n",
       "\u001b[32manswer \"Outstanding Paper\" would be considered CORRECT, because \"award\" is presumed in the question.\\n    - For the\u001b[0m\n",
       "\u001b[32mquestion \"What is the height of Jason Wei in meters?\", the gold target is \"1.73 m\". The predicted answer \"1.75\" \u001b[0m\n",
       "\u001b[32mwould be considered CORRECT, because meters is specified in the question.\\n    - For the question \"What is the name\u001b[0m\n",
       "\u001b[32mof Barack Obama\\'s wife?\", the gold target is \"Michelle Obama\". The predicted answer \"Michelle\" would be considered\u001b[0m\n",
       "\u001b[32mCORRECT, because the last name can be presumed.\\n- Do not punish for typos in people\\'s name if it\\'s clearly the \u001b[0m\n",
       "\u001b[32msame name.\\n    - For example, if the gold target is \"Hyung Won Chung\", you can consider the following predicted \u001b[0m\n",
       "\u001b[32manswers as correct: \"Hyoong Won Choong\", \"Hyungwon Chung\", or \"Hyun Won Chung\".\\nHere is a new example. Simply \u001b[0m\n",
       "\u001b[32mreply with either CORRECT, INCORRECT, NOT ATTEMPTED. Don\\'t apologize or correct yourself if there was a mistake; \u001b[0m\n",
       "\u001b[32mwe are just trying to grade the answer.\\n```\\nQuestion: \u001b[0m\u001b[32m{\u001b[0m\u001b[32minput_query\u001b[0m\u001b[32m}\u001b[0m\u001b[32m\\nGold target: \u001b[0m\u001b[32m{\u001b[0m\u001b[32mexpected_answer\u001b[0m\u001b[32m}\u001b[0m\u001b[32m\\nPredicted \u001b[0m\n",
       "\u001b[32manswer: \u001b[0m\u001b[32m{\u001b[0m\u001b[32mgenerated_answer\u001b[0m\u001b[32m}\u001b[0m\u001b[32m\\n```\\nGrade the predicted answer of this new question as one of:\\nA: CORRECT\\nB: \u001b[0m\n",
       "\u001b[32mINCORRECT\\nC: NOT_ATTEMPTED\\nJust return the letters \"A\", \"B\", or \"C\", with no text around it.'\u001b[0m\n",
       "        \u001b[1m)\u001b[0m,\n",
       "        \u001b[33mprovider_resource_id\u001b[0m=\u001b[32m'llm-as-judge-405b-simpleqa'\u001b[0m\n",
       "    \u001b[1m)\u001b[0m\n",
       "\u001b[1m]\u001b[0m\n"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Pretty(client.scoring_functions.list())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c94a9af7",
   "metadata": {},
   "source": [
    "## Subset Of"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc8b469c-2242-4e44-ab20-ad46b220c828",
   "metadata": {},
   "source": [
    "Let's start by setting the `scoring_params` to use the `subset_of` function mentioned earlier and see what it comes back with.  \n",
    "Here we create some `handmade_eval_rows` with the input and the expected answer, but we also add the generated answer already filled out.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "200fe9e1-b9ea-4b6d-b991-117531c870f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://llama-stack.user1-test.svc.cluster.local/v1/scoring/score \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">ScoringScoreResponse</span><span style=\"font-weight: bold\">(</span>\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">results</span>=<span style=\"font-weight: bold\">{</span>\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'basic::subset_of'</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">ScoringResult</span><span style=\"font-weight: bold\">(</span>\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">aggregated_results</span>=<span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'accuracy'</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'accuracy'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.5</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'num_correct'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1.0</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'num_total'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"font-weight: bold\">}}</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">score_rows</span>=<span style=\"font-weight: bold\">[{</span><span style=\"color: #008000; text-decoration-color: #008000\">'score'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1.0</span><span style=\"font-weight: bold\">}</span>, <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'score'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.0</span><span style=\"font-weight: bold\">}]</span>\n",
       "        <span style=\"font-weight: bold\">)</span>\n",
       "    <span style=\"font-weight: bold\">}</span>\n",
       "<span style=\"font-weight: bold\">)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;35mScoringScoreResponse\u001b[0m\u001b[1m(\u001b[0m\n",
       "    \u001b[33mresults\u001b[0m=\u001b[1m{\u001b[0m\n",
       "        \u001b[32m'basic::subset_of'\u001b[0m: \u001b[1;35mScoringResult\u001b[0m\u001b[1m(\u001b[0m\n",
       "            \u001b[33maggregated_results\u001b[0m=\u001b[1m{\u001b[0m\u001b[32m'accuracy'\u001b[0m: \u001b[1m{\u001b[0m\u001b[32m'accuracy'\u001b[0m: \u001b[1;36m0.5\u001b[0m, \u001b[32m'num_correct'\u001b[0m: \u001b[1;36m1.0\u001b[0m, \u001b[32m'num_total'\u001b[0m: \u001b[1;36m2\u001b[0m\u001b[1m}\u001b[0m\u001b[1m}\u001b[0m,\n",
       "            \u001b[33mscore_rows\u001b[0m=\u001b[1m[\u001b[0m\u001b[1m{\u001b[0m\u001b[32m'score'\u001b[0m: \u001b[1;36m1.0\u001b[0m\u001b[1m}\u001b[0m, \u001b[1m{\u001b[0m\u001b[32m'score'\u001b[0m: \u001b[1;36m0.0\u001b[0m\u001b[1m}\u001b[0m\u001b[1m]\u001b[0m\n",
       "        \u001b[1m)\u001b[0m\n",
       "    \u001b[1m}\u001b[0m\n",
       "\u001b[1m)\u001b[0m\n"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "handmade_eval_rows = [\n",
    "    {\n",
    "        \"input_query\": \"What is your favorite food?\",\n",
    "        \"generated_answer\": \"Tapas are my favorites.\",\n",
    "        \"expected_answer\": \"Tapas\",\n",
    "    },\n",
    "    {\n",
    "        \"input_query\": \"What is your favorite food?\",\n",
    "        \"generated_answer\": \"I really like tapas.\",\n",
    "        \"expected_answer\": \"Tapas\",\n",
    "    }\n",
    "]\n",
    "Pretty(handmade_eval_rows)\n",
    "\n",
    "scoring_params = {\n",
    "    \"basic::subset_of\": None,\n",
    "}\n",
    "scoring_response = client.scoring.score(\n",
    "    input_rows=handmade_eval_rows, scoring_functions=scoring_params\n",
    ")\n",
    "Pretty(scoring_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "174c2599",
   "metadata": {},
   "source": [
    "Hmm, we got half of the answers correct ü§î  \n",
    "This is because we expect tapas to be spelled with a big T in front. As mentioned before, the subset_of function expects exact matches within the generated answer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d41a6d6f",
   "metadata": {},
   "source": [
    "## Start Simple"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2901f566",
   "metadata": {},
   "source": [
    "Not only is it much faster to start with simple functional or integration tests, but it also allows us to keep the cost down as we don't evaluate the text in the response from the LLM as much as just evaluate some parameters around it or the effect it has.  \n",
    "This can test things such as: \n",
    "- **Response metadata**: latency, token count, or response length - we will see more of this in a little bit.\n",
    "- **Structural checks**: ensuring the output is valid JSON, fits a schema, or contains required fields.\n",
    "- **Behavioral signals**: whether the model calls the right API, triggers the expected downstream function, or returns a specific keyword."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e14c7e3",
   "metadata": {},
   "source": [
    "Llama Stack eval is a bit limited in what evaluations it currently provides, but we can create custom ones that help us evaluate these smaller, simpler tests.  \n",
    "We have a summary usecase, so two that would fit ours is simply:  \n",
    "- Is the response shorter than the input?\n",
    "- Does the response get parsed in the app?\n",
    "\n",
    "We'll cover the first one here just to showcase how this could look like."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fef91554",
   "metadata": {},
   "source": [
    "https://llama-stack.readthedocs.io/en/latest/references/api_reference/index.html#/paths/v1-scoring-functions/post"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "000216e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Custom eval function that uses Llama Stack helper functions\n",
    "\"\"\"\n",
    "from llama_stack_client.types import ScoringScoreResponse\n",
    "from llama_stack.apis.scoring import ScoringResult\n",
    "\n",
    "def is_shorter(input, response):\n",
    "    return {\"score\": int(response < input)}\n",
    "\n",
    "def eval_is_shorter(input_rows: list[dict]):\n",
    "    score_rows = []\n",
    "    aggregated_results = {}\n",
    "    for row in input_rows:\n",
    "        score_rows.append(\n",
    "            is_shorter(\n",
    "                input=row[\"input_query\"], \n",
    "                response=row[\"generated_answer\"]\n",
    "                )\n",
    "            )\n",
    "\n",
    "    num_total = len(score_rows)\n",
    "    num_correct = sum(row[\"score\"] for row in score_rows)\n",
    "    accuracy = (num_correct / num_total) if num_total else 0.0\n",
    "\n",
    "    aggregated_results = {\n",
    "        \"accuracy\": {\n",
    "            \"accuracy\": accuracy,\n",
    "            \"num_correct\": float(num_correct),\n",
    "            \"num_total\": num_total,\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    results = {\n",
    "        \"is_shorter::custom\": ScoringResult(score_rows=score_rows, aggregated_results=aggregated_results).model_dump()\n",
    "    }\n",
    "    return ScoringScoreResponse(results=results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9b8106c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">ScoringScoreResponse</span><span style=\"font-weight: bold\">(</span>\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">results</span>=<span style=\"font-weight: bold\">{</span>\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'is_shorter::custom'</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">ScoringResult</span><span style=\"font-weight: bold\">(</span>\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">aggregated_results</span>=<span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'accuracy'</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'accuracy'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1.0</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'num_correct'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2.0</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'num_total'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"font-weight: bold\">}}</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">score_rows</span>=<span style=\"font-weight: bold\">[{</span><span style=\"color: #008000; text-decoration-color: #008000\">'score'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1.0</span><span style=\"font-weight: bold\">}</span>, <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'score'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1.0</span><span style=\"font-weight: bold\">}]</span>\n",
       "        <span style=\"font-weight: bold\">)</span>\n",
       "    <span style=\"font-weight: bold\">}</span>\n",
       "<span style=\"font-weight: bold\">)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;35mScoringScoreResponse\u001b[0m\u001b[1m(\u001b[0m\n",
       "    \u001b[33mresults\u001b[0m=\u001b[1m{\u001b[0m\n",
       "        \u001b[32m'is_shorter::custom'\u001b[0m: \u001b[1;35mScoringResult\u001b[0m\u001b[1m(\u001b[0m\n",
       "            \u001b[33maggregated_results\u001b[0m=\u001b[1m{\u001b[0m\u001b[32m'accuracy'\u001b[0m: \u001b[1m{\u001b[0m\u001b[32m'accuracy'\u001b[0m: \u001b[1;36m1.0\u001b[0m, \u001b[32m'num_correct'\u001b[0m: \u001b[1;36m2.0\u001b[0m, \u001b[32m'num_total'\u001b[0m: \u001b[1;36m2\u001b[0m\u001b[1m}\u001b[0m\u001b[1m}\u001b[0m,\n",
       "            \u001b[33mscore_rows\u001b[0m=\u001b[1m[\u001b[0m\u001b[1m{\u001b[0m\u001b[32m'score'\u001b[0m: \u001b[1;36m1.0\u001b[0m\u001b[1m}\u001b[0m, \u001b[1m{\u001b[0m\u001b[32m'score'\u001b[0m: \u001b[1;36m1.0\u001b[0m\u001b[1m}\u001b[0m\u001b[1m]\u001b[0m\n",
       "        \u001b[1m)\u001b[0m\n",
       "    \u001b[1m}\u001b[0m\n",
       "\u001b[1m)\u001b[0m\n"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scoring_response = eval_is_shorter(\n",
    "    input_rows = handmade_eval_rows\n",
    ")\n",
    "Pretty(scoring_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "412e97f3-90b2-47c3-b2ef-49865cb6742b",
   "metadata": {},
   "source": [
    "## LLM as judgeüßë‚Äç‚öñÔ∏è"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c25f3a1e",
   "metadata": {},
   "source": [
    "Now let's try the same thing but with LLM as judge.   \n",
    "Here we are going to feed our eval_rows into an LLM to evaluate how well the generated answer matches the expected answer.  \n",
    "To make sure that the judge LLM does this, we also provide it a `JUDGE_PROMPT` it should follow, as well as a regex of expected scores (`judge_score_regexes`) from the judge.  \n",
    "In our case we let it grade the generated answers from A to E (no F's in this class üôÇ‚Äç‚ÜîÔ∏è), each with its own meaning that you can see in the judge prompt.  \n",
    "We also choose Llama 3.2 to be our judge (as we all know llamas to be the best of judges). This means that when we later later evaluate replies from the backend, we will use the same LLM to generate our answer and judge them, essentially doing a self-judge strategy. This is not always the best, but works pretty well with Llama 3.2 and we don't have any other model to use right now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d830cced-784a-49e5-a780-fbffee2ed03d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">ScoringScoreResponse</span><span style=\"font-weight: bold\">(</span>\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">results</span>=<span style=\"font-weight: bold\">{</span>\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'llm-as-judge::base'</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">ScoringResult</span><span style=\"font-weight: bold\">(</span>\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">aggregated_results</span>=<span style=\"font-weight: bold\">{}</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">score_rows</span>=<span style=\"font-weight: bold\">[</span>\n",
       "                <span style=\"font-weight: bold\">{</span>\n",
       "                    <span style=\"color: #008000; text-decoration-color: #008000\">'score'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'C'</span>,\n",
       "                    <span style=\"color: #008000; text-decoration-color: #008000\">'judge_feedback'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'Answer: C, Explanation: The GENERATED_RESPONSE contains all the same details</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">as the EXPECTED_RESPONSE.'</span>\n",
       "                <span style=\"font-weight: bold\">}</span>,\n",
       "                <span style=\"font-weight: bold\">{</span>\n",
       "                    <span style=\"color: #008000; text-decoration-color: #008000\">'score'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'C'</span>,\n",
       "                    <span style=\"color: #008000; text-decoration-color: #008000\">'judge_feedback'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'Answer: C, Explanation: The GENERATED_RESPONSE contains all the same details</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">as the EXPECTED_RESPONSE.'</span>\n",
       "                <span style=\"font-weight: bold\">}</span>\n",
       "            <span style=\"font-weight: bold\">]</span>\n",
       "        <span style=\"font-weight: bold\">)</span>\n",
       "    <span style=\"font-weight: bold\">}</span>\n",
       "<span style=\"font-weight: bold\">)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;35mScoringScoreResponse\u001b[0m\u001b[1m(\u001b[0m\n",
       "    \u001b[33mresults\u001b[0m=\u001b[1m{\u001b[0m\n",
       "        \u001b[32m'llm-as-judge::base'\u001b[0m: \u001b[1;35mScoringResult\u001b[0m\u001b[1m(\u001b[0m\n",
       "            \u001b[33maggregated_results\u001b[0m=\u001b[1m{\u001b[0m\u001b[1m}\u001b[0m,\n",
       "            \u001b[33mscore_rows\u001b[0m=\u001b[1m[\u001b[0m\n",
       "                \u001b[1m{\u001b[0m\n",
       "                    \u001b[32m'score'\u001b[0m: \u001b[32m'C'\u001b[0m,\n",
       "                    \u001b[32m'judge_feedback'\u001b[0m: \u001b[32m'Answer: C, Explanation: The GENERATED_RESPONSE contains all the same details\u001b[0m\n",
       "\u001b[32mas the EXPECTED_RESPONSE.'\u001b[0m\n",
       "                \u001b[1m}\u001b[0m,\n",
       "                \u001b[1m{\u001b[0m\n",
       "                    \u001b[32m'score'\u001b[0m: \u001b[32m'C'\u001b[0m,\n",
       "                    \u001b[32m'judge_feedback'\u001b[0m: \u001b[32m'Answer: C, Explanation: The GENERATED_RESPONSE contains all the same details\u001b[0m\n",
       "\u001b[32mas the EXPECTED_RESPONSE.'\u001b[0m\n",
       "                \u001b[1m}\u001b[0m\n",
       "            \u001b[1m]\u001b[0m\n",
       "        \u001b[1m)\u001b[0m\n",
       "    \u001b[1m}\u001b[0m\n",
       "\u001b[1m)\u001b[0m\n"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "handmade_eval_rows = [\n",
    "    {\n",
    "        \"input_query\": \"What is your favorite food?\",\n",
    "        \"generated_answer\": \"Tapas are my favorites.\",\n",
    "        \"expected_answer\": \"Tapas\",\n",
    "    },\n",
    "    {\n",
    "        \"input_query\": \"What is your favorite food?\",\n",
    "        \"generated_answer\": \"I really like tapas.\",\n",
    "        \"expected_answer\": \"Tapas\",\n",
    "    }\n",
    "]\n",
    "Pretty(handmade_eval_rows)\n",
    "\n",
    "judge_model_id = \"llama32\"\n",
    "JUDGE_PROMPT = \"\"\"\n",
    "Given a QUESTION and GENERATED_RESPONSE and EXPECTED_RESPONSE.\n",
    "\n",
    "Compare the factual content of the GENERATED_RESPONSE with the EXPECTED_RESPONSE. Ignore any differences in style, grammar, or punctuation.\n",
    "  The GENERATED_RESPONSE may either be a subset or superset of the EXPECTED_RESPONSE, or it may conflict with it. Determine which case applies. Answer the question by selecting one of the following options:\n",
    "  (A) The GENERATED_RESPONSE is a subset of the EXPECTED_RESPONSE and is fully consistent with it.\n",
    "  (B) The GENERATED_RESPONSE is a superset of the EXPECTED_RESPONSE and is fully consistent with it.\n",
    "  (C) The GENERATED_RESPONSE contains all the same details as the EXPECTED_RESPONSE.\n",
    "  (D) There is a disagreement between the GENERATED_RESPONSE and the EXPECTED_RESPONSE.\n",
    "  (E) The answers differ, but these differences don't matter from the perspective of factuality.\n",
    "\n",
    "Give your answer in the format \"Answer: One of ABCDE, Explanation: \".\n",
    "\n",
    "Your actual task:\n",
    "\n",
    "QUESTION: {input_query}\n",
    "GENERATED_RESPONSE: {generated_answer}\n",
    "EXPECTED_RESPONSE: {expected_answer}\n",
    "\"\"\"\n",
    "\n",
    "scoring_params = {\n",
    "    \"llm-as-judge::base\": {\n",
    "        \"judge_model\": judge_model_id,\n",
    "        \"prompt_template\": JUDGE_PROMPT,\n",
    "        \"type\": \"llm_as_judge\",\n",
    "        \"judge_score_regexes\": [\"Answer: (A|B|C|D|E)\"],\n",
    "    },\n",
    "}\n",
    "\n",
    "scoring_response = client.scoring.score(\n",
    "    input_rows=handmade_eval_rows, scoring_functions=scoring_params\n",
    ")\n",
    "Pretty(scoring_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "360d4cbd",
   "metadata": {},
   "source": [
    "You should have gotten at least a C from the judge, and you can see the reasoning for it in the `judge_feedback` field.  \n",
    "Feel free to try out some other inputs, generated answers, and expected answers üß™"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64c65b29",
   "metadata": {},
   "source": [
    "# Involve the LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e874d58c",
   "metadata": {},
   "source": [
    "So far we have just hardcoded the generated answers but these should be generated from an LLM, otherwise we are just evaluating our human selves.  \n",
    "To do this, let's send some requests to our LLM through llamastack, and then also to our backend and see how that looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "da1a3396",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"llama32\"\n",
    "\n",
    "model_eval_rows = [\n",
    "    {\n",
    "        \"input_query\": \"What is your favorite Spanish food?\",\n",
    "        \"expected_answer\": \"Tapas\",\n",
    "    },\n",
    "    {\n",
    "        \"input_query\": \"What is your favorite Turkish food?\",\n",
    "        \"expected_answer\": \"Baklava\",\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27abe1b1",
   "metadata": {},
   "source": [
    "Note how this time we don't have any generated answers yet, those will come from the LLM directly this time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1277bcd8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">[</span>\n",
       "    <span style=\"font-weight: bold\">{</span>\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'input_query'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'What is your favorite Spanish food?'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'expected_answer'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'Tapas'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'generated_answer'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">\"I don't have personal preferences or taste buds, but I can suggest some popular and </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">delicious Spanish dishes that you might enjoy:\\n\\n1. Paella: A classic Spanish dish made with saffron-infused rice,</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">vegetables, and your choice of protein (chicken, seafood, or chorizo).\\n2. Tapas: Small, shareable plates of food, </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">such as patatas bravas (spicy fried potatoes), tortilla espa√±ola (Spanish omelette), and croquetas (deep-fried </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">balls filled with ham, fish, or chicken).\\n3. Gazpacho: A refreshing cold soup made from tomatoes, peppers, </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">cucumbers, and bread, originating from the Andalusia region.\\n4. Jam√≥n ib√©rico: A cured ham from the Iberian </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Peninsula, known for its rich flavor and velvety texture.\\n5. Churros con chocolate: Fried dough sticks coated in </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">sugar, served with a rich and creamy chocolate dipping sauce.\\n6. Empanada gallega: A savory pastry filled with </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">seafood, meat, or vegetables, originating from the Galicia region.\\n7. Tortilla de patatas: A thick, round omelette</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">made with potatoes, onions, and sometimes ham or chorizo.\\n\\nThese are just a few examples of the many delicious </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Spanish dishes out there. What's your favorite Spanish food?\"</span>\n",
       "    <span style=\"font-weight: bold\">}</span>,\n",
       "    <span style=\"font-weight: bold\">{</span>\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'input_query'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'What is your favorite Turkish food?'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'expected_answer'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'Baklava'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'generated_answer'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">\"I'm just a language model, I don't have personal preferences or taste buds, but I can </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">tell you about some popular Turkish dishes that you might enjoy!\\n\\nSome of the most well-known and beloved Turkish</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">foods include:\\n\\n1. Doner Kebab: A classic Turkish dish made from layers of lamb or beef stacked on a vertical </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">spit and served in a crispy sesame-topped bun with salad, vegetables, and sauce.\\n2. Lahmacun (Turkish Pizza): A </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">thin crust topped with minced meat, onions, and spices, often served with lemon juice and herbs.\\n3. Borek: Flaky </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">pastry filled with cheese, spinach, or minced meat, often served as a snack or appetizer.\\n4. Manti (Turkish </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Ravioli): Steamed dumplings filled with meat and spices, served with yogurt and garlic sauce.\\n5. Kofte: Meatballs </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">made from ground meat (usually beef or lamb) and spices, often served with rice and vegetables.\\n6. Sis Kebab: </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Skewers of marinated meat (usually lamb or beef) grilled over an open flame.\\n7. Baklava: A sweet pastry made from </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">layers of phyllo dough, nuts, and honey, often served as a dessert.\\n\\nThese are just a few examples of the </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">delicious foods you can find in Turkey. Each region has its own specialties, so there's always something new to </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">try!\\n\\nWhich one of these dishes sounds appealing to you?\"</span>\n",
       "    <span style=\"font-weight: bold\">}</span>\n",
       "<span style=\"font-weight: bold\">]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m[\u001b[0m\n",
       "    \u001b[1m{\u001b[0m\n",
       "        \u001b[32m'input_query'\u001b[0m: \u001b[32m'What is your favorite Spanish food?'\u001b[0m,\n",
       "        \u001b[32m'expected_answer'\u001b[0m: \u001b[32m'Tapas'\u001b[0m,\n",
       "        \u001b[32m'generated_answer'\u001b[0m: \u001b[32m\"I don't have personal preferences or taste buds, but I can suggest some popular and \u001b[0m\n",
       "\u001b[32mdelicious Spanish dishes that you might enjoy:\\n\\n1. Paella: A classic Spanish dish made with saffron-infused rice,\u001b[0m\n",
       "\u001b[32mvegetables, and your choice of protein \u001b[0m\u001b[32m(\u001b[0m\u001b[32mchicken, seafood, or chorizo\u001b[0m\u001b[32m)\u001b[0m\u001b[32m.\\n2. Tapas: Small, shareable plates of food, \u001b[0m\n",
       "\u001b[32msuch as patatas bravas \u001b[0m\u001b[32m(\u001b[0m\u001b[32mspicy fried potatoes\u001b[0m\u001b[32m)\u001b[0m\u001b[32m, tortilla espa√±ola \u001b[0m\u001b[32m(\u001b[0m\u001b[32mSpanish omelette\u001b[0m\u001b[32m)\u001b[0m\u001b[32m, and croquetas \u001b[0m\u001b[32m(\u001b[0m\u001b[32mdeep-fried \u001b[0m\n",
       "\u001b[32mballs filled with ham, fish, or chicken\u001b[0m\u001b[32m)\u001b[0m\u001b[32m.\\n3. Gazpacho: A refreshing cold soup made from tomatoes, peppers, \u001b[0m\n",
       "\u001b[32mcucumbers, and bread, originating from the Andalusia region.\\n4. Jam√≥n ib√©rico: A cured ham from the Iberian \u001b[0m\n",
       "\u001b[32mPeninsula, known for its rich flavor and velvety texture.\\n5. Churros con chocolate: Fried dough sticks coated in \u001b[0m\n",
       "\u001b[32msugar, served with a rich and creamy chocolate dipping sauce.\\n6. Empanada gallega: A savory pastry filled with \u001b[0m\n",
       "\u001b[32mseafood, meat, or vegetables, originating from the Galicia region.\\n7. Tortilla de patatas: A thick, round omelette\u001b[0m\n",
       "\u001b[32mmade with potatoes, onions, and sometimes ham or chorizo.\\n\\nThese are just a few examples of the many delicious \u001b[0m\n",
       "\u001b[32mSpanish dishes out there. What's your favorite Spanish food?\"\u001b[0m\n",
       "    \u001b[1m}\u001b[0m,\n",
       "    \u001b[1m{\u001b[0m\n",
       "        \u001b[32m'input_query'\u001b[0m: \u001b[32m'What is your favorite Turkish food?'\u001b[0m,\n",
       "        \u001b[32m'expected_answer'\u001b[0m: \u001b[32m'Baklava'\u001b[0m,\n",
       "        \u001b[32m'generated_answer'\u001b[0m: \u001b[32m\"I'm just a language model, I don't have personal preferences or taste buds, but I can \u001b[0m\n",
       "\u001b[32mtell you about some popular Turkish dishes that you might enjoy!\\n\\nSome of the most well-known and beloved Turkish\u001b[0m\n",
       "\u001b[32mfoods include:\\n\\n1. Doner Kebab: A classic Turkish dish made from layers of lamb or beef stacked on a vertical \u001b[0m\n",
       "\u001b[32mspit and served in a crispy sesame-topped bun with salad, vegetables, and sauce.\\n2. Lahmacun \u001b[0m\u001b[32m(\u001b[0m\u001b[32mTurkish Pizza\u001b[0m\u001b[32m)\u001b[0m\u001b[32m: A \u001b[0m\n",
       "\u001b[32mthin crust topped with minced meat, onions, and spices, often served with lemon juice and herbs.\\n3. Borek: Flaky \u001b[0m\n",
       "\u001b[32mpastry filled with cheese, spinach, or minced meat, often served as a snack or appetizer.\\n4. Manti \u001b[0m\u001b[32m(\u001b[0m\u001b[32mTurkish \u001b[0m\n",
       "\u001b[32mRavioli\u001b[0m\u001b[32m)\u001b[0m\u001b[32m: Steamed dumplings filled with meat and spices, served with yogurt and garlic sauce.\\n5. Kofte: Meatballs \u001b[0m\n",
       "\u001b[32mmade from ground meat \u001b[0m\u001b[32m(\u001b[0m\u001b[32musually beef or lamb\u001b[0m\u001b[32m)\u001b[0m\u001b[32m and spices, often served with rice and vegetables.\\n6. Sis Kebab: \u001b[0m\n",
       "\u001b[32mSkewers of marinated meat \u001b[0m\u001b[32m(\u001b[0m\u001b[32musually lamb or beef\u001b[0m\u001b[32m)\u001b[0m\u001b[32m grilled over an open flame.\\n7. Baklava: A sweet pastry made from \u001b[0m\n",
       "\u001b[32mlayers of phyllo dough, nuts, and honey, often served as a dessert.\\n\\nThese are just a few examples of the \u001b[0m\n",
       "\u001b[32mdelicious foods you can find in Turkey. Each region has its own specialties, so there's always something new to \u001b[0m\n",
       "\u001b[32mtry!\\n\\nWhich one of these dishes sounds appealing to you?\"\u001b[0m\n",
       "    \u001b[1m}\u001b[0m\n",
       "\u001b[1m]\u001b[0m\n"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for eval_row in model_eval_rows:\n",
    "    response = client.inference.chat_completion(\n",
    "        model_id=model_id,\n",
    "        messages=[\n",
    "            {\"role\": \"user\", \"content\": eval_row[\"input_query\"]}\n",
    "        ]\n",
    "    )\n",
    "    eval_row[\"generated_answer\"] = response.completion_message.content\n",
    "\n",
    "Pretty(model_eval_rows)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea77dc63",
   "metadata": {},
   "source": [
    "And now we can just evaluate these answers just like we did before ‚úÖ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "797c2692",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">ScoringScoreResponse</span><span style=\"font-weight: bold\">(</span>\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">results</span>=<span style=\"font-weight: bold\">{</span>\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'llm-as-judge::base'</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">ScoringResult</span><span style=\"font-weight: bold\">(</span>\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">aggregated_results</span>=<span style=\"font-weight: bold\">{}</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">score_rows</span>=<span style=\"font-weight: bold\">[</span>\n",
       "                <span style=\"font-weight: bold\">{</span>\n",
       "                    <span style=\"color: #008000; text-decoration-color: #008000\">'score'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'E'</span>,\n",
       "                    <span style=\"color: #008000; text-decoration-color: #008000\">'judge_feedback'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'Answer: E, Explanation: The GENERATED RESPONSE contains a list of popular </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Spanish dishes, including Tapas, but it does not express a personal preference or opinion, which is why the </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">differences in style and the additional information do not affect the factual content.'</span>\n",
       "                <span style=\"font-weight: bold\">}</span>,\n",
       "                <span style=\"font-weight: bold\">{</span>\n",
       "                    <span style=\"color: #008000; text-decoration-color: #008000\">'score'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'E'</span>,\n",
       "                    <span style=\"color: #008000; text-decoration-color: #008000\">'judge_feedback'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'Answer: E, Explanation: The GENERATED RESPONSE contains a list of popular </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Turkish dishes, including Baklava, but it does not directly answer the question \"What is your favorite Turkish </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">food?\" as it is phrased. The GENERATED RESPONSE is focused on providing information about Turkish cuisine, while </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">the EXPECTED RESPONSE is a personal preference. The differences in the question and response do not affect the </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">factual content, as the GENERATED RESPONSE provides accurate information about Turkish dishes, including Baklava.'</span>\n",
       "                <span style=\"font-weight: bold\">}</span>\n",
       "            <span style=\"font-weight: bold\">]</span>\n",
       "        <span style=\"font-weight: bold\">)</span>\n",
       "    <span style=\"font-weight: bold\">}</span>\n",
       "<span style=\"font-weight: bold\">)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;35mScoringScoreResponse\u001b[0m\u001b[1m(\u001b[0m\n",
       "    \u001b[33mresults\u001b[0m=\u001b[1m{\u001b[0m\n",
       "        \u001b[32m'llm-as-judge::base'\u001b[0m: \u001b[1;35mScoringResult\u001b[0m\u001b[1m(\u001b[0m\n",
       "            \u001b[33maggregated_results\u001b[0m=\u001b[1m{\u001b[0m\u001b[1m}\u001b[0m,\n",
       "            \u001b[33mscore_rows\u001b[0m=\u001b[1m[\u001b[0m\n",
       "                \u001b[1m{\u001b[0m\n",
       "                    \u001b[32m'score'\u001b[0m: \u001b[32m'E'\u001b[0m,\n",
       "                    \u001b[32m'judge_feedback'\u001b[0m: \u001b[32m'Answer: E, Explanation: The GENERATED RESPONSE contains a list of popular \u001b[0m\n",
       "\u001b[32mSpanish dishes, including Tapas, but it does not express a personal preference or opinion, which is why the \u001b[0m\n",
       "\u001b[32mdifferences in style and the additional information do not affect the factual content.'\u001b[0m\n",
       "                \u001b[1m}\u001b[0m,\n",
       "                \u001b[1m{\u001b[0m\n",
       "                    \u001b[32m'score'\u001b[0m: \u001b[32m'E'\u001b[0m,\n",
       "                    \u001b[32m'judge_feedback'\u001b[0m: \u001b[32m'Answer: E, Explanation: The GENERATED RESPONSE contains a list of popular \u001b[0m\n",
       "\u001b[32mTurkish dishes, including Baklava, but it does not directly answer the question \"What is your favorite Turkish \u001b[0m\n",
       "\u001b[32mfood?\" as it is phrased. The GENERATED RESPONSE is focused on providing information about Turkish cuisine, while \u001b[0m\n",
       "\u001b[32mthe EXPECTED RESPONSE is a personal preference. The differences in the question and response do not affect the \u001b[0m\n",
       "\u001b[32mfactual content, as the GENERATED RESPONSE provides accurate information about Turkish dishes, including Baklava.'\u001b[0m\n",
       "                \u001b[1m}\u001b[0m\n",
       "            \u001b[1m]\u001b[0m\n",
       "        \u001b[1m)\u001b[0m\n",
       "    \u001b[1m}\u001b[0m\n",
       "\u001b[1m)\u001b[0m\n"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scoring_response = client.scoring.score(\n",
    "    input_rows=model_eval_rows, scoring_functions=scoring_params\n",
    ")\n",
    "Pretty(scoring_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d0c9b5d",
   "metadata": {},
   "source": [
    "# Evaluate the Backend"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57f80077",
   "metadata": {},
   "source": [
    "We can also evaluate our backend instead of the model, all we need to do is send the `input queries` to the backend and put the inputs into `generated_answer`.  \n",
    "Since our backend is prompted to summarize text, we add tests that works well for such tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a228d6bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "backend_url = \"http://canopy-backend.user1-canopy.svc.cluster.local:8000\"\n",
    "endpoint_to_test = \"/summarize\"\n",
    "\n",
    "backend_eval_rows = [\n",
    "    {\n",
    "        \"input_query\": \"Llama 3.2 is a state-of-the-art language model that excels in various natural language processing tasks, including summarization, translation, and question answering.\",\n",
    "        \"expected_answer\": \"Llama 3.2 is a top-tier language model for NLP tasks.\",\n",
    "    },\n",
    "    {\n",
    "        \"input_query\": \"Artificial intelligence and machine learning have revolutionized numerous industries in recent years. \\\n",
    "From healthcare diagnostics that can detect diseases earlier than human doctors, to autonomous vehicles that promise safer transportation, \\\n",
    "to recommendation systems that personalize our digital experiences, AI technologies are becoming increasingly sophisticated. \\\n",
    "However, these advances also bring challenges including ethical concerns about bias in algorithms, job displacement due to automation, and the need for robust data privacy protections?\",\n",
    "        \"expected_answer\": \"AI and ML have transformed industries through healthcare diagnostics, autonomous vehicles, and recommendation systems, but also raise concerns about bias, job displacement, and privacy.\",\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f323c083",
   "metadata": {},
   "outputs": [],
   "source": [
    "def send_request(payload, url):\n",
    "    import httpx\n",
    "    import json\n",
    "    full_response = \"\"\n",
    "\n",
    "    with httpx.Client(timeout=None) as client:\n",
    "        with client.stream(\"POST\", url, json=payload) as response:\n",
    "            for line in response.iter_lines():\n",
    "                if line.startswith(\"data: \"):\n",
    "                    try:\n",
    "                        data = json.loads(line[len(\"data: \"):])\n",
    "                        full_response += data.get(\"delta\", \"\")\n",
    "                    except json.JSONDecodeError:\n",
    "                        continue\n",
    "\n",
    "    return full_response\n",
    "\n",
    "def prompt_backend(prompt, backend_url, endpoint_to_test):\n",
    "    from urllib.parse import urljoin\n",
    "    url = urljoin(backend_url, endpoint_to_test)\n",
    "    payload = {\n",
    "        \"prompt\": prompt\n",
    "    }\n",
    "    return send_request(payload, url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "628c33e0",
   "metadata": {},
   "outputs": [
    {
     "ename": "ConnectError",
     "evalue": "[Errno -2] Name or service not known",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mConnectError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/app-root/lib64/python3.11/site-packages/httpx/_transports/default.py:101\u001b[39m, in \u001b[36mmap_httpcore_exceptions\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    100\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m101\u001b[39m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[32m    102\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/app-root/lib64/python3.11/site-packages/httpx/_transports/default.py:250\u001b[39m, in \u001b[36mHTTPTransport.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    249\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m map_httpcore_exceptions():\n\u001b[32m--> \u001b[39m\u001b[32m250\u001b[39m     resp = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_pool\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    252\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(resp.stream, typing.Iterable)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/app-root/lib64/python3.11/site-packages/httpcore/_sync/connection_pool.py:256\u001b[39m, in \u001b[36mConnectionPool.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    255\u001b[39m     \u001b[38;5;28mself\u001b[39m._close_connections(closing)\n\u001b[32m--> \u001b[39m\u001b[32m256\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exc \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    258\u001b[39m \u001b[38;5;66;03m# Return the response. Note that in this case we still have to manage\u001b[39;00m\n\u001b[32m    259\u001b[39m \u001b[38;5;66;03m# the point at which the response is closed.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/app-root/lib64/python3.11/site-packages/httpcore/_sync/connection_pool.py:236\u001b[39m, in \u001b[36mConnectionPool.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    234\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    235\u001b[39m     \u001b[38;5;66;03m# Send the request on the assigned connection.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m236\u001b[39m     response = \u001b[43mconnection\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    237\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpool_request\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\n\u001b[32m    238\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    239\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m ConnectionNotAvailable:\n\u001b[32m    240\u001b[39m     \u001b[38;5;66;03m# In some cases a connection may initially be available to\u001b[39;00m\n\u001b[32m    241\u001b[39m     \u001b[38;5;66;03m# handle a request, but then become unavailable.\u001b[39;00m\n\u001b[32m    242\u001b[39m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[32m    243\u001b[39m     \u001b[38;5;66;03m# In this case we clear the connection and try again.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/app-root/lib64/python3.11/site-packages/httpcore/_sync/connection.py:101\u001b[39m, in \u001b[36mHTTPConnection.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    100\u001b[39m     \u001b[38;5;28mself\u001b[39m._connect_failed = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m101\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exc\n\u001b[32m    103\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._connection.handle_request(request)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/app-root/lib64/python3.11/site-packages/httpcore/_sync/connection.py:78\u001b[39m, in \u001b[36mHTTPConnection.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m     77\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._connection \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m78\u001b[39m     stream = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_connect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     80\u001b[39m     ssl_object = stream.get_extra_info(\u001b[33m\"\u001b[39m\u001b[33mssl_object\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/app-root/lib64/python3.11/site-packages/httpcore/_sync/connection.py:124\u001b[39m, in \u001b[36mHTTPConnection._connect\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    123\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m Trace(\u001b[33m\"\u001b[39m\u001b[33mconnect_tcp\u001b[39m\u001b[33m\"\u001b[39m, logger, request, kwargs) \u001b[38;5;28;01mas\u001b[39;00m trace:\n\u001b[32m--> \u001b[39m\u001b[32m124\u001b[39m     stream = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_network_backend\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconnect_tcp\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    125\u001b[39m     trace.return_value = stream\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/app-root/lib64/python3.11/site-packages/httpcore/_backends/sync.py:207\u001b[39m, in \u001b[36mSyncBackend.connect_tcp\u001b[39m\u001b[34m(self, host, port, timeout, local_address, socket_options)\u001b[39m\n\u001b[32m    202\u001b[39m exc_map: ExceptionMapping = {\n\u001b[32m    203\u001b[39m     socket.timeout: ConnectTimeout,\n\u001b[32m    204\u001b[39m     \u001b[38;5;167;01mOSError\u001b[39;00m: ConnectError,\n\u001b[32m    205\u001b[39m }\n\u001b[32m--> \u001b[39m\u001b[32m207\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mwith\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mmap_exceptions\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexc_map\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    208\u001b[39m \u001b[43m    \u001b[49m\u001b[43msock\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43msocket\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcreate_connection\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    209\u001b[39m \u001b[43m        \u001b[49m\u001b[43maddress\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    210\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    211\u001b[39m \u001b[43m        \u001b[49m\u001b[43msource_address\u001b[49m\u001b[43m=\u001b[49m\u001b[43msource_address\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    212\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib64/python3.11/contextlib.py:158\u001b[39m, in \u001b[36m_GeneratorContextManager.__exit__\u001b[39m\u001b[34m(self, typ, value, traceback)\u001b[39m\n\u001b[32m    157\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m158\u001b[39m     \u001b[38;5;28mself\u001b[39m.gen.throw(typ, value, traceback)\n\u001b[32m    159\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m    160\u001b[39m     \u001b[38;5;66;03m# Suppress StopIteration *unless* it's the same exception that\u001b[39;00m\n\u001b[32m    161\u001b[39m     \u001b[38;5;66;03m# was passed to throw().  This prevents a StopIteration\u001b[39;00m\n\u001b[32m    162\u001b[39m     \u001b[38;5;66;03m# raised inside the \"with\" statement from being suppressed.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/app-root/lib64/python3.11/site-packages/httpcore/_exceptions.py:14\u001b[39m, in \u001b[36mmap_exceptions\u001b[39m\u001b[34m(map)\u001b[39m\n\u001b[32m     13\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(exc, from_exc):\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m to_exc(exc) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mexc\u001b[39;00m\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[31mConnectError\u001b[39m: [Errno -2] Name or service not known",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mConnectError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m eval_row \u001b[38;5;129;01min\u001b[39;00m backend_eval_rows:\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m     eval_row[\u001b[33m\"\u001b[39m\u001b[33mgenerated_answer\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[43mprompt_backend\u001b[49m\u001b[43m(\u001b[49m\u001b[43meval_row\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43minput_query\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbackend_url\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mendpoint_to_test\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      4\u001b[39m Pretty(backend_eval_rows)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 24\u001b[39m, in \u001b[36mprompt_backend\u001b[39m\u001b[34m(prompt, backend_url, endpoint_to_test)\u001b[39m\n\u001b[32m     20\u001b[39m url = urljoin(backend_url, endpoint_to_test)\n\u001b[32m     21\u001b[39m payload = {\n\u001b[32m     22\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mprompt\u001b[39m\u001b[33m\"\u001b[39m: prompt\n\u001b[32m     23\u001b[39m }\n\u001b[32m---> \u001b[39m\u001b[32m24\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msend_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpayload\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 7\u001b[39m, in \u001b[36msend_request\u001b[39m\u001b[34m(payload, url)\u001b[39m\n\u001b[32m      4\u001b[39m full_response = \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m httpx.Client(timeout=\u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m client:\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mwith\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mclient\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mPOST\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjson\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpayload\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mas\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mline\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m.\u001b[49m\u001b[43miter_lines\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mline\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstartswith\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdata: \u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib64/python3.11/contextlib.py:137\u001b[39m, in \u001b[36m_GeneratorContextManager.__enter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    135\u001b[39m \u001b[38;5;28;01mdel\u001b[39;00m \u001b[38;5;28mself\u001b[39m.args, \u001b[38;5;28mself\u001b[39m.kwds, \u001b[38;5;28mself\u001b[39m.func\n\u001b[32m    136\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m137\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28mself\u001b[39m.gen)\n\u001b[32m    138\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[32m    139\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mgenerator didn\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt yield\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/app-root/lib64/python3.11/site-packages/httpx/_client.py:868\u001b[39m, in \u001b[36mClient.stream\u001b[39m\u001b[34m(self, method, url, content, data, files, json, params, headers, cookies, auth, follow_redirects, timeout, extensions)\u001b[39m\n\u001b[32m    845\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    846\u001b[39m \u001b[33;03mAlternative to `httpx.request()` that streams the response body\u001b[39;00m\n\u001b[32m    847\u001b[39m \u001b[33;03minstead of loading it into memory at once.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    853\u001b[39m \u001b[33;03m[0]: /quickstart#streaming-responses\u001b[39;00m\n\u001b[32m    854\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    855\u001b[39m request = \u001b[38;5;28mself\u001b[39m.build_request(\n\u001b[32m    856\u001b[39m     method=method,\n\u001b[32m    857\u001b[39m     url=url,\n\u001b[32m   (...)\u001b[39m\u001b[32m    866\u001b[39m     extensions=extensions,\n\u001b[32m    867\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m868\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    869\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    870\u001b[39m \u001b[43m    \u001b[49m\u001b[43mauth\u001b[49m\u001b[43m=\u001b[49m\u001b[43mauth\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    871\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    872\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    873\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    874\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    875\u001b[39m     \u001b[38;5;28;01myield\u001b[39;00m response\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/app-root/lib64/python3.11/site-packages/httpx/_client.py:914\u001b[39m, in \u001b[36mClient.send\u001b[39m\u001b[34m(self, request, stream, auth, follow_redirects)\u001b[39m\n\u001b[32m    910\u001b[39m \u001b[38;5;28mself\u001b[39m._set_timeout(request)\n\u001b[32m    912\u001b[39m auth = \u001b[38;5;28mself\u001b[39m._build_request_auth(request, auth)\n\u001b[32m--> \u001b[39m\u001b[32m914\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_send_handling_auth\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    915\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    916\u001b[39m \u001b[43m    \u001b[49m\u001b[43mauth\u001b[49m\u001b[43m=\u001b[49m\u001b[43mauth\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    917\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    918\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhistory\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    919\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    920\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    921\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m stream:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/app-root/lib64/python3.11/site-packages/httpx/_client.py:942\u001b[39m, in \u001b[36mClient._send_handling_auth\u001b[39m\u001b[34m(self, request, auth, follow_redirects, history)\u001b[39m\n\u001b[32m    939\u001b[39m request = \u001b[38;5;28mnext\u001b[39m(auth_flow)\n\u001b[32m    941\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m942\u001b[39m     response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_send_handling_redirects\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    943\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    944\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    945\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhistory\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhistory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    946\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    947\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    948\u001b[39m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/app-root/lib64/python3.11/site-packages/httpx/_client.py:979\u001b[39m, in \u001b[36mClient._send_handling_redirects\u001b[39m\u001b[34m(self, request, follow_redirects, history)\u001b[39m\n\u001b[32m    976\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._event_hooks[\u001b[33m\"\u001b[39m\u001b[33mrequest\u001b[39m\u001b[33m\"\u001b[39m]:\n\u001b[32m    977\u001b[39m     hook(request)\n\u001b[32m--> \u001b[39m\u001b[32m979\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_send_single_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    980\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    981\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._event_hooks[\u001b[33m\"\u001b[39m\u001b[33mresponse\u001b[39m\u001b[33m\"\u001b[39m]:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/app-root/lib64/python3.11/site-packages/httpx/_client.py:1014\u001b[39m, in \u001b[36mClient._send_single_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m   1009\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m   1010\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mAttempted to send an async request with a sync Client instance.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1011\u001b[39m     )\n\u001b[32m   1013\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m request_context(request=request):\n\u001b[32m-> \u001b[39m\u001b[32m1014\u001b[39m     response = \u001b[43mtransport\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1016\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response.stream, SyncByteStream)\n\u001b[32m   1018\u001b[39m response.request = request\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/app-root/lib64/python3.11/site-packages/httpx/_transports/default.py:249\u001b[39m, in \u001b[36mHTTPTransport.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    235\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mhttpcore\u001b[39;00m\n\u001b[32m    237\u001b[39m req = httpcore.Request(\n\u001b[32m    238\u001b[39m     method=request.method,\n\u001b[32m    239\u001b[39m     url=httpcore.URL(\n\u001b[32m   (...)\u001b[39m\u001b[32m    247\u001b[39m     extensions=request.extensions,\n\u001b[32m    248\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m249\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mwith\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mmap_httpcore_exceptions\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    250\u001b[39m \u001b[43m    \u001b[49m\u001b[43mresp\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_pool\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    252\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(resp.stream, typing.Iterable)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib64/python3.11/contextlib.py:158\u001b[39m, in \u001b[36m_GeneratorContextManager.__exit__\u001b[39m\u001b[34m(self, typ, value, traceback)\u001b[39m\n\u001b[32m    156\u001b[39m     value = typ()\n\u001b[32m    157\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m158\u001b[39m     \u001b[38;5;28mself\u001b[39m.gen.throw(typ, value, traceback)\n\u001b[32m    159\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m    160\u001b[39m     \u001b[38;5;66;03m# Suppress StopIteration *unless* it's the same exception that\u001b[39;00m\n\u001b[32m    161\u001b[39m     \u001b[38;5;66;03m# was passed to throw().  This prevents a StopIteration\u001b[39;00m\n\u001b[32m    162\u001b[39m     \u001b[38;5;66;03m# raised inside the \"with\" statement from being suppressed.\u001b[39;00m\n\u001b[32m    163\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m exc \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m value\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/app-root/lib64/python3.11/site-packages/httpx/_transports/default.py:118\u001b[39m, in \u001b[36mmap_httpcore_exceptions\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    115\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[32m    117\u001b[39m message = \u001b[38;5;28mstr\u001b[39m(exc)\n\u001b[32m--> \u001b[39m\u001b[32m118\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m mapped_exc(message) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mexc\u001b[39;00m\n",
      "\u001b[31mConnectError\u001b[39m: [Errno -2] Name or service not known"
     ]
    }
   ],
   "source": [
    "for eval_row in backend_eval_rows:\n",
    "    eval_row[\"generated_answer\"] = prompt_backend(eval_row[\"input_query\"], backend_url, endpoint_to_test)\n",
    "\n",
    "Pretty(backend_eval_rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e814b840",
   "metadata": {},
   "source": [
    "And again, as soon as we have what we want to evaluate in a json format, we can evaluate it with Llamastack."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffb3e838",
   "metadata": {},
   "outputs": [],
   "source": [
    "scoring_response = client.scoring.score(\n",
    "    input_rows=backend_eval_rows, scoring_functions=scoring_params\n",
    ")\n",
    "Pretty(scoring_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d78602af",
   "metadata": {},
   "source": [
    "# Datasets üìñ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "370a44f0",
   "metadata": {},
   "source": [
    "Finally, let's use a dataset with already populated inputs and expected answers and see how well our model and backend does against those."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c40aa068-836d-4ce7-b687-ae0d8aaf7ef7",
   "metadata": {},
   "source": [
    "## SimpleQA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf386e2a",
   "metadata": {},
   "source": [
    "SimpleQA is a (as the name suggests) simple dataset with questions and answers that you can run on your model.  \n",
    "You can find the full dataset, as well as a few others, here: https://huggingface.co/llamastack/datasets  \n",
    "In our case, we don't want to wait to evaluate the full dataset, so we will take 5 examples from the training part of this dataset to test our model on.  \n",
    "Similarily, we will just test on our model this time, partially because our `summarize` endpoint is not prompted for handling QA, and partially because we have already seen how we can evaluate our backend above.  \n",
    "\n",
    "To fetch the dataset we use Llamastack again, where we can register the dataset which allows us to fetch data from it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdea22fe-3ce5-42e1-a6c1-e80b574309e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "simpleqa_dataset_id = \"huggingface::simpleqa\"\n",
    "\n",
    "_ = client.datasets.register(\n",
    "    purpose=\"eval/messages-answer\",\n",
    "    source={\n",
    "        \"type\": \"uri\",\n",
    "        \"uri\": \"huggingface://datasets/llamastack/simpleqa?split=train\",\n",
    "    },\n",
    "    dataset_id=simpleqa_dataset_id,\n",
    ")\n",
    "\n",
    "dataset_eval_rows = client.datasets.iterrows(\n",
    "    dataset_id=simpleqa_dataset_id,\n",
    "    limit=5,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e9e5a99-3bf4-4fe5-8576-3d04ca91b7ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "Pretty(dataset_eval_rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b399035",
   "metadata": {},
   "source": [
    "As you can see, the dataset is already formated with `input_query` and `expected_answer`, and we get some extra information such as `metadata` and `chat_completion_input`.  \n",
    "\n",
    "We could now just input this evaluation set to our model just like we did before, but since we are only evaluating the model we will make use of another Llamastack functionality called `benchmarks`.  \n",
    "This simply passes the dataset through the model and returns the response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8309743-b63d-4275-8ed0-e039d0d57b2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.benchmarks.register(\n",
    "    benchmark_id=\"meta-reference::simpleqa\",\n",
    "    dataset_id=simpleqa_dataset_id,\n",
    "    scoring_functions=[\"llm-as-judge::base\"],\n",
    ")\n",
    "\n",
    "response = client.eval.evaluate_rows(\n",
    "    benchmark_id=\"meta-reference::simpleqa\",\n",
    "    input_rows=dataset_eval_rows.data,\n",
    "    scoring_functions=[\"llm-as-judge::base\"],\n",
    "    benchmark_config={\n",
    "        \"eval_candidate\": {\n",
    "            \"type\": \"model\",\n",
    "            \"model\": model_id,\n",
    "            \"sampling_params\": {\n",
    "                \"strategy\": {\n",
    "                    \"type\": \"greedy\",\n",
    "                },\n",
    "                \"max_tokens\": 4096,\n",
    "                \"repeat_penalty\": 1.0,\n",
    "            },\n",
    "        },\n",
    "    },\n",
    ")\n",
    "Pretty(response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a173fc53",
   "metadata": {},
   "source": [
    "To summarize, we have now used the Llamastack Eval endpoint to evaluate our raw LLM as well as our backend system, both with custom evaluations and with those fetched from a dataset.  \n",
    "With this knowledge, we can now build an evaluation workflow that lets us test our backend and model before it goes into production üëè"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "app-root",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
